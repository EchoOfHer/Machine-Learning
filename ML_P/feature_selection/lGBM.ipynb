{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867c5331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b85caec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('../data/salary.train.processed.csv').set_index('id')\n",
    "data_test = pd.read_csv('../data/salary.test.processed.csv').set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e71a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example: assuming data_train is your processed dataset\n",
    "X = data_train.drop(columns=['label'])\n",
    "y = data_train['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7429b6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corr_with_label = X.corrwith(y).abs().sort_values(ascending=False)\n",
    "\n",
    "print(\"Top correlated features with label:\")\n",
    "print(corr_with_label.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5d8e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.1\n",
    "selected_features = corr_with_label[corr_with_label >= threshold].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190f93dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_selected = X[selected_features]\n",
    "print(f\"Selected {len(selected_features)} features based on correlation >= {threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc3582e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = X_selected.corr().abs()\n",
    "\n",
    "# Create upper triangle mask\n",
    "upper = corr_matrix.where(~np.tril(np.ones(corr_matrix.shape)).astype(bool))\n",
    "\n",
    "# Drop features with high inter-correlation\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.9)]\n",
    "\n",
    "X_final = X_selected.drop(columns=to_drop)\n",
    "print(f\"Dropped {len(to_drop)} redundant features.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22015c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6922733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# à¸„à¸³à¸™à¸§à¸“ correlation matrix (à¹€à¸‰à¸žà¸²à¸° numeric features)\n",
    "corr_matrix = data_train.corr(numeric_only=True)\n",
    "\n",
    "# à¸ªà¸£à¹‰à¸²à¸‡ heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(\n",
    "    corr_matrix,\n",
    "    annot=False,  # à¹„à¸¡à¹ˆà¹à¸ªà¸”à¸‡à¸•à¸±à¸§à¹€à¸¥à¸‚à¸šà¸™ heatmap\n",
    "    cmap='coolwarm',\n",
    "    center=0,\n",
    "    linewidths=0.5,  # à¸„à¸§à¸²à¸¡à¸«à¸™à¸²à¸‚à¸­à¸‡à¹€à¸ªà¹‰à¸™à¹à¸šà¹ˆà¸‡\n",
    "    linecolor='white'  # à¸ªà¸µà¸‚à¸­à¸‡à¹€à¸ªà¹‰à¸™à¹à¸šà¹ˆà¸‡\n",
    ")\n",
    "plt.title(\"Correlation Heatmap of All Features and Target\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e3ddb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fc69b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b47a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e180cb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('../data/salary.train.processed.csv').set_index('id')\n",
    "data_test = pd.read_csv('../data/salary.test.processed.csv').set_index('id')\n",
    "data_train = data_train[X_final.columns.tolist() + ['label']]\n",
    "data_test = data_test[X_final.columns.tolist() + ['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f18338",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import lightgbm as lgbm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    train_df = data_train\n",
    "    test_df = data_test\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Could not find the processed CSV files.\")\n",
    "    raise\n",
    "\n",
    "X_train = train_df.drop('label', axis=1)\n",
    "y_train = train_df['label']\n",
    "X_test = test_df.drop('label', axis=1)\n",
    "y_test = test_df['label']\n",
    "# --- End of Data Loading ---\n",
    "\n",
    "# --- Train-Validation Split ---\n",
    "X_train_final, X_val_final, y_train_final, y_val_final = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "# --- Define LightGBM Model with Default Parameters ---\n",
    "lgbm_model = lgbm.LGBMClassifier(\n",
    "    objective='binary',\n",
    "    metric='binary_logloss',\n",
    "    random_state=42,\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.1,  # à¸„à¹ˆà¸² default\n",
    "    num_leaves=31,      # à¸„à¹ˆà¸² default\n",
    "    max_depth=-1,       # à¸„à¹ˆà¸² default (à¹„à¸¡à¹ˆà¸ˆà¸³à¸à¸±à¸”à¸„à¸§à¸²à¸¡à¸¥à¸¶à¸)\n",
    "    min_child_samples=20,  # à¸„à¹ˆà¸² default\n",
    "    subsample=1.0,      # à¸„à¹ˆà¸² default\n",
    "    colsample_bytree=1.0,  # à¸„à¹ˆà¸² default\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "# --- Train the Model with Early Stopping ---\n",
    "lgbm_model.fit(\n",
    "    X_train_final,\n",
    "    y_train_final,\n",
    "    eval_set=[(X_val_final, y_val_final)],\n",
    "    eval_metric='logloss',\n",
    "    callbacks=[lgbm.early_stopping(50, verbose=False)]\n",
    ")\n",
    "\n",
    "# --- Evaluate on Test Set ---\n",
    "y_pred_lgbm = lgbm_model.predict(X_test)\n",
    "\n",
    "print(\"\\nLightGBM Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_lgbm, digits=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34e5f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('../data/salary.train.processed.csv').set_index('id')\n",
    "data_test = pd.read_csv('../data/salary.test.processed.csv').set_index('id')\n",
    "data_train = data_train[X_final.columns.tolist() + ['label']]\n",
    "data_test = data_test[X_final.columns.tolist() + ['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac49f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import optuna\n",
    "import numpy as np\n",
    "import lightgbm as lgbm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "import sklearn\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    train_df = data_train\n",
    "    test_df = data_test\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Could not find the processed CSV files.\")\n",
    "    raise\n",
    "\n",
    "X_train = train_df.drop('label', axis=1)\n",
    "y_train = train_df['label']\n",
    "X_test = test_df.drop('label', axis=1)\n",
    "y_test = test_df['label']\n",
    "# --- End of Data Loading ---\n",
    "\n",
    "# --- Step 1: Define the Objective Function ---\n",
    "def objective_lgbm(trial):\n",
    "    \"\"\"\n",
    "    Objective function with manual Cross-Validation and Early Stopping.\n",
    "    \"\"\"\n",
    "    # 1. Define the search space for LightGBM\n",
    "    param = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'random_state': 42,\n",
    "        'n_estimators': 1000,\n",
    "        'verbose': -1,\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 10, 50),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 30),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0)\n",
    "    }\n",
    "    \n",
    "    # 2. Set up 3-Fold Cross-Validation\n",
    "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "\n",
    "    # 3. Manually run the CV loop\n",
    "    for train_idx, val_idx in skf.split(X_train, y_train):\n",
    "        X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        \n",
    "        model = lgbm.LGBMClassifier(**param)\n",
    "        \n",
    "        # Train with early stopping\n",
    "        model.fit(\n",
    "            X_train_fold, \n",
    "            y_train_fold,\n",
    "            eval_set=[(X_val_fold, y_val_fold)],\n",
    "            eval_metric='logloss',\n",
    "            callbacks=[lgbm.early_stopping(50, verbose=False)]\n",
    "        )\n",
    "        \n",
    "        preds = model.predict(X_val_fold)\n",
    "        f1 = sklearn.metrics.f1_score(y_val_fold, preds, average='binary')\n",
    "        scores.append(f1)\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "# --- Step 2: Create and Run the Study ---\n",
    "study_name = \"lgbm_salary_tuning\"  # à¸•à¸±à¹‰à¸‡à¸Šà¸·à¹ˆà¸­à¹‚à¸›à¸£à¹€à¸ˆà¸à¸•à¹Œ (à¹à¸•à¹ˆà¹„à¸¡à¹ˆà¸šà¸±à¸™à¸—à¸¶à¸à¸¥à¸‡ storage)\n",
    "\n",
    "print(f\"Starting Optuna study: '{study_name}'\")\n",
    "print(f\"Running 100 new trials...\")\n",
    "\n",
    "# à¸ªà¸£à¹‰à¸²à¸‡ Study à¹ƒà¸«à¸¡à¹ˆà¹‚à¸”à¸¢à¹„à¸¡à¹ˆà¹ƒà¸Šà¹‰ storage\n",
    "study_lgbm = optuna.create_study(\n",
    "    study_name=study_name,\n",
    "    direction='maximize'\n",
    ")\n",
    "\n",
    "# à¸£à¸±à¸™ optimization\n",
    "study_lgbm.optimize(objective_lgbm, n_trials=100)\n",
    "print(\"Study complete!\")\n",
    "print(f\"Total number of trials in study: {len(study_lgbm.trials)}\")\n",
    "\n",
    "# --- Step 3: Get Best Params ---\n",
    "print(\"\\nBest trial:\")\n",
    "print(f\"  Value (Mean F1): {study_lgbm.best_value:.4f}\")\n",
    "print(\"  Best Params: \")\n",
    "print(study_lgbm.best_params)\n",
    "\n",
    "# à¹€à¸à¹‡à¸šà¸žà¸²à¸£à¸²à¸¡à¸´à¹€à¸•à¸­à¸£à¹Œà¸—à¸µà¹ˆà¸”à¸µà¸—à¸µà¹ˆà¸ªà¸¸à¸”\n",
    "best_params = study_lgbm.best_params\n",
    "print(f\"\\nSuccessfully stored parameters in 'best_params' variable.\")\n",
    "\n",
    "# --- Step 4: Train the FINAL Model ---\n",
    "X_train_final, X_val_final, y_train_final, y_val_final = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "final_lgbm = lgbm.LGBMClassifier(\n",
    "    random_state=42,\n",
    "    n_estimators=1000,\n",
    "    **best_params\n",
    ")\n",
    "\n",
    "final_lgbm.fit(\n",
    "    X_train_final, \n",
    "    y_train_final,\n",
    "    eval_set=[(X_val_final, y_val_final)],\n",
    "    eval_metric='logloss',\n",
    "    callbacks=[lgbm.early_stopping(50, verbose=False)]\n",
    ")\n",
    "\n",
    "# --- Step 5: Evaluate the Optuna-Tuned Model on the TEST set ---\n",
    "y_pred_optuna_lgbm = final_lgbm.predict(X_test)\n",
    "\n",
    "print(\"\\nOptuna-Tuned LightGBM Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_optuna_lgbm, digits=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b115976",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd220a2",
   "metadata": {},
   "source": [
    "n_estimators= 900, learning_rate= 0.05487718973019812, num_leaves:=63, min_child_samples 42, subsample=0.8406013365172204, colsample_bytree= 0.1319962821123755, reg_alpha= 0.995264535862729, reg_lambda= 0.01927049495328959,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbf3957",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('../data/salary.train.processed.csv').set_index('id')\n",
    "data_test = pd.read_csv('../data/salary.test.processed.csv').set_index('id')\n",
    "data_train = data_train[X_final.columns.tolist() + ['label']]\n",
    "data_test = data_test[X_final.columns.tolist() + ['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2c1cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import lightgbm as lgb\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# --- 1. Load Data (to calculate the weight) ---\n",
    "try:\n",
    "    data_train_full = data_train\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: à¹„à¸¡à¹ˆà¸žà¸šà¹„à¸Ÿà¸¥à¹Œ salary.train.processed.csv à¸à¸£à¸¸à¸“à¸²à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š path\")\n",
    "    # exit() \n",
    "\n",
    "X_full = data_train_full.drop(['label'], axis='columns')\n",
    "y_full = data_train_full['label']\n",
    "\n",
    "# --- 2. Calculate scale_pos_weight ---\n",
    "# This is the same logic as your XGBoost code\n",
    "scale_pos_weight = len(y_full[y_full == 0]) / len(y_full[y_full == 1])\n",
    "print(f\"Calculated scale_pos_weight: {scale_pos_weight:.4f}\")\n",
    "\n",
    "# --- 3. Define Your Best Parameters ---\n",
    "# These are the params you provided in your prompt\n",
    "best_lgbm_params = best_params\n",
    "\n",
    "# --- 4. Create and Train the Model ---\n",
    "print(\"\\nTraining LightGBM model with class weighting...\")\n",
    "\n",
    "lgbm_model_final = lgb.LGBMClassifier(\n",
    "    **best_lgbm_params,       # Apply all your tuned parameters\n",
    "    scale_pos_weight=scale_pos_weight, # ðŸ‘ˆ Here is the class weight\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train the model on the full training dataset\n",
    "lgbm_model_final.fit(X_full, y_full)\n",
    "\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# --- 5. Evaluate on Test Data (Recommended) ---\n",
    "print(\"\\nEvaluating model on test data...\")\n",
    "\n",
    "data_test_lgbm = data_test\n",
    "data_test_lgbm['prediction'] = lgbm_model_final.predict(data_test_lgbm.drop(['label'], axis='columns'))\n",
    "\n",
    "# Print the report\n",
    "report_scores_lgbm = sklearn.metrics.classification_report(\n",
    "    y_true=data_test_lgbm['label'],\n",
    "    y_pred=data_test_lgbm['prediction'],\n",
    "    digits=6,\n",
    "    output_dict=True\n",
    ")\n",
    "df_score_lgbm = pandas.DataFrame(report_scores_lgbm).transpose()\n",
    "\n",
    "print(\"\\nLightGBM (Tuned + Weighted) Report:\")\n",
    "print(df_score_lgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2c25c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('../data/salary.train.processed.csv').set_index('id')\n",
    "data_test = pd.read_csv('../data/salary.test.processed.csv').set_index('id')\n",
    "data_train = data_train[X_final.columns.tolist() + ['label']]\n",
    "data_test = data_test[X_final.columns.tolist() + ['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b515b261",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import joblib\n",
    "import pandas\n",
    "import lightgbm as lgb\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE  # ðŸ‘ˆ 1. Import SMOTE\n",
    "\n",
    "# --- 1. Load Data (Needed for training) ---\n",
    "try:\n",
    "    data_train_full = data_train\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: à¹„à¸¡à¹ˆà¸žà¸šà¹„à¸Ÿà¸¥à¹Œ salary.train.processed.csv à¸à¸£à¸¸à¸“à¸²à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š path\")\n",
    "    # exit() \n",
    "\n",
    "X_full = data_train_full.drop(['label'], axis='columns')\n",
    "y_full = data_train_full['label']\n",
    "\n",
    "print(f\"Original training data shape: {X_full.shape}\")\n",
    "print(f\"Original label distribution:\\n{y_full.value_counts()}\")\n",
    "\n",
    "# --- 2. Define Your Best Parameters ---\n",
    "# These are the params you provided\n",
    "best_lgbm_params = best_params\n",
    "\n",
    "# --- 3. Apply SMOTE to the Training Data ---\n",
    "print(\"\\nApplying SMOTE to the training data...\")\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_full, y_full)\n",
    "\n",
    "print(f\"New resampled training data shape: {X_resampled.shape}\")\n",
    "print(f\"New resampled label distribution:\\n{y_resampled.value_counts()}\")\n",
    "\n",
    "# --- 4. Create and Train the Final Model (with NO class_weight) ---\n",
    "print(\"\\nTraining final LGBM model on SMOTEd data...\")\n",
    "\n",
    "lgbm_model_final = lgb.LGBMClassifier(\n",
    "    **best_lgbm_params,\n",
    "    # âš ï¸ NO 'scale_pos_weight' or 'class_weight' here\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 5. Train the model on the NEW resampled data\n",
    "lgbm_model_final.fit(X_resampled, y_resampled)\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# --- 6. Evaluate on ORIGINAL Test Data ---\n",
    "print(\"\\nEvaluating model on *original* test data...\")\n",
    "\n",
    "data_test_lgbm = data_test\n",
    "\n",
    "# IMPORTANT: Do NOT apply SMOTE to the test data.\n",
    "X_test = data_test_lgbm.drop(['label'], axis='columns')\n",
    "y_test = data_test_lgbm['label']\n",
    "\n",
    "data_test_lgbm['prediction'] = lgbm_model_final.predict(X_test)\n",
    "\n",
    "# Print the report\n",
    "report_scores_lgbm = sklearn.metrics.classification_report(\n",
    "    y_true=y_test,\n",
    "    y_pred=data_test_lgbm['prediction'],\n",
    "    digits=6,\n",
    "    output_dict=True\n",
    ")\n",
    "df_score_lgbm = pandas.DataFrame(report_scores_lgbm).transpose()\n",
    "\n",
    "print(\"\\nLightGBM (Tuned + SMOTE) Report:\")\n",
    "print(df_score_lgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e188c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('../data/salary.train.processed.csv').set_index('id')\n",
    "data_test = pd.read_csv('../data/salary.test.processed.csv').set_index('id')\n",
    "data_train = data_train[X_final.columns.tolist() + ['label']]\n",
    "data_test = data_test[X_final.columns.tolist() + ['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66335604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import lightgbm as lgb\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "print(\"\\n--- 3. Testing LightGBM with SMOTETomek ---\")\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    data_train_full = data_train\n",
    "    data_test_lgbm = data_test\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: à¹„à¸¡à¹ˆà¸žà¸šà¹„à¸Ÿà¸¥à¹Œ salary.train.processed.csv à¸à¸£à¸¸à¸“à¸²à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š path\")\n",
    "    # exit() \n",
    "\n",
    "X_full = data_train_full.drop(['label'], axis='columns')\n",
    "y_full = data_train_full['label']\n",
    "X_test = data_test_lgbm.drop(['label'], axis='columns')\n",
    "y_test = data_test_lgbm['label']\n",
    "\n",
    "# --- Apply SMOTETomek ---\n",
    "print(\"Applying SMOTETomek...\")\n",
    "smt = SMOTETomek(random_state=42, n_jobs=-1)\n",
    "X_resampled, y_resampled = smt.fit_resample(X_full, y_full)\n",
    "print(f\"New resampled label distribution:\\n{y_resampled.value_counts()}\")\n",
    "\n",
    "# --- Define Parameters ---\n",
    "best_lgbm_params = best_params\n",
    "\n",
    "# --- Create and Train Model ---\n",
    "lgbm_model = lgb.LGBMClassifier(\n",
    "    **best_lgbm_params,\n",
    "    # âš ï¸ NO 'scale_pos_weight' or 'class_weight'\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "lgbm_model.fit(X_resampled, y_resampled) # Train on SMOTETomek data\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# --- Evaluate ---\n",
    "y_pred = lgbm_model.predict(X_test)\n",
    "report = classification_report(y_test, y_pred, digits=6, output_dict=True)\n",
    "df_report = pandas.DataFrame(report).transpose()\n",
    "\n",
    "print(\"\\nLightGBM (Tuned + SMOTETomek) Report:\")\n",
    "print(df_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56f3483",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('../data/salary.train.processed.csv').set_index('id')\n",
    "data_test = pd.read_csv('../data/salary.test.processed.csv').set_index('id')\n",
    "data_train = data_train[X_final.columns.tolist() + ['label']]\n",
    "data_test = data_test[X_final.columns.tolist() + ['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22984af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import lightgbm as lgb\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "print(\"\\n--- 4. Testing LightGBM with ADASYN ---\")\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    data_train_full = data_train\n",
    "    data_test_lgbm = data_test\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: à¹„à¸¡à¹ˆà¸žà¸šà¹„à¸Ÿà¸¥à¹Œ salary.train.processed.csv à¸à¸£à¸¸à¸“à¸²à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š path\")\n",
    "    # exit() \n",
    "\n",
    "X_full = data_train_full.drop(['label'], axis='columns')\n",
    "y_full = data_train_full['label']\n",
    "X_test = data_test_lgbm.drop(['label'], axis='columns')\n",
    "y_test = data_test_lgbm['label']\n",
    "\n",
    "# --- Apply ADASYN ---\n",
    "print(\"Applying ADASYN...\")\n",
    "ada = ADASYN(random_state=42)\n",
    "X_resampled, y_resampled = ada.fit_resample(X_full, y_full)\n",
    "print(f\"New resampled label distribution:\\n{y_resampled.value_counts()}\")\n",
    "\n",
    "# --- Define Parameters ---\n",
    "best_lgbm_params = best_params\n",
    "\n",
    "# --- Create and Train Model ---\n",
    "lgbm_model = lgb.LGBMClassifier(\n",
    "    **best_lgbm_params,\n",
    "    # âš ï¸ NO 'scale_pos_weight' or 'class_weight'\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "lgbm_model.fit(X_resampled, y_resampled) # Train on ADASYN data\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# --- Evaluate ---\n",
    "y_pred = lgbm_model.predict(X_test)\n",
    "report = classification_report(y_test, y_pred, digits=6, output_dict=True)\n",
    "df_report = pandas.DataFrame(report).transpose()\n",
    "\n",
    "print(\"\\nLightGBM (Tuned + ADASYN) Report:\")\n",
    "print(df_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24274188",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('../data/salary.train.processed.csv').set_index('id')\n",
    "data_test = pd.read_csv('../data/salary.test.processed.csv').set_index('id')\n",
    "data_train = data_train[X_final.columns.tolist() + ['label']]\n",
    "data_test = data_test[X_final.columns.tolist() + ['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62930543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import lightgbm as lgb\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "print(\"\\n--- 3. Testing LightGBM with SMOTETomek ---\")\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    data_train_full = data_train\n",
    "    data_test_lgbm = data_test\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: à¹„à¸¡à¹ˆà¸žà¸šà¹„à¸Ÿà¸¥à¹Œ salary.train.processed.csv à¸à¸£à¸¸à¸“à¸²à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š path\")\n",
    "    # exit() \n",
    "\n",
    "X_full = data_train_full.drop(['label'], axis='columns')\n",
    "y_full = data_train_full['label']\n",
    "X_test = data_test_lgbm.drop(['label'], axis='columns')\n",
    "y_test = data_test_lgbm['label']\n",
    "\n",
    "# --- Apply SMOTETomek ---\n",
    "print(\"Applying SMOTETomek...\")\n",
    "smt = SMOTETomek(random_state=42, n_jobs=-1)\n",
    "X_resampled, y_resampled = smt.fit_resample(X_full, y_full)\n",
    "print(f\"New resampled label distribution:\\n{y_resampled.value_counts()}\")\n",
    "\n",
    "# --- Define Parameters ---\n",
    "# final_best_lgbm_params = best_params\n",
    "\n",
    "# --- Create and Train Model ---\n",
    "best_lgbm_model = lgb.LGBMClassifier(\n",
    "    n_estimators= 900, learning_rate= 0.05487718973019812, num_leaves=63, min_child_samples=42, subsample=0.8406013365172204, colsample_bytree=0.1319962821123755, reg_alpha=0.995264535862729, reg_lambda=0.01927049495328959,\n",
    "    # âš ï¸ NO 'scale_pos_weight' or 'class_weight'\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "best_lgbm_model.fit(X_resampled, y_resampled) # Train on SMOTETomek data\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# --- Evaluate ---\n",
    "y_pred = best_lgbm_model.predict(X_test)\n",
    "report = classification_report(y_test, y_pred, digits=6, output_dict=True)\n",
    "df_report = pandas.DataFrame(report).transpose()\n",
    "\n",
    "print(\"\\nLightGBM (Tuned + SMOTETomek) Report:\")\n",
    "print(df_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbec6c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_lgbm_model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec1ed6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(best_lgbm_model, '../model/lgbm/lgbm_model_final_smote.joblib')\n",
    "with open('../model/lgbm/lgbm_config.json','w')as f:\n",
    "    json.dump(\n",
    "        obj=best_lgbm_model.get_params(),\n",
    "        fp=f,\n",
    "        indent = 4\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f68df1",
   "metadata": {},
   "source": [
    "{\n",
    "    'reg_lambda': 0, \n",
    "    'reg_alpha': 0.1, \n",
    "    'num_leaves': 31, \n",
    "    'n_estimators': 200, \n",
    "    'max_depth': 30, \n",
    "    'learning_rate': 0.05,\n",
    "    'random_state': 42, \n",
    "    'n_jobs': -1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429b91cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('../data/salary.train.processed.csv').set_index('id')\n",
    "data_test = pd.read_csv('../data/salary.test.processed.csv').set_index('id')\n",
    "data_train = data_train[X_final.columns.tolist() + ['label']]\n",
    "data_test = data_test[X_final.columns.tolist() + ['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bbb3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "{'n_estimators': 102, 'learning_rate': 0.26992371517621444, 'num_leaves': 22, 'max_depth': 5, 'reg_alpha': 8.715022016449311e-06, 'reg_lambda': 3.587506772725946, 'subsample': 0.9863285464013338, 'colsample_bytree': 0.7550416705324272}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e09056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import f1_score, precision_recall_curve, classification_report\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"--- Training Final LightGBM Model with Specific Parameters ---\")\n",
    "\n",
    "# --- 1. à¹‚à¸«à¸¥à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ ---\n",
    "try:\n",
    "    train_df = pd.read_csv(\"../data/salary.train.processed.csv\")\n",
    "    test_df = pd.read_csv(\"../data/salary.test.processed.csv\")\n",
    "    print(\"Files loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Files not found. Make sure 'salary.train.processed.csv' and 'salary.test.processed.csv' are in the same directory as the script.\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. à¸„à¸³à¸™à¸§à¸“ Weight (We don't use it, but it's good to check) ---\n",
    "label_counts = train_df['label'].value_counts()\n",
    "scale_pos_weight = label_counts[0] / label_counts[1]\n",
    "print(f\"Data Check: scale_pos_weight = {scale_pos_weight:.4f}\")\n",
    "\n",
    "# --- 3. à¸ªà¸£à¹‰à¸²à¸‡à¸Šà¸¸à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ Train/Test ---\n",
    "columns_to_drop = ['id', 'social-security-number', 'house-number', 'fnlwgt']\n",
    "target_column = 'label'\n",
    "\n",
    "train_cols_to_drop_actual = [col for col in columns_to_drop if col in train_df.columns]\n",
    "y_train = train_df[target_column]\n",
    "X_train = train_df.drop(columns=train_cols_to_drop_actual + [target_column])\n",
    "\n",
    "test_cols_to_drop_actual = [col for col in columns_to_drop if col in test_df.columns]\n",
    "y_test = test_df[target_column]\n",
    "X_test = test_df.drop(columns=test_cols_to_drop_actual + [target_column])\n",
    "\n",
    "print(\"Data splits created.\")\n",
    "\n",
    "# --- 4. à¸à¸³à¸«à¸™à¸”à¹‚à¸¡à¹€à¸”à¸¥à¹à¸¥à¸° Parameters à¸—à¸µà¹ˆà¸„à¸¸à¸“à¹€à¸¥à¸·à¸­à¸ ---\n",
    "final_params = {\n",
    "    'n_estimators': 900,\n",
    "    'learning_rate': 0.05487718973019812,\n",
    "    'num_leaves': 63,\n",
    "    'min_child_samples': 42,\n",
    "    'subsample': 0.8406013365172204,\n",
    "    'colsample_bytree': 0.1319962821123755,\n",
    "    'reg_alpha': 0.995264535862729,\n",
    "    'reg_lambda': 0.01927049495328959,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1\n",
    "    # 'scale_pos_weight' is intentionally removed\n",
    "}\n",
    "\n",
    "finalbest_lgbm_model = lgb.LGBMClassifier(**final_params)\n",
    "\n",
    "print(\"\\nStarting model training with specified parameters...\")\n",
    "# --- 5. à¹€à¸£à¸´à¹ˆà¸¡à¸à¸²à¸£ Train à¹‚à¸¡à¹€à¸”à¸¥ ---\n",
    "finalbest_lgbm_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# --- 6. à¸›à¸£à¸°à¹€à¸¡à¸´à¸™à¸œà¸¥à¹‚à¸¡à¹€à¸”à¸¥ (à¸—à¸µà¹ˆ Default Threshold 0.5) ---\n",
    "print(\"--- Results on Test Set (Default 0.5 Threshold) ---\")\n",
    "y_pred_default = finalbest_lgbm_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_default, digits=4))\n",
    "print(\"=\"*50)\n",
    "\n",
    "# --- 7. à¸„à¹‰à¸™à¸«à¸² F1 à¸—à¸µà¹ˆà¸”à¸µà¸—à¸µà¹ˆà¸ªà¸¸à¸” à¹à¸¥à¸°à¹à¸ªà¸”à¸‡ Full Report ---\n",
    "print(\"\\n--- Finding Optimal Threshold for Best F1-Score ---\")\n",
    "y_pred_probs = finalbest_lgbm_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_probs)\n",
    "f1_scores = (2 * precision * recall) / (precision + recall)\n",
    "f1_scores = np.nan_to_num(f1_scores)\n",
    "\n",
    "best_f1_index = np.argmax(f1_scores[:-1])\n",
    "best_f1 = f1_scores[best_f1_index]\n",
    "best_threshold = thresholds[best_f1_index]\n",
    "\n",
    "print(f\"   Best Threshold Found: {best_threshold:.4f}\")\n",
    "print(f\"   Best F1-Score (for class 1.0): {best_f1:.4f}\")\n",
    "\n",
    "# --- à¸™à¸µà¹ˆà¸„à¸·à¸­à¸ªà¹ˆà¸§à¸™à¸—à¸µà¹ˆà¹€à¸žà¸´à¹ˆà¸¡à¹€à¸‚à¹‰à¸²à¸¡à¸² ---\n",
    "# 7b. à¸ªà¸£à¹‰à¸²à¸‡ predictions à¹‚à¸”à¸¢à¹ƒà¸Šà¹‰ threshold à¸—à¸µà¹ˆà¸”à¸µà¸—à¸µà¹ˆà¸ªà¸¸à¸”\n",
    "y_pred_optimal = (y_pred_probs >= best_threshold).astype(int)\n",
    "\n",
    "print(\"\\n--- Full Report at Optimal Threshold ---\")\n",
    "# 7c. à¸žà¸´à¸¡à¸žà¹Œ classification report à¸‰à¸šà¸±à¸šà¹€à¸•à¹‡à¸¡\n",
    "print(classification_report(y_test, y_pred_optimal, digits=4))\n",
    "# ---------------------------------\n",
    "\n",
    "print(\"\\n--- Script Finished ---\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d6a0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7c. à¸žà¸´à¸¡à¸žà¹Œ classification report à¸‰à¸šà¸±à¸šà¹€à¸•à¹‡à¸¡\n",
    "print(classification_report(y_test, y_pred_optimal, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fcd1ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running the NEW Champion Model (F1 0.8105) ---\n",
      "Files loaded successfully.\n",
      "Data splits created.\n",
      "Calculated scale_pos_weight: 1.3882\n",
      "Using Best Threshold: 0.4675\n",
      "Training final champion model...\n",
      "[LightGBM] [Info] Number of positive: 7001, number of negative: 9719\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000722 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 125\n",
      "[LightGBM] [Info] Number of data points in the train set: 16720, number of used features: 49\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.418720 -> initscore=-0.328030\n",
      "[LightGBM] [Info] Start training from score -0.328030\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Evaluating model...\n",
      "\n",
      "==================================================\n",
      "--- Final NEW Champion Model Results ---\n",
      "   Best F1 Score:  0.8105\n",
      "   Recall:         0.8872\n",
      "   Precision:      0.7459\n",
      "==================================================\n",
      "\n",
      "--- Full Classification Report (at Optimal Threshold) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9044    0.7794    0.8373      2416\n",
      "         1.0     0.7459    0.8872    0.8105      1764\n",
      "\n",
      "    accuracy                         0.8249      4180\n",
      "   macro avg     0.8252    0.8333    0.8239      4180\n",
      "weighted avg     0.8375    0.8249    0.8260      4180\n",
      "\n",
      "\n",
      "--- Script Finished ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"--- Running the NEW Champion Model (F1 0.8105) ---\")\n",
    "\n",
    "# --- 1. à¹‚à¸«à¸¥à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ ---\n",
    "try:\n",
    "    train_df = pd.read_csv(\"../data/salary.train.processed.csv\")\n",
    "    test_df = pd.read_csv(\"../data/salary.test.processed.csv\")\n",
    "    print(\"Files loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Files not found.\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. à¸ªà¸£à¹‰à¸²à¸‡à¸Šà¸¸à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ Train/Test ---\n",
    "columns_to_drop = ['id', 'social-security-number', 'house-number', 'fnlwgt']\n",
    "target_column = 'label'\n",
    "\n",
    "train_cols_to_drop = [col for col in columns_to_drop if col in train_df.columns]\n",
    "y_train = train_df[target_column]\n",
    "X_train = train_df.drop(columns=train_cols_to_drop + [target_column])\n",
    "\n",
    "test_cols_to_drop = [col for col in columns_to_drop if col in test_df.columns]\n",
    "y_test = test_df[target_column]\n",
    "X_test = test_df.drop(columns=test_cols_to_drop + [target_column])\n",
    "\n",
    "print(\"Data splits created.\")\n",
    "\n",
    "# --- 3. ðŸ”‘ \"à¸ªà¸¹à¸•à¸£\" à¸—à¸µà¹ˆà¸”à¸µà¸—à¸µà¹ˆà¸ªà¸¸à¸” (The Full Recipe F1 0.8105) ---\n",
    "\n",
    "# 3a. à¸„à¹ˆà¸²à¸žà¸²à¸£à¸²à¸¡à¸´à¹€à¸•à¸­à¸£à¹Œà¹‚à¸¡à¹€à¸”à¸¥ (à¸ˆà¸²à¸ Optuna à¸£à¸­à¸šà¸¥à¹ˆà¸²à¸ªà¸¸à¸”)\n",
    "best_params = {\n",
    "    'n_estimators': 102,\n",
    "    'learning_rate': 0.26992371517621444,\n",
    "    'num_leaves': 22,\n",
    "    'max_depth': 5,\n",
    "    'reg_alpha': 8.715022016449311e-06,\n",
    "    'reg_lambda': 3.587506772725946,\n",
    "    'subsample': 0.9863285464013338,\n",
    "    'colsample_bytree': 0.7550416705324272,\n",
    "    \n",
    "    # à¸žà¸²à¸£à¸²à¸¡à¸´à¹€à¸•à¸­à¸£à¹Œà¸„à¸‡à¸—à¸µà¹ˆ (Static) à¸—à¸µà¹ˆà¸•à¹‰à¸­à¸‡à¹ƒà¸ªà¹ˆ\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "# 3b. à¸„à¹ˆà¸²à¸–à¹ˆà¸§à¸‡à¸™à¹‰à¸³à¸«à¸™à¸±à¸ (à¹à¸à¹‰à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹„à¸¡à¹ˆà¸ªà¸¡à¸”à¸¸à¸¥)\n",
    "# (à¸„à¹ˆà¸²à¸™à¸µà¹‰à¸¡à¸²à¸ˆà¸²à¸à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ à¹„à¸¡à¹ˆà¹„à¸”à¹‰à¸¡à¸²à¸ˆà¸²à¸ Optuna à¹à¸•à¹ˆà¸–à¸¹à¸à¹ƒà¸Šà¹‰à¸•à¸­à¸™à¹€à¸—à¸£à¸™)\n",
    "# We calculate it just in case, to be precise\n",
    "label_counts = y_train.value_counts()\n",
    "scale_pos_weight = label_counts[0] / label_counts[1]\n",
    "print(f\"Calculated scale_pos_weight: {scale_pos_weight:.4f}\")\n",
    "\n",
    "\n",
    "# 3c. à¹€à¸à¸“à¸‘à¹Œà¸à¸²à¸£à¸•à¸±à¸”à¸ªà¸´à¸™à¹ƒà¸ˆ (Threshold) à¸—à¸µà¹ˆà¸”à¸µà¸—à¸µà¹ˆà¸ªà¸¸à¸”\n",
    "best_threshold = 0.4675\n",
    "\n",
    "print(f\"Using Best Threshold: {best_threshold}\")\n",
    "\n",
    "# --- 4. à¸ªà¸£à¹‰à¸²à¸‡à¹à¸¥à¸°à¹€à¸—à¸£à¸™à¹‚à¸¡à¹€à¸”à¸¥ ---\n",
    "print(\"Training final champion model...\")\n",
    "final_model_lgbm = lgb.LGBMClassifier(\n",
    "    scale_pos_weight=scale_pos_weight, \n",
    "    **best_params\n",
    ")\n",
    "\n",
    "final_model_lgbm.fit(X_train, y_train)\n",
    "\n",
    "# --- 5. à¸›à¸£à¸°à¹€à¸¡à¸´à¸™à¸œà¸¥à¸šà¸™ Test Set (à¹‚à¸”à¸¢à¹ƒà¸Šà¹‰ Threshold à¸—à¸µà¹ˆà¸”à¸µà¸—à¸µà¹ˆà¸ªà¸¸à¸”) ---\n",
    "print(\"Evaluating model...\")\n",
    "# 5a. à¸”à¸¶à¸‡à¸„à¹ˆà¸²à¸„à¸§à¸²à¸¡à¸™à¹ˆà¸²à¸ˆà¸°à¹€à¸›à¹‡à¸™ (Probability)\n",
    "y_pred_probs = final_model_lgbm.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 5b. à¹ƒà¸Šà¹‰ Threshold à¸—à¸µà¹ˆà¸”à¸µà¸—à¸µà¹ˆà¸ªà¸¸à¸”à¹ƒà¸™à¸à¸²à¸£à¸•à¸±à¸”à¸ªà¸´à¸™à¹ƒà¸ˆ\n",
    "y_final_pred = (y_pred_probs >= best_threshold).astype(int)\n",
    "\n",
    "# 5c. à¸„à¸³à¸™à¸§à¸“à¸œà¸¥à¸¥à¸±à¸žà¸˜à¹Œ\n",
    "final_f1 = f1_score(y_test, y_final_pred)\n",
    "final_recall = recall_score(y_test, y_final_pred)\n",
    "final_precision = precision_score(y_test, y_final_pred)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"--- Final NEW Champion Model Results ---\")\n",
    "print(f\"   Best F1 Score:  {final_f1:.4f}\")\n",
    "print(f\"   Recall:         {final_recall:.4f}\")\n",
    "print(f\"   Precision:      {final_precision:.4f}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# --- 6. à¸žà¸´à¸¡à¸žà¹Œ classification report à¸‰à¸šà¸±à¸šà¹€à¸•à¹‡à¸¡ ---\n",
    "print(\"\\n--- Full Classification Report (at Optimal Threshold) ---\")\n",
    "print(classification_report(y_test, y_final_pred, digits=4))\n",
    "# -------------------------------------------------\n",
    "\n",
    "print(\"\\n--- Script Finished ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74970baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Full Classification Report (at Optimal Threshold) ---\")\n",
    "print(classification_report(y_test, y_final_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605bc40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import json\n",
    "joblib.dump(final_model_lgbm, '../model/lgbm/lgbm_model_final_smote.joblib')\n",
    "with open('../model/lgbm/lgbm_config.json','w')as f:\n",
    "    json.dump(\n",
    "        obj=final_model_lgbm.get_params(),\n",
    "        fp=f,\n",
    "        indent = 4\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58a992f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7257b6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from category_encoders import TargetEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"--- Running Optuna Search with CV (v5 - Improved) ---\")\n",
    "\n",
    "# --- 1. Load Data ---\n",
    "try:\n",
    "    train_df = pd.read_csv(\"../data/salary.train.processed.csv\")\n",
    "    test_df = pd.read_csv(\"../data/salary.test.processed.csv\")\n",
    "    print(\"Files loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Files not found. Make sure paths are correct.\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. Preprocess Data ---\n",
    "columns_to_drop = ['id', 'social-security-number', 'house-number', 'fnlwgt']\n",
    "target_column = 'label'\n",
    "\n",
    "# 2a. Apply Target Encoding for Categorical Features\n",
    "categorical_columns = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\n",
    "if any(col in train_df.columns for col in categorical_columns):\n",
    "    encoder = TargetEncoder(cols=[col for col in categorical_columns if col in train_df.columns])\n",
    "    train_df = encoder.fit_transform(train_df, train_df[target_column])\n",
    "    test_df = encoder.transform(test_df)\n",
    "\n",
    "# 2b. Scale Numerical Features\n",
    "numerical_columns = ['age', 'hours-per-week', 'capital-gain', 'capital-loss']\n",
    "scaler = StandardScaler()\n",
    "if any(col in train_df.columns for col in numerical_columns):\n",
    "    train_df[numerical_columns] = scaler.fit_transform(train_df[numerical_columns])\n",
    "    test_df[numerical_columns] = scaler.transform(test_df[numerical_columns])\n",
    "\n",
    "# 2c. Prepare Data Splits\n",
    "train_cols_to_drop = [col for col in columns_to_drop if col in train_df.columns]\n",
    "y_train_full = train_df[target_column]  # with 'label'\n",
    "X_train_full = train_df.drop(columns=train_cols_to_drop + [target_column]) # without label\n",
    "\n",
    "test_cols_to_drop = [col for col in columns_to_drop if col in test_df.columns]\n",
    "y_test = test_df[target_column]  # with 'label'\n",
    "X_test = test_df.drop(columns=test_cols_to_drop + [target_column]) # without label\n",
    "\n",
    "# 2d. Apply SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_full, y_train_full = smote.fit_resample(X_train_full, y_train_full)\n",
    "\n",
    "\n",
    "# 2e. Split for CV and Thresholding\n",
    "X_train_cv, X_val_thresh, y_train_cv, y_val_thresh = train_test_split(\n",
    "    X_train_full, y_train_full, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y_train_full\n",
    ")\n",
    "\n",
    "print(f\"Optuna CV set: {X_train_cv.shape[0]} samples\")\n",
    "print(f\"Threshold-finding set: {X_val_thresh.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# 2f. Calculate scale_pos_weight\n",
    "label_counts = y_train_full.value_counts()\n",
    "scale_pos_weight = label_counts[0] / label_counts[1]\n",
    "print(f\"Calculated scale_pos_weight: {scale_pos_weight:.4f}\")\n",
    "\n",
    "# --- 3. Define Optuna Objective Function ---\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.3, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 30),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'scale_pos_weight': trial.suggest_float('scale_pos_weight', 1.0, 3.0),\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    \n",
    "    max_n_estimators = trial.suggest_int('n_estimators', 100, 1000)\n",
    "    cv_folds = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    f1_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in cv_folds.split(X_train_cv, y_train_cv):\n",
    "        X_train_fold, X_val_fold = X_train_cv.iloc[train_idx], X_train_cv.iloc[val_idx]\n",
    "        y_train_fold, y_val_fold = y_train_cv.iloc[train_idx], y_train_cv.iloc[val_idx]\n",
    "        \n",
    "        model = lgb.LGBMClassifier(**params, n_estimators=max_n_estimators)\n",
    "        model.fit(\n",
    "            X_train_fold, y_train_fold,\n",
    "            eval_set=[(X_val_fold, y_val_fold)],\n",
    "            eval_metric='auc',\n",
    "            callbacks=[lgb.early_stopping(20, verbose=False)]\n",
    "        )\n",
    "        \n",
    "        y_val_probs = model.predict_proba(X_val_fold)[:, 1]\n",
    "        thresholds = np.arange(0.35, 0.50, 0.001)\n",
    "        best_f1 = 0\n",
    "        for thresh in thresholds:\n",
    "            preds = (y_val_probs >= thresh).astype(int)\n",
    "            f1 = f1_score(y_val_fold, preds)\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "        \n",
    "        f1_scores.append(best_f1)\n",
    "    \n",
    "    mean_f1 = np.mean(f1_scores)\n",
    "    trial.set_user_attr('best_n_estimators', model.best_iteration_)\n",
    "    return mean_f1\n",
    "\n",
    "# --- 4. Run Optuna Study ---\n",
    "print(\"\\n--- Starting Optuna optimization (this may take time)... ---\")\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "print(\"Optimization finished.\")\n",
    "print(f\"Best trial (Mean CV F1): {study.best_value:.4f}\")\n",
    "\n",
    "# --- 5. Get Best Parameters ---\n",
    "best_hyperparams = study.best_params\n",
    "best_n_estimators = study.best_trial.user_attrs['best_n_estimators']\n",
    "\n",
    "print(f\"Found Optimal n_estimators: {best_n_estimators}\")\n",
    "print(\"Found Best Hyperparameters:\")\n",
    "for key, value in best_hyperparams.items():\n",
    "    if key != 'n_estimators':\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# --- 6. Train Final Model ---\n",
    "print(\"\\n--- Training final model on ALL training data... ---\")\n",
    "final_params_for_model = {\n",
    "    **best_hyperparams,\n",
    "    'n_estimators': best_n_estimators,\n",
    "    'objective': 'binary',\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "final_model = lgb.LGBMClassifier(**final_params_for_model)\n",
    "final_model.fit(X_train_full, y_train_full)\n",
    "\n",
    "# --- 7. Find Best Threshold ---\n",
    "print(\"Finding optimal decision threshold (for F1) on held-out validation set...\")\n",
    "y_val_probs = final_model.predict_proba(X_val_thresh)[:, 1]\n",
    "thresholds = np.arange(0.35, 0.50, 0.001)\n",
    "best_f1 = 0\n",
    "best_threshold = 0.5\n",
    "\n",
    "for thresh in thresholds:\n",
    "    preds = (y_val_probs >= thresh).astype(int)\n",
    "    f1 = f1_score(y_val_thresh, preds)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = thresh\n",
    "\n",
    "print(f\"Best Threshold for F1: {best_threshold:.4f} (with F1: {best_f1:.4f} on validation data)\")\n",
    "\n",
    "# --- 8. Final Evaluation ---\n",
    "print(\"\\n--- Evaluating final model on TEST set... ---\")\n",
    "y_test_probs = final_model.predict_proba(X_test)[:, 1]\n",
    "y_final_pred = (y_test_probs >= best_threshold).astype(int)\n",
    "\n",
    "final_f1 = f1_score(y_test, y_final_pred)\n",
    "final_recall = recall_score(y_test, y_final_pred)\n",
    "final_precision = precision_score(y_test, y_final_pred)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"--- Final Model Results (Optuna + CV) ---\")\n",
    "print(f\"   Best F1 Score:  {final_f1:.4f}\")\n",
    "print(f\"   Recall:         {final_recall:.4f}\")\n",
    "print(f\"   Precision:      {final_precision:.4f}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\n--- Full Classification Report (at Optimal F1 Threshold) ---\")\n",
    "print(classification_report(y_test, y_final_pred, digits=4))\n",
    "\n",
    "print(\"\\n--- Script Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b233b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3be3d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
