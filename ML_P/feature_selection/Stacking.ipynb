{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cccaa1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the Stacking Classifier... (This will take a long time)\n",
      "Watch the console for progress updates like [Parallel(...)]:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 120\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining the Stacking Classifier... (This will take a long time)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWatch the console for progress updates like [Parallel(...)]:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 120\u001b[0m stacking_model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# --- Evaluate ---\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:63\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m extra_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_args)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extra_args \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# extra_args > 0\u001b[39;00m\n\u001b[0;32m     66\u001b[0m args_msg \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name, arg)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(kwonly_args[:extra_args], args[\u001b[38;5;241m-\u001b[39mextra_args:])\n\u001b[0;32m     69\u001b[0m ]\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_stacking.py:717\u001b[0m, in \u001b[0;36mStackingClassifier.fit\u001b[1;34m(self, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    716\u001b[0m     fit_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m sample_weight\n\u001b[1;32m--> 717\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(X, y_encoded, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_stacking.py:212\u001b[0m, in \u001b[0;36m_BaseStacking.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    207\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mappend(estimator)\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;66;03m# Fit the base estimators on the whole training data. Those\u001b[39;00m\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;66;03m# base estimators will be used in transform, predict, and\u001b[39;00m\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;66;03m# predict_proba. They are exposed publicly.\u001b[39;00m\n\u001b[1;32m--> 212\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_ \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)(\n\u001b[0;32m    213\u001b[0m         delayed(_fit_single_estimator)(\n\u001b[0;32m    214\u001b[0m             clone(est), X, y, routed_params[name][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    215\u001b[0m         )\n\u001b[0;32m    216\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m name, est \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(names, all_estimators)\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m est \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    218\u001b[0m     )\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamed_estimators_ \u001b[38;5;241m=\u001b[39m Bunch()\n\u001b[0;32m    221\u001b[0m est_fitted_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     76\u001b[0m )\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 1. Define Your Feature List\n",
    "# -----------------------------------------------------------------\n",
    "SELECTED_FEATURES = [\n",
    "    'marital-status_married-civ-spouse', 'relationship_husband',\n",
    "    'marital-status_never-married', 'education-num', 'capitalgain',\n",
    "    'age-group', 'relationship_own-child', 'hoursperweek', 'sex_male',\n",
    "    'sex_female', 'relationship_not-in-family', 'occupation_prof-specialty',\n",
    "    'occupation_other-service', 'relationship_unmarried',\n",
    "    'marital-status_divorced', 'capitalloss', 'occupation_exec-managerial',\n",
    "    'workclass_self-emp-inc', 'relationship_wife', 'workclass_private',\n",
    "    'race_black', 'race_white', 'relationship_other-relative',\n",
    "    'occupation_handlers-cleaners'\n",
    "]\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 2. Load and Prepare Data\n",
    "# -----------------------------------------------------------------\n",
    "try:\n",
    "    data_train = pd.read_csv('../data/salary.train.processed.csv').set_index('id')\n",
    "    data_test = pd.read_csv('../data/salary.test.processed.csv').set_index('id')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Training or Test file not found.\")\n",
    "    print(\"Please check the path: ../data/salary.train.processed.csv\")\n",
    "    # You might want to exit or handle this error appropriately\n",
    "    # For this example, we'll stop\n",
    "    exit()\n",
    "\n",
    "\n",
    "data_train = data_train[SELECTED_FEATURES + ['label']]\n",
    "data_test = data_test[SELECTED_FEATURES + ['label']]\n",
    "\n",
    "X_train = data_train[SELECTED_FEATURES]\n",
    "y_train = data_train['label']\n",
    "X_test = data_test[SELECTED_FEATURES]\n",
    "y_test = data_test['label']\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 3. Define Your Samplers\n",
    "# -----------------------------------------------------------------\n",
    "sampler_adasyn = ADASYN(random_state=42)\n",
    "sampler_smotetomek = SMOTETomek(random_state=42)\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 4. Define Your Best Base Models\n",
    "# -----------------------------------------------------------------\n",
    "params_rf = {\n",
    "    'n_estimators': 300, 'max_depth': 12, 'min_samples_leaf': 2, 'min_samples_split': 9,\n",
    "    'criterion': 'entropy', 'max_features': 0.42082357754585725, 'random_state': 42, 'n_jobs': -1\n",
    "}\n",
    "pipe_rf = Pipeline([('model', RandomForestClassifier(**params_rf))])\n",
    "\n",
    "params_lgbm = {\n",
    "    'learning_rate': 0.03713445949663834, 'num_leaves': 24, 'max_depth': 4,\n",
    "    'min_child_samples': 10, 'subsample': 0.8821481250200053,\n",
    "    'colsample_bytree': 0.7473748858313227, 'random_state': 42, 'n_jobs': -1\n",
    "}\n",
    "pipe_lgbm = Pipeline([('model', LGBMClassifier(**params_lgbm))])\n",
    "\n",
    "params_logreg = {\n",
    "    'C': 2.7420181030569966, 'penalty': 'elasticnet', 'l1_ratio': 0.9302376392883114,\n",
    "    'solver': 'saga', 'max_iter': 1000, 'random_state': 42\n",
    "}\n",
    "pipe_logreg = Pipeline([('sampler', sampler_adasyn), ('model', LogisticRegression(**params_logreg))])\n",
    "\n",
    "params_mlp = {\n",
    "    'hidden_layer_sizes': (226, 112, 185), 'activation': 'relu', 'alpha': 0.09988973301090445,\n",
    "    'learning_rate_init': 0.0003520144637184677, 'random_state': 42,\n",
    "    'max_iter': 500, 'early_stopping': True\n",
    "}\n",
    "pipe_mlp = Pipeline([('sampler', sampler_adasyn), ('model', MLPClassifier(**params_mlp))])\n",
    "\n",
    "params_cat = {\n",
    "    'iterations': 457, 'depth': 7, 'learning_rate': 0.013380005910139176,\n",
    "    'l2_leaf_reg': 0.06785680825867879, 'border_count': 237,\n",
    "    'random_strength': 0.004259876881997753, 'random_state': 42, 'verbose': 0\n",
    "}\n",
    "pipe_cat = Pipeline([('sampler', sampler_smotetomek), ('model', CatBoostClassifier(**params_cat))])\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 5. Define the Stacking Classifier\n",
    "# -----------------------------------------------------------------\n",
    "base_estimators = [\n",
    "    ('rf', pipe_rf),\n",
    "    ('lgbm', pipe_lgbm),\n",
    "    ('logreg_adasyn', pipe_logreg),\n",
    "    ('mlp_adasyn', pipe_mlp),\n",
    "    ('cat_smotetomek', pipe_cat)\n",
    "]\n",
    "\n",
    "meta_model = LogisticRegression(random_state=42)\n",
    "\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=base_estimators,\n",
    "    final_estimator=meta_model,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=11,  # <-- SET TO A HIGH NUMBER FOR MAX PROGRESS UPDATES\n",
    "    passthrough=False\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 6. Train and Evaluate\n",
    "# -----------------------------------------------------------------\n",
    "print(\"Training the Stacking Classifier... (This will take a long time)\")\n",
    "print(\"Watch the console for progress updates like [Parallel(...)]:\")\n",
    "\n",
    "stacking_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "\n",
    "# --- Evaluate ---\n",
    "y_pred_stack = stacking_model.predict(X_test)\n",
    "y_pred_proba_stack = stacking_model.predict_proba(X_test)\n",
    "\n",
    "print(\"\\n--- Stacking Model Performance ---\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_stack):.4f}\")\n",
    "print(classification_report(y_test, y_pred_stack,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1628c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing data...\n",
      "Data loaded successfully. Training with 56 features.\n",
      "\n",
      "Training the Stacking Classifier on 56 features...\n",
      "This will take a long time. Watch for [Parallel(...)] messages.\n",
      "\n",
      "Training complete! Total time: 2.19 minutes\n",
      "\n",
      "--- Stacking Model Performance (4 Digits) ---\n",
      "Accuracy: 0.8280\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8631    0.8349    0.8487      2416\n",
      "         1.0     0.7835    0.8186    0.8007      1764\n",
      "\n",
      "    accuracy                         0.8280      4180\n",
      "   macro avg     0.8233    0.8267    0.8247      4180\n",
      "weighted avg     0.8295    0.8280    0.8284      4180\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import time\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 1. Load and Prepare Data (No Feature Selection)\n",
    "# -----------------------------------------------------------------\n",
    "print(\"Loading and preparing data...\")\n",
    "try:\n",
    "    data_train = pd.read_csv('../data/salary.train.processed.csv').set_index('id')\n",
    "    data_test = pd.read_csv('../data/salary.test.processed.csv').set_index('id')\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Please check the path to your data files.\")\n",
    "    exit()\n",
    "\n",
    "# Define X and y from the full loaded data\n",
    "# X_train will be all columns EXCEPT 'label'\n",
    "X_train = data_train.drop('label', axis=1)\n",
    "y_train = data_train['label']\n",
    "\n",
    "# X_test will be all columns EXCEPT 'label'\n",
    "X_test = data_test.drop('label', axis=1)\n",
    "y_test = data_test['label']\n",
    "\n",
    "print(f\"Data loaded successfully. Training with {X_train.shape[1]} features.\")\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 2. Define Your Samplers\n",
    "# -----------------------------------------------------------------\n",
    "sampler_adasyn = ADASYN(random_state=42)\n",
    "sampler_smotetomek = SMOTETomek(random_state=42)\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 3. Define Your Best Base Models\n",
    "# -----------------------------------------------------------------\n",
    "params_rf = {\n",
    "    'n_estimators': 300, 'max_depth': 12, 'min_samples_leaf': 2, 'min_samples_split': 9,\n",
    "    'criterion': 'entropy', 'max_features': 0.42082357754585725, 'random_state': 42, 'n_jobs': -1\n",
    "}\n",
    "pipe_rf = Pipeline([('model', RandomForestClassifier(**params_rf))])\n",
    "\n",
    "params_lgbm = {\n",
    "    'learning_rate': 0.03713445949663834, 'num_leaves': 24, 'max_depth': 4,\n",
    "    'min_child_samples': 10, 'subsample': 0.8821481250200053,\n",
    "    'colsample_bytree': 0.7473748858313227, 'random_state': 42, 'n_jobs': -1, 'verbose': -1\n",
    "}\n",
    "pipe_lgbm = Pipeline([('model', LGBMClassifier(**params_lgbm))])\n",
    "\n",
    "params_logreg = {\n",
    "    'C': 2.7420181030569966, 'penalty': 'elasticnet', 'l1_ratio': 0.9302376392883114,\n",
    "    'solver': 'saga', 'max_iter': 1000, 'random_state': 42\n",
    "}\n",
    "pipe_logreg = Pipeline([('sampler', sampler_adasyn), ('model', LogisticRegression(**params_logreg))])\n",
    "\n",
    "params_mlp = {\n",
    "    'hidden_layer_sizes': (226, 112, 185), 'activation': 'relu', 'alpha': 0.09988973301090445,\n",
    "    'learning_rate_init': 0.0003520144637184677, 'random_state': 42,\n",
    "    'max_iter': 500, 'early_stopping': True\n",
    "}\n",
    "pipe_mlp = Pipeline([('sampler', sampler_adasyn), ('model', MLPClassifier(**params_mlp))])\n",
    "\n",
    "params_cat = {\n",
    "    'iterations': 457, 'depth': 7, 'learning_rate': 0.013380005910139176,\n",
    "    'l2_leaf_reg': 0.06785680825867879, 'border_count': 237,\n",
    "    'random_strength': 0.004259876881997753, 'random_state': 42, 'verbose': 0\n",
    "}\n",
    "pipe_cat = Pipeline([('sampler', sampler_smotetomek), ('model', CatBoostClassifier(**params_cat))])\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 4. Define the Stacking Classifier\n",
    "# -----------------------------------------------------------------\n",
    "base_estimators = [\n",
    "    ('rf', pipe_rf),\n",
    "    ('lgbm', pipe_lgbm),\n",
    "    ('logreg_adasyn', pipe_logreg),\n",
    "    ('mlp_adasyn', pipe_mlp),\n",
    "    ('cat_smotetomek', pipe_cat)\n",
    "]\n",
    "\n",
    "meta_model = LogisticRegression(random_state=42)\n",
    "\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=base_estimators,\n",
    "    final_estimator=meta_model,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=11,  # This will print text updates like [Parallel(..): Done 2 of 5 ..]\n",
    "    passthrough=False\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 5. Train and Evaluate\n",
    "# -----------------------------------------------------------------\n",
    "print(f\"\\nTraining the Stacking Classifier on {X_train.shape[1]} features...\")\n",
    "print(\"This will take a long time. Watch for [Parallel(...)] messages.\")\n",
    "\n",
    "start_time = time.time()\n",
    "stacking_model.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"\\nTraining complete! Total time: {(end_time - start_time) / 60:.2f} minutes\")\n",
    "\n",
    "# --- Evaluate ---\n",
    "y_pred_stack = stacking_model.predict(X_test)\n",
    "y_pred_proba_stack = stacking_model.predict_proba(X_test)\n",
    "\n",
    "print(\"\\n--- Stacking Model Performance (4 Digits) ---\")\n",
    "\n",
    "# Print Accuracy formatted to 4 decimal places\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_stack):.4f}\")\n",
    "\n",
    "# Print Classification Report formatted to 4 decimal places\n",
    "print(classification_report(y_test, y_pred_stack, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3b2cfdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing data...\n",
      "Data loaded successfully. Training with 24 selected features.\n",
      "\n",
      "Training Stack (Top 3) with Feature Selection (24 features)...\n",
      "\n",
      "Training complete! Total time: 0.53 minutes\n",
      "\n",
      "--- Stack (Top 3) + Feature Selection Performance (4 Digits) ---\n",
      "Accuracy: 0.8244\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8625    0.8282    0.8450      2416\n",
      "         1.0     0.7769    0.8192    0.7975      1764\n",
      "\n",
      "    accuracy                         0.8244      4180\n",
      "   macro avg     0.8197    0.8237    0.8212      4180\n",
      "weighted avg     0.8264    0.8244    0.8249      4180\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import time\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 1. PASTE Your Feature List Here\n",
    "# -----------------------------------------------------------------\n",
    "SELECTED_FEATURES = [\n",
    "    'marital-status_married-civ-spouse', 'relationship_husband',\n",
    "    'marital-status_never-married', 'education-num', 'capitalgain',\n",
    "    'age-group', 'relationship_own-child', 'hoursperweek', 'sex_male',\n",
    "    'sex_female', 'relationship_not-in-family', 'occupation_prof-specialty',\n",
    "    'occupation_other-service', 'relationship_unmarried',\n",
    "    'marital-status_divorced', 'capitalloss', 'occupation_exec-managerial',\n",
    "    'workclass_self-emp-inc', 'relationship_wife', 'workclass_private',\n",
    "    'race_black', 'race_white', 'relationship_other-relative',\n",
    "    'occupation_handlers-cleaners'\n",
    "]\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 2. Load and Prepare Data\n",
    "# -----------------------------------------------------------------\n",
    "print(\"Loading and preparing data...\")\n",
    "try:\n",
    "    data_train = pd.read_csv('../data/salary.train.processed.csv').set_index('id')\n",
    "    data_test = pd.read_csv('../data/salary.test.processed.csv').set_index('id')\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Filter data to ONLY your selected features\n",
    "X_train = data_train[SELECTED_FEATURES]\n",
    "y_train = data_train['label']\n",
    "X_test = data_test[SELECTED_FEATURES]\n",
    "y_test = data_test['label']\n",
    "\n",
    "print(f\"Data loaded successfully. Training with {X_train.shape[1]} selected features.\")\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 3. Define Your Sampler\n",
    "# -----------------------------------------------------------------\n",
    "sampler_adasyn = ADASYN(random_state=42)\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 4. Define Your \"A-List\" Models (using your params)\n",
    "# -----------------------------------------------------------------\n",
    "params_rf = {\n",
    "    'n_estimators': 300, 'max_depth': 12, 'min_samples_leaf': 2, 'min_samples_split': 9,\n",
    "    'criterion': 'entropy', 'max_features': 0.42082357754585725, 'random_state': 42, 'n_jobs': -1\n",
    "}\n",
    "pipe_rf = Pipeline([('model', RandomForestClassifier(**params_rf))])\n",
    "\n",
    "params_lgbm = {\n",
    "    'learning_rate': 0.03713445949663834, 'num_leaves': 24, 'max_depth': 4,\n",
    "    'min_child_samples': 10, 'subsample': 0.8821481250200053,\n",
    "    'colsample_bytree': 0.7473748858313227, 'random_state': 42, 'n_jobs': -1, 'verbose': -1\n",
    "}\n",
    "pipe_lgbm = Pipeline([('model', LGBMClassifier(**params_lgbm))])\n",
    "\n",
    "params_logreg = {\n",
    "    'C': 2.7420181030569966, 'penalty': 'elasticnet', 'l1_ratio': 0.9302376392883114,\n",
    "    'solver': 'saga', 'max_iter': 1000, 'random_state': 42\n",
    "}\n",
    "pipe_logreg = Pipeline([('sampler', sampler_adasyn), ('model', LogisticRegression(**params_logreg))])\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 5. Define the Stacking Classifier\n",
    "# -----------------------------------------------------------------\n",
    "base_estimators = [\n",
    "    ('rf', pipe_rf),\n",
    "    ('lgbm', pipe_lgbm),\n",
    "    ('logreg_adasyn', pipe_logreg)\n",
    "]\n",
    "\n",
    "meta_model = LogisticRegression(random_state=42)\n",
    "\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=base_estimators,\n",
    "    final_estimator=meta_model,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=11,\n",
    "    passthrough=False # Meta-model only sees predictions\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# 6. Train and Evaluate\n",
    "# -----------------------------------------------------------------\n",
    "print(f\"\\nTraining Stack (Top 3) with Feature Selection ({X_train.shape[1]} features)...\")\n",
    "start_time = time.time()\n",
    "stacking_model.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "print(f\"\\nTraining complete! Total time: {(end_time - start_time) / 60:.2f} minutes\")\n",
    "\n",
    "y_pred_stack = stacking_model.predict(X_test)\n",
    "print(f\"\\n--- Stack (Top 3) + Feature Selection Performance (4 Digits) ---\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_stack):.4f}\")\n",
    "print(classification_report(y_test, y_pred_stack, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d224d2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing data...\n",
      "Data loaded successfully. Training with 24 selected features.\n",
      "Training VotingClassifier (RF + LGBM) with Feature Selection + SMOTETomek...\n",
      "\n",
      "Training complete! Total time: 0.14 minutes\n",
      "\n",
      "--- VOTING Model Performance (4 Digits) ---\n",
      "Accuracy: 0.8220\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8881    0.7918    0.8372      2416\n",
      "         1.0     0.7517    0.8634    0.8037      1764\n",
      "\n",
      "    accuracy                         0.8220      4180\n",
      "   macro avg     0.8199    0.8276    0.8204      4180\n",
      "weighted avg     0.8306    0.8220    0.8231      4180\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import time\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# --- THE FIX IS HERE ---\n",
    "# We must use the Pipeline from imblearn, NOT from sklearn\n",
    "from imblearn.pipeline import Pipeline \n",
    "from imblearn.combine import SMOTETomek\n",
    "# -------------------------\n",
    "\n",
    "# --- Use your 24 selected features ---\n",
    "SELECTED_FEATURES = [\n",
    "    'marital-status_married-civ-spouse', 'relationship_husband',\n",
    "    'marital-status_never-married', 'education-num', 'capitalgain',\n",
    "    'age-group', 'relationship_own-child', 'hoursperweek', 'sex_male',\n",
    "    'sex_female', 'relationship_not-in-family', 'occupation_prof-specialty',\n",
    "    'occupation_other-service', 'relationship_unmarried',\n",
    "    'marital-status_divorced', 'capitalloss', 'occupation_exec-managerial',\n",
    "    'workclass_self-emp-inc', 'relationship_wife', 'workclass_private',\n",
    "    'race_black', 'race_white', 'relationship_other-relative',\n",
    "    'occupation_handlers-cleaners'\n",
    "]\n",
    "\n",
    "# --- Load and filter data ---\n",
    "print(\"Loading and preparing data...\")\n",
    "try:\n",
    "    data_train = pd.read_csv('../data/salary.train.processed.csv').set_index('id')\n",
    "    data_test = pd.read_csv('../data/salary.test.processed.csv').set_index('id')\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Filter data to ONLY your selected features\n",
    "X_train_filtered = data_train[SELECTED_FEATURES]\n",
    "y_train = data_train['label']\n",
    "X_test_filtered = data_test[SELECTED_FEATURES]\n",
    "y_test = data_test['label']\n",
    "\n",
    "print(f\"Data loaded successfully. Training with {X_train_filtered.shape[1]} selected features.\")\n",
    "\n",
    "# --- Define your sampler ---\n",
    "sampler_smotetomek = SMOTETomek(random_state=42)\n",
    "\n",
    "# --- Define Your Top 2 Models with Best Params ---\n",
    "params_rf = {\n",
    "    'n_estimators': 300, 'max_depth': 12, 'min_samples_leaf': 2, 'min_samples_split': 9,\n",
    "    'criterion': 'entropy', 'max_features': 0.42082357754585725, 'random_state': 42, 'n_jobs': -1\n",
    "}\n",
    "pipe_rf = Pipeline([\n",
    "    ('sampler', sampler_smotetomek), # Using the sampler that gave you the best RF score\n",
    "    ('model', RandomForestClassifier(**params_rf))\n",
    "])\n",
    "\n",
    "params_lgbm = {\n",
    "    'learning_rate': 0.03713445949663834, 'num_leaves': 24, 'max_depth': 4,\n",
    "    'min_child_samples': 10, 'subsample': 0.8821481250200053,\n",
    "    'colsample_bytree': 0.7473748858313227, 'random_state': 42, 'n_jobs': -1, 'verbose': -1\n",
    "}\n",
    "pipe_lgbm = Pipeline([\n",
    "    ('sampler', sampler_smotetomek),\n",
    "    ('model', LGBMClassifier(**params_lgbm))\n",
    "])\n",
    "\n",
    "# --- Create the Voting Classifier ---\n",
    "voting_model = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', pipe_rf),\n",
    "        ('lgbm', pipe_lgbm)\n",
    "    ],\n",
    "    voting='soft',\n",
    "    n_jobs=-1,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# --- Train and Evaluate ---\n",
    "print(\"Training VotingClassifier (RF + LGBM) with Feature Selection + SMOTETomek...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# This .fit() call should work now\n",
    "voting_model.fit(X_train_filtered, y_train)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\nTraining complete! Total time: {(end_time - start_time) / 60:.2f} minutes\")\n",
    "\n",
    "y_pred_vote = voting_model.predict(X_test_filtered)\n",
    "\n",
    "print(\"\\n--- VOTING Model Performance (4 Digits) ---\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_vote):.4f}\")\n",
    "print(classification_report(y_test, y_pred_vote, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f97937b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
