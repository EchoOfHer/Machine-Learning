{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dacb6c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the MLP Classifier...\n",
      "Iteration 1, loss = 0.52827667\n",
      "Validation score: 0.811005\n",
      "Iteration 2, loss = 0.39811452\n",
      "Validation score: 0.808612\n",
      "Iteration 3, loss = 0.38356991\n",
      "Validation score: 0.815191\n",
      "Iteration 4, loss = 0.37720204\n",
      "Validation score: 0.815789\n",
      "Iteration 5, loss = 0.37307379\n",
      "Validation score: 0.818182\n",
      "Iteration 6, loss = 0.36958425\n",
      "Validation score: 0.815789\n",
      "Iteration 7, loss = 0.36655434\n",
      "Validation score: 0.811603\n",
      "Iteration 8, loss = 0.36371961\n",
      "Validation score: 0.814593\n",
      "Iteration 9, loss = 0.36154710\n",
      "Validation score: 0.809809\n",
      "Iteration 10, loss = 0.35873219\n",
      "Validation score: 0.815789\n",
      "Iteration 11, loss = 0.35707486\n",
      "Validation score: 0.812799\n",
      "Iteration 12, loss = 0.35470151\n",
      "Validation score: 0.812201\n",
      "Iteration 13, loss = 0.35336092\n",
      "Validation score: 0.814593\n",
      "Iteration 14, loss = 0.35042624\n",
      "Validation score: 0.812799\n",
      "Iteration 15, loss = 0.34902052\n",
      "Validation score: 0.810407\n",
      "Iteration 16, loss = 0.34739592\n",
      "Validation score: 0.811603\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training complete.\n",
      "\n",
      "MLP Accuracy: 0.8230\n",
      "\n",
      "MLP Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8474    0.8460    0.8467      2416\n",
      "         1.0     0.7896    0.7914    0.7905      1764\n",
      "\n",
      "    accuracy                         0.8230      4180\n",
      "   macro avg     0.8185    0.8187    0.8186      4180\n",
      "weighted avg     0.8230    0.8230    0.8230      4180\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# --- 1. Load Your Data (You already did this) ---\n",
    "datatrain_df = pd.read_csv('./data/salary.train.processed.csv', index_col='id')\n",
    "test_df = pd.read_csv('./data/salary.test.processed.csv', index_col='id')\n",
    "\n",
    "# --- 2. Separate Features (X) and Target (y) ---\n",
    "# --- IMPORTANT: Change 'target' to your actual target column name! ---\n",
    "target_column = 'label' # Or 'salary', 'income', etc.\n",
    "\n",
    "X_train = datatrain_df.drop(target_column, axis=1)\n",
    "y_train = datatrain_df[target_column]\n",
    "\n",
    "X_test = test_df.drop(target_column, axis=1)\n",
    "y_test = test_df[target_column]\n",
    "\n",
    "# --- 3. CRITICAL STEP: Feature Scaling ---\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler ONLY on the training data\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Transform both training and testing data\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --- 4. Initialize and Train the MLP/DNN ---\n",
    "print(\"Training the MLP Classifier...\")\n",
    "\n",
    "# This creates a network with 2 hidden layers: one with 64 neurons, one with 32\n",
    "mlp_model = MLPClassifier(\n",
    "    hidden_layer_sizes=(64, 32),  # The architecture of your network\n",
    "    max_iter=1000,               # Max epochs (passes through data)\n",
    "    early_stopping=True,         # Stops training when validation score stops improving\n",
    "    random_state=42,             # For reproducible results\n",
    "    verbose=True                 # Set to True to see training progress\n",
    ")\n",
    "\n",
    "# Train the model on the SCALED data\n",
    "mlp_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# --- 5. Evaluate the Model ---\n",
    "# Make predictions on the SCALED test data\n",
    "y_pred_mlp = mlp_model.predict(X_test_scaled)\n",
    "\n",
    "# Print the results\n",
    "print(f\"\\nMLP Accuracy: {accuracy_score(y_test, y_pred_mlp):.4f}\")\n",
    "print(\"\\nMLP Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_mlp, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11970a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-22 07:13:44,310] A new study created in memory with name: no-name-f0369536-b741-4225-b59b-5e1b3637bcf9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Optuna hyperparameter search...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd0fd688dd2b4a2ea21f012d7d4f6e7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-22 07:13:54,013] Trial 0 finished with value: 0.807894921464363 and parameters: {'n_layers': 3, 'n_units_l0': 46, 'n_units_l1': 50, 'n_units_l2': 89, 'activation': 'tanh', 'solver': 'sgd', 'alpha': 0.0004629269617008387, 'learning_rate_init': 0.002625198512757082}. Best is trial 0 with value: 0.807894921464363.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\natth\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:697: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import optuna\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# --- 1. Load Your Data ---\n",
    "datatrain_df = pd.read_csv('./data/salary.train.processed.csv', index_col='id')\n",
    "test_df = pd.read_csv('./data/salary.test.processed.csv', index_col='id')\n",
    "\n",
    "# --- 2. Separate Features (X) and Target (y) ---\n",
    "# --- IMPORTANT: Change 'target' to your actual target column name! ---\n",
    "target_column = 'label' # Or 'salary', 'income', etc.\n",
    "\n",
    "X_train = datatrain_df.drop(target_column, axis=1)\n",
    "y_train = datatrain_df[target_column]\n",
    "\n",
    "X_test = test_df.drop(target_column, axis=1)\n",
    "y_test = test_df[target_column]\n",
    "\n",
    "# --- 3. CRITICAL STEP: Feature Scaling ---\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --- 4. Define the Optuna Objective Function ---\n",
    "# This function will be called once per trial\n",
    "def objective(trial):\n",
    "    # --- Define the Hyperparameter Search Space ---\n",
    "    \n",
    "    # 1. Number of hidden layers (e.g., 1, 2, or 3)\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 3)\n",
    "    \n",
    "    # 2. Size of each hidden layer\n",
    "    layers = []\n",
    "    for i in range(n_layers):\n",
    "        # Suggest a number of neurons (e.g., 16 to 128)\n",
    "        layers.append(trial.suggest_int(f'n_units_l{i}', 16, 128))\n",
    "    \n",
    "    hidden_layer_sizes = tuple(layers)\n",
    "    \n",
    "    # 3. Activation function\n",
    "    activation = trial.suggest_categorical('activation', ['relu', 'tanh'])\n",
    "    \n",
    "    # 4. Solver (Algorithm for weight optimization)\n",
    "    solver = trial.suggest_categorical('solver', ['adam', 'sgd'])\n",
    "    \n",
    "    # 5. Regularization strength (helps prevent overfitting)\n",
    "    # --- CHANGED: Using suggest_float as per the warning ---\n",
    "    alpha = trial.suggest_float('alpha', 1e-5, 1e-1, log=True)\n",
    "    \n",
    "    # 6. Initial learning rate (only for 'sgd' or 'adam')\n",
    "    # --- CHANGED: Using suggest_float as per the warning ---\n",
    "    learning_rate_init = trial.suggest_float('learning_rate_init', 1e-4, 1e-2, log=True)\n",
    "\n",
    "    # --- Create the Model ---\n",
    "    model = MLPClassifier(\n",
    "        hidden_layer_sizes=hidden_layer_sizes,\n",
    "        activation=activation,\n",
    "        solver=solver,\n",
    "        alpha=alpha,\n",
    "        learning_rate_init=learning_rate_init,\n",
    "        max_iter=500,  # Give it enough time to converge\n",
    "        early_stopping=True, # Good practice for individual trials\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # --- Evaluate the Model ---\n",
    "    # We use 3-fold cross-validation on the training data.\n",
    "    # This gives a more stable score than a single train/validation split.\n",
    "    score = cross_val_score(model, X_train_scaled, y_train, cv=3, scoring='accuracy')\n",
    "    \n",
    "    # Return the mean accuracy from the cross-validation\n",
    "    return score.mean()\n",
    "\n",
    "# --- 5. Run the Optuna Study ---\n",
    "print(\"Starting Optuna hyperparameter search...\")\n",
    "\n",
    "# We want to MAXIMIZE accuracy\n",
    "study = optuna.create_study(direction='maximize')\n",
    "\n",
    "# Run 100 trials, and show the progress bar!\n",
    "study.optimize(\n",
    "    objective, \n",
    "    n_trials=100, \n",
    "    show_progress_bar=True  # This enables the progress bar\n",
    ")\n",
    "\n",
    "print(\"\\nSearch complete.\")\n",
    "print(f\"Best trial (accuracy): {study.best_value:.4f}\")\n",
    "print(\"Best hyperparameters found:\")\n",
    "print(study.best_params)\n",
    "\n",
    "# --- 6. Train the FINAL Model with the Best Params ---\n",
    "print(\"\\nTraining final model with best hyperparameters...\")\n",
    "\n",
    "# Get a copy of the best parameters\n",
    "best_params = study.best_params.copy()\n",
    "\n",
    "# --- Reconstruct the hidden_layer_sizes tuple from the best params ---\n",
    "# 1. Get the number of layers and REMOVE it from the dictionary\n",
    "n_layers = best_params.pop('n_layers')\n",
    "\n",
    "# 2. Build the layers list by REMOVING each n_units_l{i} key\n",
    "layers = []\n",
    "for i in range(n_layers):\n",
    "    layer_size = best_params.pop(f'n_units_l{i}')\n",
    "    layers.append(layer_size)\n",
    "\n",
    "# 3. Create the final tuple that MLPClassifier understands\n",
    "final_hidden_layer_sizes = tuple(layers)\n",
    "\n",
    "# Now, 'best_params' only contains keys that MLPClassifier accepts\n",
    "# (e.g., 'activation', 'solver', 'alpha')\n",
    "\n",
    "# Create a new MLP model using the reconstructed tuple and the rest of the params\n",
    "final_mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=final_hidden_layer_sizes, # Pass the tuple we just built\n",
    "    max_iter=1000, \n",
    "    early_stopping=True,\n",
    "    random_state=42,\n",
    "    **best_params # This unpacks the *cleaned* dictionary\n",
    ")\n",
    "\n",
    "# Train on the FULL scaled training set\n",
    "final_mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "# --- 7. Evaluate on the Test Set ---\n",
    "y_pred_final = final_mlp.predict(X_test_scaled)\n",
    "\n",
    "print(f\"\\nFinal Model Accuracy on Test Set: {accuracy_score(y_test, y_pred_final):.4f}\")\n",
    "print(\"\\nFinal Model Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_final,digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c0b114",
   "metadata": {},
   "source": [
    "### ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af450bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 4. Testing MLPClassifier with ADASYN ---\n",
      "Applying ADASYN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\natth\\anaconda3\\Lib\\site-packages\\imblearn\\over_sampling\\_adasyn.py:156: FutureWarning: The parameter `n_jobs` has been deprecated in 0.10 and will be removed in 0.12. You can pass an nearest neighbors estimator where `n_jobs` is already set instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\natth\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] The system cannot find the file specified\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"c:\\Users\\natth\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\natth\\anaconda3\\Lib\\subprocess.py\", line 548, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\natth\\anaconda3\\Lib\\subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"c:\\Users\\natth\\anaconda3\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New resampled label distribution:\n",
      "label\n",
      "1.0    9726\n",
      "0.0    9719\n",
      "Name: count, dtype: int64\n",
      "Model training complete.\n",
      "\n",
      "MLPClassifier (Tuned + ADASYN) Report:\n",
      "              precision    recall  f1-score     support\n",
      "0.0            0.842930  0.757450  0.797907  2416.00000\n",
      "1.0            0.708313  0.806689  0.754307  1764.00000\n",
      "accuracy       0.778230  0.778230  0.778230     0.77823\n",
      "macro avg      0.775621  0.782070  0.776107  4180.00000\n",
      "weighted avg   0.786120  0.778230  0.779507  4180.00000\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "print(\"\\n--- 4. Testing MLPClassifier with ADASYN ---\")\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    data_train_full = pandas.read_csv('./data/salary.train.processed.csv').set_index('id')\n",
    "    data_test_mlp = pandas.read_csv('./data/salary.test.processed.csv').set_index('id')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: ไม่พบไฟล์ salary.train.processed.csv กรุณาตรวจสอบ path\")\n",
    "    # exit() \n",
    "\n",
    "X_full = data_train_full.drop(['label'], axis='columns')\n",
    "y_full = data_train_full['label']\n",
    "X_test = data_test_mlp.drop(['label'], axis='columns')\n",
    "y_test = data_test_mlp['label']\n",
    "\n",
    "# --- Apply ADASYN ---\n",
    "print(\"Applying ADASYN...\")\n",
    "ada = ADASYN(random_state=42, n_jobs=-1)\n",
    "X_resampled, y_resampled = ada.fit_resample(X_full, y_full)\n",
    "print(f\"New resampled label distribution:\\n{y_resampled.value_counts()}\")\n",
    "\n",
    "# --- Define Parameters (Translated) ---\n",
    "best_mlp_params = {\n",
    "    'hidden_layer_sizes': (99,),\n",
    "    'activation': 'relu',\n",
    "    'solver': 'adam',\n",
    "    'alpha': 0.0009771293502957021,\n",
    "    'learning_rate_init': 0.0005471619343332291\n",
    "}\n",
    "\n",
    "# --- Create and Train Pipeline ---\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()), # Step 1: Scale\n",
    "    ('model', MLPClassifier(\n",
    "        **best_mlp_params,\n",
    "        # ⚠️ NO 'class_weight'\n",
    "        random_state=42,\n",
    "        max_iter=1000\n",
    "    ))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_resampled, y_resampled) # Train on ADASYN data\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# --- Evaluate ---\n",
    "y_pred = pipeline.predict(X_test)\n",
    "report = classification_report(y_test, y_pred, digits=6, output_dict=True)\n",
    "df_report = pandas.DataFrame(report).transpose()\n",
    "\n",
    "print(\"\\nMLPClassifier (Tuned + ADASYN) Report:\")\n",
    "print(df_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cfd3e7",
   "metadata": {},
   "source": [
    "### SMOTETomek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab9a677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 3. Testing MLPClassifier with SMOTETomek ---\n",
      "Applying SMOTETomek...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\natth\\anaconda3\\Lib\\site-packages\\imblearn\\over_sampling\\_smote\\base.py:370: FutureWarning: The parameter `n_jobs` has been deprecated in 0.10 and will be removed in 0.12. You can pass an nearest neighbors estimator where `n_jobs` is already set instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New resampled label distribution:\n",
      "label\n",
      "1.0    8914\n",
      "0.0    8914\n",
      "Name: count, dtype: int64\n",
      "Model training complete.\n",
      "\n",
      "MLPClassifier (Tuned + SMOTETomek) Report:\n",
      "              precision    recall  f1-score      support\n",
      "0.0            0.838025  0.800911  0.819048  2416.000000\n",
      "1.0            0.742918  0.787982  0.764787  1764.000000\n",
      "accuracy       0.795455  0.795455  0.795455     0.795455\n",
      "macro avg      0.790472  0.794446  0.791917  4180.000000\n",
      "weighted avg   0.797889  0.795455  0.796149  4180.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "print(\"\\n--- 3. Testing MLPClassifier with SMOTETomek ---\")\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    data_train_full = pandas.read_csv('./data/salary.train.processed.csv').set_index('id')\n",
    "    data_test_mlp = pandas.read_csv('./data/salary.test.processed.csv').set_index('id')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: ไม่พบไฟล์ salary.train.processed.csv กรุณาตรวจสอบ path\")\n",
    "    # exit() \n",
    "\n",
    "X_full = data_train_full.drop(['label'], axis='columns')\n",
    "y_full = data_train_full['label']\n",
    "X_test = data_test_mlp.drop(['label'], axis='columns')\n",
    "y_test = data_test_mlp['label']\n",
    "\n",
    "# --- Apply SMOTETomek ---\n",
    "print(\"Applying SMOTETomek...\")\n",
    "smt = SMOTETomek(random_state=42, n_jobs=-1)\n",
    "X_resampled, y_resampled = smt.fit_resample(X_full, y_full)\n",
    "print(f\"New resampled label distribution:\\n{y_resampled.value_counts()}\")\n",
    "\n",
    "# --- Define Parameters (Translated) ---\n",
    "best_mlp_params = {\n",
    "    'hidden_layer_sizes': (99,),\n",
    "    'activation': 'relu',\n",
    "    'solver': 'adam',\n",
    "    'alpha': 0.0009771293502957021,\n",
    "    'learning_rate_init': 0.0005471619343332291\n",
    "}\n",
    "\n",
    "# --- Create and Train Pipeline ---\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()), # Step 1: Scale\n",
    "    ('model', MLPClassifier(\n",
    "        **best_mlp_params,\n",
    "        # ⚠️ NO 'class_weight'\n",
    "        random_state=42,\n",
    "        max_iter=1000\n",
    "    ))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_resampled, y_resampled) # Train on SMOTETomek data\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# --- Evaluate ---\n",
    "y_pred = pipeline.predict(X_test)\n",
    "report = classification_report(y_test, y_pred, digits=6, output_dict=True)\n",
    "df_report = pandas.DataFrame(report).transpose()\n",
    "\n",
    "print(\"\\nMLPClassifier (Tuned + SMOTETomek) Report:\")\n",
    "print(df_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe867d8c",
   "metadata": {},
   "source": [
    "### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b68700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 2. Testing MLPClassifier with SMOTE ---\n",
      "Applying SMOTE...\n",
      "New resampled label distribution:\n",
      "label\n",
      "1.0    9719\n",
      "0.0    9719\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\natth\\anaconda3\\Lib\\site-packages\\imblearn\\over_sampling\\_smote\\base.py:370: FutureWarning: The parameter `n_jobs` has been deprecated in 0.10 and will be removed in 0.12. You can pass an nearest neighbors estimator where `n_jobs` is already set instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training complete.\n",
      "\n",
      "MLPClassifier (Tuned + SMOTE) Report:\n",
      "              precision    recall  f1-score      support\n",
      "0.0            0.839664  0.786838  0.812393  2416.000000\n",
      "1.0            0.731211  0.794218  0.761413  1764.000000\n",
      "accuracy       0.789952  0.789952  0.789952     0.789952\n",
      "macro avg      0.785438  0.790528  0.786903  4180.000000\n",
      "weighted avg   0.793896  0.789952  0.790879  4180.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "print(\"\\n--- 2. Testing MLPClassifier with SMOTE ---\")\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    data_train_full = pandas.read_csv('./data/salary.train.processed.csv').set_index('id')\n",
    "    data_test_mlp = pandas.read_csv('./data/salary.test.processed.csv').set_index('id')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: ไม่พบไฟล์ salary.train.processed.csv กรุณาตรวจสอบ path\")\n",
    "    # exit() \n",
    "\n",
    "X_full = data_train_full.drop(['label'], axis='columns')\n",
    "y_full = data_train_full['label']\n",
    "X_test = data_test_mlp.drop(['label'], axis='columns')\n",
    "y_test = data_test_mlp['label']\n",
    "\n",
    "# --- Apply SMOTE ---\n",
    "print(\"Applying SMOTE...\")\n",
    "smote = SMOTE(random_state=42, n_jobs=-1)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_full, y_full)\n",
    "print(f\"New resampled label distribution:\\n{y_resampled.value_counts()}\")\n",
    "\n",
    "# --- Define Parameters (Translated) ---\n",
    "best_mlp_params = {\n",
    "    'hidden_layer_sizes': (99,),\n",
    "    'activation': 'relu',\n",
    "    'solver': 'adam',\n",
    "    'alpha': 0.0009771293502957021,\n",
    "    'learning_rate_init': 0.0005471619343332291\n",
    "}\n",
    "\n",
    "# --- Create and Train Pipeline ---\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()), # Step 1: Scale\n",
    "    ('model', MLPClassifier(\n",
    "        **best_mlp_params,\n",
    "        # ⚠️ NO 'class_weight'\n",
    "        random_state=42,\n",
    "        max_iter=1000\n",
    "    ))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_resampled, y_resampled) # Train on SMOTEd data\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# --- Evaluate ---\n",
    "y_pred = pipeline.predict(X_test)\n",
    "report = classification_report(y_test, y_pred, digits=6, output_dict=True)\n",
    "df_report = pandas.DataFrame(report).transpose()\n",
    "\n",
    "print(\"\\nMLPClassifier (Tuned + SMOTE) Report:\")\n",
    "print(df_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cf3301",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
