{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dacb6c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the MLP Classifier...\n",
      "Iteration 1, loss = 0.47795976\n",
      "Validation score: 0.808014\n",
      "Iteration 2, loss = 0.39126194\n",
      "Validation score: 0.811005\n",
      "Iteration 3, loss = 0.37756859\n",
      "Validation score: 0.811603\n",
      "Iteration 4, loss = 0.37214412\n",
      "Validation score: 0.811603\n",
      "Iteration 5, loss = 0.36786536\n",
      "Validation score: 0.814593\n",
      "Iteration 6, loss = 0.36383573\n",
      "Validation score: 0.808612\n",
      "Iteration 7, loss = 0.36155652\n",
      "Validation score: 0.812799\n",
      "Iteration 8, loss = 0.35879974\n",
      "Validation score: 0.811603\n",
      "Iteration 9, loss = 0.35622479\n",
      "Validation score: 0.807416\n",
      "Iteration 10, loss = 0.35323548\n",
      "Validation score: 0.813397\n",
      "Iteration 11, loss = 0.35178585\n",
      "Validation score: 0.803230\n",
      "Iteration 12, loss = 0.34926975\n",
      "Validation score: 0.812201\n",
      "Iteration 13, loss = 0.34742408\n",
      "Validation score: 0.808612\n",
      "Iteration 14, loss = 0.34545054\n",
      "Validation score: 0.805024\n",
      "Iteration 15, loss = 0.34404154\n",
      "Validation score: 0.808612\n",
      "Iteration 16, loss = 0.34184621\n",
      "Validation score: 0.812201\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training complete.\n",
      "\n",
      "MLP Accuracy: 0.8246\n",
      "\n",
      "MLP Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8422    0.8572    0.8496      2416\n",
      "         1.0     0.7995    0.7800    0.7897      1764\n",
      "\n",
      "    accuracy                         0.8246      4180\n",
      "   macro avg     0.8209    0.8186    0.8197      4180\n",
      "weighted avg     0.8242    0.8246    0.8243      4180\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# --- 1. Load Your Data (You already did this) ---\n",
    "datatrain_df = pd.read_csv('./Data/salary.train.processed.csv', index_col='id')\n",
    "test_df = pd.read_csv('./Data/salary.test.processed.csv', index_col='id')\n",
    "\n",
    "# --- 2. Separate Features (X) and Target (y) ---\n",
    "# --- IMPORTANT: Change 'target' to your actual target column name! ---\n",
    "target_column = 'label' # Or 'salary', 'income', etc.\n",
    "\n",
    "X_train = datatrain_df.drop(target_column, axis=1)\n",
    "y_train = datatrain_df[target_column]\n",
    "\n",
    "X_test = test_df.drop(target_column, axis=1)\n",
    "y_test = test_df[target_column]\n",
    "\n",
    "# --- 3. CRITICAL STEP: Feature Scaling ---\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler ONLY on the training data\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Transform both training and testing data\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --- 4. Initialize and Train the MLP/DNN ---\n",
    "print(\"Training the MLP Classifier...\")\n",
    "\n",
    "# This creates a network with 2 hidden layers: one with 64 neurons, one with 32\n",
    "mlp_model = MLPClassifier(\n",
    "    hidden_layer_sizes=(64, 32),  # The architecture of your network\n",
    "    max_iter=1000,               # Max epochs (passes through data)\n",
    "    early_stopping=True,         # Stops training when validation score stops improving\n",
    "    random_state=42,             # For reproducible results\n",
    "    verbose=True                 # Set to True to see training progress\n",
    ")\n",
    "\n",
    "# Train the model on the SCALED data\n",
    "mlp_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# --- 5. Evaluate the Model ---\n",
    "# Make predictions on the SCALED test data\n",
    "y_pred_mlp = mlp_model.predict(X_test_scaled)\n",
    "\n",
    "# Print the results\n",
    "print(f\"\\nMLP Accuracy: {accuracy_score(y_test, y_pred_mlp):.4f}\")\n",
    "print(\"\\nMLP Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_mlp, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11970a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-21 20:38:14,594] A new study created in memory with name: no-name-4bcf88a3-940d-4204-9a4c-4eca71630893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Optuna hyperparameter search...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b20e0145864147098a83a819f0abc590",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-10-21 20:38:32,487] Trial 0 finished with value: 0.811005337749692 and parameters: {'n_layers': 3, 'n_units_l0': 75, 'n_units_l1': 67, 'n_units_l2': 82, 'activation': 'relu', 'solver': 'sgd', 'alpha': 0.00032377262905578235, 'learning_rate_init': 0.0004293732823780958}. Best is trial 0 with value: 0.811005337749692.\n",
      "[I 2025-10-21 20:38:43,835] Trial 1 finished with value: 0.8072967995671204 and parameters: {'n_layers': 2, 'n_units_l0': 98, 'n_units_l1': 22, 'activation': 'tanh', 'solver': 'sgd', 'alpha': 0.0006738654166557526, 'learning_rate_init': 0.0010567152561259893}. Best is trial 0 with value: 0.811005337749692.\n",
      "[I 2025-10-21 20:38:45,577] Trial 2 finished with value: 0.8140552980111772 and parameters: {'n_layers': 1, 'n_units_l0': 35, 'activation': 'relu', 'solver': 'adam', 'alpha': 1.1769068817391707e-05, 'learning_rate_init': 0.0030651455995225957}. Best is trial 2 with value: 0.8140552980111772.\n",
      "[I 2025-10-21 20:38:48,864] Trial 3 finished with value: 0.8087322599287967 and parameters: {'n_layers': 1, 'n_units_l0': 41, 'activation': 'tanh', 'solver': 'sgd', 'alpha': 0.004243565960514713, 'learning_rate_init': 0.008886720363505048}. Best is trial 2 with value: 0.8140552980111772.\n",
      "[I 2025-10-21 20:38:55,080] Trial 4 finished with value: 0.8139953892463349 and parameters: {'n_layers': 1, 'n_units_l0': 93, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.055812612566590154, 'learning_rate_init': 0.0007872544361444448}. Best is trial 2 with value: 0.8140552980111772.\n",
      "[I 2025-10-21 20:39:05,432] Trial 5 finished with value: 0.7745209428830502 and parameters: {'n_layers': 2, 'n_units_l0': 69, 'n_units_l1': 122, 'activation': 'relu', 'solver': 'sgd', 'alpha': 0.008687678383973878, 'learning_rate_init': 0.0001121205603846751}. Best is trial 2 with value: 0.8140552980111772.\n",
      "[I 2025-10-21 20:39:09,707] Trial 6 finished with value: 0.8103469100565666 and parameters: {'n_layers': 1, 'n_units_l0': 89, 'activation': 'tanh', 'solver': 'adam', 'alpha': 1.3882120739833272e-05, 'learning_rate_init': 0.0029157026422994385}. Best is trial 2 with value: 0.8140552980111772.\n",
      "[I 2025-10-21 20:39:14,112] Trial 7 finished with value: 0.7864830696414121 and parameters: {'n_layers': 1, 'n_units_l0': 18, 'activation': 'relu', 'solver': 'sgd', 'alpha': 0.0006164212771666672, 'learning_rate_init': 0.00014890012416154223}. Best is trial 2 with value: 0.8140552980111772.\n",
      "[I 2025-10-21 20:39:30,327] Trial 8 finished with value: 0.8114836313866816 and parameters: {'n_layers': 2, 'n_units_l0': 89, 'n_units_l1': 106, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.04261174554829559, 'learning_rate_init': 0.0003398187421586796}. Best is trial 2 with value: 0.8140552980111772.\n",
      "[I 2025-10-21 20:39:33,604] Trial 9 finished with value: 0.8129187483701608 and parameters: {'n_layers': 3, 'n_units_l0': 75, 'n_units_l1': 61, 'n_units_l2': 66, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.007917007117114443, 'learning_rate_init': 0.009566386510211269}. Best is trial 2 with value: 0.8140552980111772.\n",
      "[I 2025-10-21 20:39:35,976] Trial 10 finished with value: 0.8145336345704414 and parameters: {'n_layers': 1, 'n_units_l0': 125, 'activation': 'relu', 'solver': 'adam', 'alpha': 1.1587553748217268e-05, 'learning_rate_init': 0.002508822414883298}. Best is trial 10 with value: 0.8145336345704414.\n",
      "[I 2025-10-21 20:39:38,363] Trial 11 finished with value: 0.8115432718873072 and parameters: {'n_layers': 1, 'n_units_l0': 115, 'activation': 'relu', 'solver': 'adam', 'alpha': 1.17673419624273e-05, 'learning_rate_init': 0.002926955296801833}. Best is trial 10 with value: 0.8145336345704414.\n",
      "[I 2025-10-21 20:39:39,981] Trial 12 finished with value: 0.8136367521804999 and parameters: {'n_layers': 1, 'n_units_l0': 45, 'activation': 'relu', 'solver': 'adam', 'alpha': 7.416944941992664e-05, 'learning_rate_init': 0.0030034512199535127}. Best is trial 10 with value: 0.8145336345704414.\n",
      "[I 2025-10-21 20:39:42,248] Trial 13 finished with value: 0.8148328242858865 and parameters: {'n_layers': 2, 'n_units_l0': 128, 'n_units_l1': 23, 'activation': 'relu', 'solver': 'adam', 'alpha': 5.964110029575939e-05, 'learning_rate_init': 0.0019920216593412154}. Best is trial 13 with value: 0.8148328242858865.\n",
      "[I 2025-10-21 20:39:45,511] Trial 14 finished with value: 0.8128589147192992 and parameters: {'n_layers': 2, 'n_units_l0': 127, 'n_units_l1': 16, 'activation': 'relu', 'solver': 'adam', 'alpha': 7.664984422788072e-05, 'learning_rate_init': 0.0015125574045063903}. Best is trial 13 with value: 0.8148328242858865.\n",
      "[I 2025-10-21 20:39:49,131] Trial 15 finished with value: 0.813995485821453 and parameters: {'n_layers': 3, 'n_units_l0': 128, 'n_units_l1': 49, 'n_units_l2': 127, 'activation': 'relu', 'solver': 'adam', 'alpha': 7.903217380649457e-05, 'learning_rate_init': 0.005268241818569405}. Best is trial 13 with value: 0.8148328242858865.\n",
      "[I 2025-10-21 20:39:53,211] Trial 16 finished with value: 0.8119619894070831 and parameters: {'n_layers': 2, 'n_units_l0': 111, 'n_units_l1': 92, 'activation': 'relu', 'solver': 'adam', 'alpha': 3.9061608099888096e-05, 'learning_rate_init': 0.0014351724021274246}. Best is trial 13 with value: 0.8148328242858865.\n",
      "[I 2025-10-21 20:39:56,641] Trial 17 finished with value: 0.8132178951633313 and parameters: {'n_layers': 3, 'n_units_l0': 114, 'n_units_l1': 41, 'n_units_l2': 22, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.00021886520255067222, 'learning_rate_init': 0.001880606757211145}. Best is trial 13 with value: 0.8148328242858865.\n",
      "[I 2025-10-21 20:40:00,777] Trial 18 finished with value: 0.8125600576514825 and parameters: {'n_layers': 2, 'n_units_l0': 103, 'n_units_l1': 89, 'activation': 'relu', 'solver': 'adam', 'alpha': 3.2382962695454024e-05, 'learning_rate_init': 0.0006203355745555933}. Best is trial 13 with value: 0.8148328242858865.\n",
      "[I 2025-10-21 20:40:02,959] Trial 19 finished with value: 0.810167720290473 and parameters: {'n_layers': 2, 'n_units_l0': 60, 'n_units_l1': 40, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.00019760231817031714, 'learning_rate_init': 0.005223887612045687}. Best is trial 13 with value: 0.8148328242858865.\n",
      "[I 2025-10-21 20:40:05,597] Trial 20 finished with value: 0.8120816781699437 and parameters: {'n_layers': 3, 'n_units_l0': 120, 'n_units_l1': 28, 'n_units_l2': 18, 'activation': 'relu', 'solver': 'adam', 'alpha': 2.7730405389797825e-05, 'learning_rate_init': 0.00450103647526291}. Best is trial 13 with value: 0.8148328242858865.\n",
      "[I 2025-10-21 20:40:07,774] Trial 21 finished with value: 0.8144740369920903 and parameters: {'n_layers': 1, 'n_units_l0': 27, 'activation': 'relu', 'solver': 'adam', 'alpha': 1.0472998024867743e-05, 'learning_rate_init': 0.0026805386910499773}. Best is trial 13 with value: 0.8148328242858865.\n",
      "[I 2025-10-21 20:40:09,046] Trial 22 finished with value: 0.8140552336277652 and parameters: {'n_layers': 1, 'n_units_l0': 25, 'activation': 'relu', 'solver': 'adam', 'alpha': 2.8083360562931747e-05, 'learning_rate_init': 0.0021726667800429675}. Best is trial 13 with value: 0.8148328242858865.\n",
      "[I 2025-10-21 20:40:10,924] Trial 23 finished with value: 0.8150720515836462 and parameters: {'n_layers': 1, 'n_units_l0': 59, 'activation': 'relu', 'solver': 'adam', 'alpha': 1.0483726107996898e-05, 'learning_rate_init': 0.0011366750484710067}. Best is trial 23 with value: 0.8150720515836462.\n",
      "[I 2025-10-21 20:40:12,834] Trial 24 finished with value: 0.8153710159571497 and parameters: {'n_layers': 1, 'n_units_l0': 57, 'activation': 'relu', 'solver': 'adam', 'alpha': 7.034496933774916e-05, 'learning_rate_init': 0.0010958908815007428}. Best is trial 24 with value: 0.8153710159571497.\n",
      "[I 2025-10-21 20:40:15,984] Trial 25 finished with value: 0.8104070012410761 and parameters: {'n_layers': 2, 'n_units_l0': 54, 'n_units_l1': 80, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.00011540492658604014, 'learning_rate_init': 0.0010645511819307243}. Best is trial 24 with value: 0.8153710159571497.\n",
      "[I 2025-10-21 20:40:24,324] Trial 26 finished with value: 0.8028716847398415 and parameters: {'n_layers': 1, 'n_units_l0': 60, 'activation': 'tanh', 'solver': 'sgd', 'alpha': 0.0013220481449503659, 'learning_rate_init': 0.0004971675493780818}. Best is trial 24 with value: 0.8153710159571497.\n",
      "[I 2025-10-21 20:40:27,677] Trial 27 finished with value: 0.815371101801699 and parameters: {'n_layers': 2, 'n_units_l0': 81, 'n_units_l1': 54, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.001603528703222103, 'learning_rate_init': 0.0008487803435120983}. Best is trial 27 with value: 0.815371101801699.\n",
      "[I 2025-10-21 20:40:31,076] Trial 28 finished with value: 0.8136363551494593 and parameters: {'n_layers': 1, 'n_units_l0': 82, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.0017540820665500486, 'learning_rate_init': 0.0002902316337399041}. Best is trial 27 with value: 0.815371101801699.\n",
      "[I 2025-10-21 20:40:34,970] Trial 29 finished with value: 0.8039480895435052 and parameters: {'n_layers': 2, 'n_units_l0': 70, 'n_units_l1': 59, 'activation': 'relu', 'solver': 'sgd', 'alpha': 0.00045259585539197353, 'learning_rate_init': 0.0007854980101068628}. Best is trial 27 with value: 0.815371101801699.\n",
      "[I 2025-10-21 20:40:41,726] Trial 30 finished with value: 0.8120816137865315 and parameters: {'n_layers': 3, 'n_units_l0': 54, 'n_units_l1': 125, 'n_units_l2': 125, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.00333967484442157, 'learning_rate_init': 0.0002770125280879504}. Best is trial 27 with value: 0.815371101801699.\n",
      "[I 2025-10-21 20:40:44,344] Trial 31 finished with value: 0.8113636958207419 and parameters: {'n_layers': 2, 'n_units_l0': 63, 'n_units_l1': 34, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.00031542095363192366, 'learning_rate_init': 0.001351908122162771}. Best is trial 27 with value: 0.815371101801699.\n",
      "[I 2025-10-21 20:40:47,238] Trial 32 finished with value: 0.8127394834900866 and parameters: {'n_layers': 2, 'n_units_l0': 45, 'n_units_l1': 48, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.00015829708052647474, 'learning_rate_init': 0.000880995701639424}. Best is trial 27 with value: 0.815371101801699.\n",
      "[I 2025-10-21 20:40:49,807] Trial 33 finished with value: 0.8129787751712584 and parameters: {'n_layers': 2, 'n_units_l0': 75, 'n_units_l1': 49, 'activation': 'relu', 'solver': 'adam', 'alpha': 2.085605058436534e-05, 'learning_rate_init': 0.0011610932269285788}. Best is trial 27 with value: 0.815371101801699.\n",
      "[I 2025-10-21 20:41:03,468] Trial 34 finished with value: 0.8072374380612798 and parameters: {'n_layers': 1, 'n_units_l0': 104, 'activation': 'tanh', 'solver': 'sgd', 'alpha': 4.935337899864799e-05, 'learning_rate_init': 0.0005931139658293738}. Best is trial 27 with value: 0.815371101801699.\n",
      "[I 2025-10-21 20:41:05,006] Trial 35 finished with value: 0.8127992742186735 and parameters: {'n_layers': 1, 'n_units_l0': 52, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.001034091840728497, 'learning_rate_init': 0.001808906501852677}. Best is trial 27 with value: 0.815371101801699.\n",
      "[I 2025-10-21 20:41:07,452] Trial 36 finished with value: 0.8123804923154857 and parameters: {'n_layers': 2, 'n_units_l0': 36, 'n_units_l1': 29, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.002871708870872515, 'learning_rate_init': 0.0007110853592319241}. Best is trial 27 with value: 0.815371101801699.\n",
      "[I 2025-10-21 20:41:14,977] Trial 37 finished with value: 0.8081344062957706 and parameters: {'n_layers': 1, 'n_units_l0': 83, 'activation': 'tanh', 'solver': 'sgd', 'alpha': 0.023263055518717726, 'learning_rate_init': 0.0010049193325675151}. Best is trial 27 with value: 0.815371101801699.\n",
      "[I 2025-10-21 20:41:17,914] Trial 38 finished with value: 0.8113639426238212 and parameters: {'n_layers': 2, 'n_units_l0': 65, 'n_units_l1': 82, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.0005762239055429311, 'learning_rate_init': 0.0016351743456108144}. Best is trial 27 with value: 0.815371101801699.\n",
      "[I 2025-10-21 20:41:24,054] Trial 39 finished with value: 0.8100480315276126 and parameters: {'n_layers': 1, 'n_units_l0': 96, 'activation': 'tanh', 'solver': 'adam', 'alpha': 2.031227653649503e-05, 'learning_rate_init': 0.0038315726381585266}. Best is trial 27 with value: 0.815371101801699.\n",
      "[I 2025-10-21 20:41:29,959] Trial 40 finished with value: 0.8006582903418465 and parameters: {'n_layers': 2, 'n_units_l0': 40, 'n_units_l1': 73, 'activation': 'relu', 'solver': 'sgd', 'alpha': 0.007528540460022527, 'learning_rate_init': 0.0004968473145453617}. Best is trial 27 with value: 0.815371101801699.\n",
      "[I 2025-10-21 20:41:32,221] Trial 41 finished with value: 0.814473918955835 and parameters: {'n_layers': 1, 'n_units_l0': 122, 'activation': 'relu', 'solver': 'adam', 'alpha': 1.4595010416245835e-05, 'learning_rate_init': 0.0022384949203748017}. Best is trial 27 with value: 0.815371101801699.\n",
      "[I 2025-10-21 20:41:34,325] Trial 42 finished with value: 0.8143542409235431 and parameters: {'n_layers': 1, 'n_units_l0': 108, 'activation': 'relu', 'solver': 'adam', 'alpha': 5.406297780881432e-05, 'learning_rate_init': 0.0012121385492221358}. Best is trial 27 with value: 0.815371101801699.\n",
      "[I 2025-10-21 20:41:36,730] Trial 43 finished with value: 0.8116028694656583 and parameters: {'n_layers': 1, 'n_units_l0': 82, 'activation': 'relu', 'solver': 'adam', 'alpha': 1.966878732328257e-05, 'learning_rate_init': 0.0036774588717436297}. Best is trial 27 with value: 0.815371101801699.\n",
      "[I 2025-10-21 20:41:39,409] Trial 44 finished with value: 0.8130985390480995 and parameters: {'n_layers': 1, 'n_units_l0': 121, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.00011629147026041718, 'learning_rate_init': 0.0020673462926402352}. Best is trial 27 with value: 0.815371101801699.\n",
      "[I 2025-10-21 20:41:41,238] Trial 45 finished with value: 0.8130982493227455 and parameters: {'n_layers': 1, 'n_units_l0': 100, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.00033411887147713633, 'learning_rate_init': 0.007126092668256201}. Best is trial 27 with value: 0.815371101801699.\n",
      "[I 2025-10-21 20:41:44,133] Trial 46 finished with value: 0.8166868304782403 and parameters: {'n_layers': 1, 'n_units_l0': 50, 'activation': 'relu', 'solver': 'adam', 'alpha': 1.8270038867131823e-05, 'learning_rate_init': 0.0003928411594662225}. Best is trial 46 with value: 0.8166868304782403.\n",
      "[I 2025-10-21 20:41:50,381] Trial 47 finished with value: 0.8117824026099489 and parameters: {'n_layers': 1, 'n_units_l0': 50, 'activation': 'tanh', 'solver': 'adam', 'alpha': 3.947787739408065e-05, 'learning_rate_init': 0.00037929086479793406}. Best is trial 46 with value: 0.8166868304782403.\n",
      "[I 2025-10-21 20:41:53,119] Trial 48 finished with value: 0.813875679022337 and parameters: {'n_layers': 2, 'n_units_l0': 47, 'n_units_l1': 16, 'activation': 'relu', 'solver': 'adam', 'alpha': 1.5889643982350972e-05, 'learning_rate_init': 0.00022430466883240684}. Best is trial 46 with value: 0.8166868304782403.\n",
      "[I 2025-10-21 20:41:59,479] Trial 49 finished with value: 0.8117824884544983 and parameters: {'n_layers': 3, 'n_units_l0': 76, 'n_units_l1': 61, 'n_units_l2': 61, 'activation': 'relu', 'solver': 'adam', 'alpha': 5.6417788889502354e-05, 'learning_rate_init': 0.00017635862190193936}. Best is trial 46 with value: 0.8166868304782403.\n",
      "[I 2025-10-21 20:42:01,158] Trial 50 finished with value: 0.8148926686673169 and parameters: {'n_layers': 1, 'n_units_l0': 58, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.0017953998156904673, 'learning_rate_init': 0.0008894096306149781}. Best is trial 46 with value: 0.8166868304782403.\n",
      "[I 2025-10-21 20:42:02,799] Trial 51 finished with value: 0.8152514559611131 and parameters: {'n_layers': 1, 'n_units_l0': 58, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.0018362117136619548, 'learning_rate_init': 0.0008815564251451035}. Best is trial 46 with value: 0.8166868304782403.\n",
      "[I 2025-10-21 20:42:05,381] Trial 52 finished with value: 0.8152513915777012 and parameters: {'n_layers': 1, 'n_units_l0': 57, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.001624588087672256, 'learning_rate_init': 0.000895434886925058}. Best is trial 46 with value: 0.8166868304782403.\n",
      "[I 2025-10-21 20:42:07,402] Trial 53 finished with value: 0.8154907583728535 and parameters: {'n_layers': 1, 'n_units_l0': 67, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.0007466681063078396, 'learning_rate_init': 0.0006349580693242135}. Best is trial 46 with value: 0.8166868304782403.\n",
      "[I 2025-10-21 20:42:09,539] Trial 54 finished with value: 0.8149524593959038 and parameters: {'n_layers': 1, 'n_units_l0': 67, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.0008033038367475318, 'learning_rate_init': 0.0006464155409308762}. Best is trial 46 with value: 0.8166868304782403.\n",
      "[I 2025-10-21 20:42:11,516] Trial 55 finished with value: 0.8113040016672728 and parameters: {'n_layers': 1, 'n_units_l0': 72, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.004463803012931624, 'learning_rate_init': 0.0004582835474010484}. Best is trial 46 with value: 0.8166868304782403.\n",
      "[I 2025-10-21 20:42:13,567] Trial 56 finished with value: 0.8144738760335603 and parameters: {'n_layers': 1, 'n_units_l0': 88, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.002260492710697681, 'learning_rate_init': 0.0005779822344974654}. Best is trial 46 with value: 0.8166868304782403.\n",
      "[I 2025-10-21 20:42:15,616] Trial 57 finished with value: 0.8128589469110051 and parameters: {'n_layers': 1, 'n_units_l0': 49, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.001222644629740558, 'learning_rate_init': 0.00037619838749002515}. Best is trial 46 with value: 0.8166868304782403.\n",
      "[I 2025-10-21 20:42:17,615] Trial 58 finished with value: 0.8149521911316873 and parameters: {'n_layers': 1, 'n_units_l0': 63, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.016336424461231507, 'learning_rate_init': 0.0008590769657955011}. Best is trial 46 with value: 0.8166868304782403.\n",
      "[I 2025-10-21 20:42:20,979] Trial 59 finished with value: 0.8126798322588922 and parameters: {'n_layers': 1, 'n_units_l0': 42, 'activation': 'relu', 'solver': 'sgd', 'alpha': 0.005698464460328791, 'learning_rate_init': 0.0007361782841002835}. Best is trial 46 with value: 0.8166868304782403.\n",
      "[I 2025-10-21 20:42:24,386] Trial 60 finished with value: 0.8132778683115856 and parameters: {'n_layers': 1, 'n_units_l0': 55, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.0015398685478537537, 'learning_rate_init': 0.0005585753611517197}. Best is trial 46 with value: 0.8166868304782403.\n",
      "[I 2025-10-21 20:42:25,738] Trial 61 finished with value: 0.8136965214479496 and parameters: {'n_layers': 1, 'n_units_l0': 58, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.0006820865424076469, 'learning_rate_init': 0.0013272922580081566}. Best is trial 46 with value: 0.8166868304782403.\n",
      "[I 2025-10-21 20:42:27,507] Trial 62 finished with value: 0.8148926042839048 and parameters: {'n_layers': 1, 'n_units_l0': 68, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.0030177103143693763, 'learning_rate_init': 0.001046203708497666}. Best is trial 46 with value: 0.8166868304782403.\n",
      "[I 2025-10-21 20:42:30,343] Trial 63 finished with value: 0.8145934252990282 and parameters: {'n_layers': 1, 'n_units_l0': 77, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.0008571946261466807, 'learning_rate_init': 0.0006873935926595534}. Best is trial 46 with value: 0.8166868304782403.\n",
      "[I 2025-10-21 20:42:31,950] Trial 64 finished with value: 0.8155505276403031 and parameters: {'n_layers': 1, 'n_units_l0': 62, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.000405458607294722, 'learning_rate_init': 0.0009130654908933117}. Best is trial 46 with value: 0.8166868304782403.\n",
      "[I 2025-10-21 20:42:34,048] Trial 65 finished with value: 0.8148325023688266 and parameters: {'n_layers': 1, 'n_units_l0': 63, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.00039055975069351626, 'learning_rate_init': 0.0008610649150482101}. Best is trial 46 with value: 0.8166868304782403.\n",
      "[I 2025-10-21 20:42:35,999] Trial 66 finished with value: 0.8109451607206332 and parameters: {'n_layers': 1, 'n_units_l0': 72, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.0005037018772655266, 'learning_rate_init': 0.00041539910591527676}. Best is trial 46 with value: 0.8166868304782403.\n",
      "[I 2025-10-21 20:42:38,544] Trial 67 finished with value: 0.8148328886692985 and parameters: {'n_layers': 1, 'n_units_l0': 56, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.00027455145857608633, 'learning_rate_init': 0.00029869468799012213}. Best is trial 46 with value: 0.8166868304782403.\n",
      "[I 2025-10-21 20:42:41,266] Trial 68 finished with value: 0.8138756575611997 and parameters: {'n_layers': 1, 'n_units_l0': 66, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.0011024469297427728, 'learning_rate_init': 0.0009453246667049203}. Best is trial 46 with value: 0.8166868304782403.\n",
      "[I 2025-10-21 20:42:43,669] Trial 69 finished with value: 0.8154907154505788 and parameters: {'n_layers': 1, 'n_units_l0': 80, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.0022854526130633786, 'learning_rate_init': 0.0005217554095275923}. Best is trial 46 with value: 0.8166868304782403.\n",
      "[I 2025-10-21 20:42:47,026] Trial 70 finished with value: 0.8076556297831913 and parameters: {'n_layers': 1, 'n_units_l0': 79, 'activation': 'relu', 'solver': 'sgd', 'alpha': 0.002027803656066048, 'learning_rate_init': 0.0005189728137339876}. Best is trial 46 with value: 0.8166868304782403.\n",
      "[I 2025-10-21 20:42:49,269] Trial 71 finished with value: 0.81513178865939 and parameters: {'n_layers': 1, 'n_units_l0': 87, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.0886985678084017, 'learning_rate_init': 0.001601973465450092}. Best is trial 46 with value: 0.8166868304782403.\n",
      "[I 2025-10-21 20:42:51,355] Trial 72 finished with value: 0.8144140745744047 and parameters: {'n_layers': 1, 'n_units_l0': 61, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.00409944993923352, 'learning_rate_init': 0.0007774355522376708}. Best is trial 46 with value: 0.8166868304782403.\n",
      "[I 2025-10-21 20:42:53,282] Trial 73 finished with value: 0.8130385229775706 and parameters: {'n_layers': 1, 'n_units_l0': 70, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.0014581203747180407, 'learning_rate_init': 0.000665956239803764}. Best is trial 46 with value: 0.8166868304782403.\n",
      "[I 2025-10-21 20:42:55,325] Trial 74 finished with value: 0.8141152067760195 and parameters: {'n_layers': 1, 'n_units_l0': 53, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.0024451866846632968, 'learning_rate_init': 0.0012586804931532017}. Best is trial 46 with value: 0.8166868304782403.\n",
      "[I 2025-10-21 20:42:57,910] Trial 75 finished with value: 0.8155504310651852 and parameters: {'n_layers': 1, 'n_units_l0': 94, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.0006847319880010645, 'learning_rate_init': 0.0004303087238646033}. Best is trial 46 with value: 0.8166868304782403.\n",
      "[I 2025-10-21 20:43:00,855] Trial 76 finished with value: 0.8169260148537253 and parameters: {'n_layers': 1, 'n_units_l0': 92, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.0007141246050583104, 'learning_rate_init': 0.0003193669644882347}. Best is trial 76 with value: 0.8169260148537253.\n",
      "[I 2025-10-21 20:43:04,418] Trial 77 finished with value: 0.8150121964716474 and parameters: {'n_layers': 1, 'n_units_l0': 94, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.00022048322176968036, 'learning_rate_init': 0.00024712204369577666}. Best is trial 76 with value: 0.8169260148537253.\n",
      "[I 2025-10-21 20:43:21,028] Trial 78 finished with value: 0.8129191132094954 and parameters: {'n_layers': 3, 'n_units_l0': 93, 'n_units_l1': 103, 'n_units_l2': 94, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.0006457262212183753, 'learning_rate_init': 0.00042826592136720204}. Best is trial 76 with value: 0.8169260148537253.\n",
      "[I 2025-10-21 20:43:23,217] Trial 79 finished with value: 0.811722837223304 and parameters: {'n_layers': 1, 'n_units_l0': 84, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.00010272280210735163, 'learning_rate_init': 0.0003453097557374233}. Best is trial 76 with value: 0.8169260148537253.\n",
      "[I 2025-10-21 20:43:25,598] Trial 80 finished with value: 0.8132775893168004 and parameters: {'n_layers': 1, 'n_units_l0': 80, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.0001616701836348794, 'learning_rate_init': 0.00019165679925778236}. Best is trial 76 with value: 0.8169260148537253.\n",
      "[I 2025-10-21 20:43:28,558] Trial 81 finished with value: 0.8150122608550593 and parameters: {'n_layers': 1, 'n_units_l0': 86, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.0008846825379848742, 'learning_rate_init': 0.00034353623032544516}. Best is trial 76 with value: 0.8169260148537253.\n",
      "[I 2025-10-21 20:43:31,216] Trial 82 finished with value: 0.815670076905771 and parameters: {'n_layers': 1, 'n_units_l0': 92, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.0004119892757562332, 'learning_rate_init': 0.00047369712137488516}. Best is trial 76 with value: 0.8169260148537253.\n",
      "[I 2025-10-21 20:43:34,511] Trial 83 finished with value: 0.8129787322489835 and parameters: {'n_layers': 1, 'n_units_l0': 98, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.0005037311742379911, 'learning_rate_init': 0.00046470473302695974}. Best is trial 76 with value: 0.8169260148537253.\n",
      "[I 2025-10-21 20:43:37,247] Trial 84 finished with value: 0.8164475280664999 and parameters: {'n_layers': 1, 'n_units_l0': 92, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.0004085767333259912, 'learning_rate_init': 0.000304937138151315}. Best is trial 76 with value: 0.8169260148537253.\n",
      "[I 2025-10-21 20:43:40,730] Trial 85 finished with value: 0.8157299427483385 and parameters: {'n_layers': 1, 'n_units_l0': 106, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.00039089033642856835, 'learning_rate_init': 0.00038485460063619867}. Best is trial 76 with value: 0.8169260148537253.\n",
      "[I 2025-10-21 20:43:43,699] Trial 86 finished with value: 0.8153111608451508 and parameters: {'n_layers': 1, 'n_units_l0': 90, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.0003536078547722936, 'learning_rate_init': 0.00025833041050211616}. Best is trial 76 with value: 0.8169260148537253.\n",
      "[I 2025-10-21 20:43:50,843] Trial 87 finished with value: 0.8082542774782984 and parameters: {'n_layers': 1, 'n_units_l0': 107, 'activation': 'relu', 'solver': 'sgd', 'alpha': 0.00043959520088775406, 'learning_rate_init': 0.00030431918764093805}. Best is trial 76 with value: 0.8169260148537253.\n",
      "[I 2025-10-21 20:43:53,451] Trial 88 finished with value: 0.8147729155210444 and parameters: {'n_layers': 1, 'n_units_l0': 90, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.0007410489078375024, 'learning_rate_init': 0.000375110258225123}. Best is trial 76 with value: 0.8169260148537253.\n",
      "[I 2025-10-21 20:43:58,825] Trial 89 finished with value: 0.8141749545823315 and parameters: {'n_layers': 1, 'n_units_l0': 102, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.00022127440441994622, 'learning_rate_init': 0.00011468160611092491}. Best is trial 76 with value: 0.8169260148537253.\n",
      "[I 2025-10-21 20:44:03,875] Trial 90 finished with value: 0.812440207930092 and parameters: {'n_layers': 1, 'n_units_l0': 112, 'activation': 'tanh', 'solver': 'adam', 'alpha': 0.000286913238931183, 'learning_rate_init': 0.00022375620321503007}. Best is trial 76 with value: 0.8169260148537253.\n",
      "[I 2025-10-21 20:44:07,591] Trial 91 finished with value: 0.8160290466192346 and parameters: {'n_layers': 1, 'n_units_l0': 96, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.0005745953286115224, 'learning_rate_init': 0.0005395323066182211}. Best is trial 76 with value: 0.8169260148537253.\n",
      "[I 2025-10-21 20:44:10,257] Trial 92 finished with value: 0.8135169346508153 and parameters: {'n_layers': 1, 'n_units_l0': 95, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.0005705217802688628, 'learning_rate_init': 0.00040869674736080855}. Best is trial 76 with value: 0.8169260148537253.\n",
      "[I 2025-10-21 20:44:14,103] Trial 93 finished with value: 0.8177034874755914 and parameters: {'n_layers': 1, 'n_units_l0': 99, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.0009771293502957021, 'learning_rate_init': 0.0005471619343332291}. Best is trial 93 with value: 0.8177034874755914.\n",
      "[I 2025-10-21 20:44:17,288] Trial 94 finished with value: 0.8136964034116941 and parameters: {'n_layers': 1, 'n_units_l0': 98, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.0009609310412388205, 'learning_rate_init': 0.00031891100426974716}. Best is trial 93 with value: 0.8177034874755914.\n",
      "[I 2025-10-21 20:44:19,859] Trial 95 finished with value: 0.8154907047200101 and parameters: {'n_layers': 1, 'n_units_l0': 104, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.0004680515825673283, 'learning_rate_init': 0.0006092596874898405}. Best is trial 93 with value: 0.8177034874755914.\n",
      "[I 2025-10-21 20:44:24,024] Trial 96 finished with value: 0.8133374873510739 and parameters: {'n_layers': 1, 'n_units_l0': 117, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.0001698546100066594, 'learning_rate_init': 0.000498379110134326}. Best is trial 93 with value: 0.8177034874755914.\n",
      "[I 2025-10-21 20:44:27,432] Trial 97 finished with value: 0.8166270075579473 and parameters: {'n_layers': 1, 'n_units_l0': 92, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.00026735349099971426, 'learning_rate_init': 0.00044543872568757377}. Best is trial 93 with value: 0.8177034874755914.\n",
      "[I 2025-10-21 20:44:29,980] Trial 98 finished with value: 0.8144140853049734 and parameters: {'n_layers': 1, 'n_units_l0': 107, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.0002899373239790439, 'learning_rate_init': 0.0004470345080789394}. Best is trial 93 with value: 0.8177034874755914.\n",
      "[I 2025-10-21 20:44:33,063] Trial 99 finished with value: 0.8160289178524106 and parameters: {'n_layers': 1, 'n_units_l0': 92, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.00038356746621155685, 'learning_rate_init': 0.0002668770794359642}. Best is trial 93 with value: 0.8177034874755914.\n",
      "\n",
      "Search complete.\n",
      "Best trial (accuracy): 0.8177\n",
      "Best hyperparameters found:\n",
      "{'n_layers': 1, 'n_units_l0': 99, 'activation': 'relu', 'solver': 'adam', 'alpha': 0.0009771293502957021, 'learning_rate_init': 0.0005471619343332291}\n",
      "\n",
      "Training final model with best hyperparameters...\n",
      "\n",
      "Final Model Accuracy on Test Set: 0.8244\n",
      "\n",
      "Final Model Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8484    0.8477    0.8480      2416\n",
      "         1.0     0.7916    0.7925    0.7921      1764\n",
      "\n",
      "    accuracy                         0.8244      4180\n",
      "   macro avg     0.8200    0.8201    0.8201      4180\n",
      "weighted avg     0.8244    0.8244    0.8244      4180\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import optuna\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# --- 1. Load Your Data ---\n",
    "datatrain_df = pd.read_csv('./Data/salary.train.processed.csv', index_col='id')\n",
    "test_df = pd.read_csv('./Data/salary.test.processed.csv', index_col='id')\n",
    "\n",
    "# --- 2. Separate Features (X) and Target (y) ---\n",
    "# --- IMPORTANT: Change 'target' to your actual target column name! ---\n",
    "target_column = 'label' # Or 'salary', 'income', etc.\n",
    "\n",
    "X_train = datatrain_df.drop(target_column, axis=1)\n",
    "y_train = datatrain_df[target_column]\n",
    "\n",
    "X_test = test_df.drop(target_column, axis=1)\n",
    "y_test = test_df[target_column]\n",
    "\n",
    "# --- 3. CRITICAL STEP: Feature Scaling ---\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --- 4. Define the Optuna Objective Function ---\n",
    "# This function will be called once per trial\n",
    "def objective(trial):\n",
    "    # --- Define the Hyperparameter Search Space ---\n",
    "    \n",
    "    # 1. Number of hidden layers (e.g., 1, 2, or 3)\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 3)\n",
    "    \n",
    "    # 2. Size of each hidden layer\n",
    "    layers = []\n",
    "    for i in range(n_layers):\n",
    "        # Suggest a number of neurons (e.g., 16 to 128)\n",
    "        layers.append(trial.suggest_int(f'n_units_l{i}', 16, 128))\n",
    "    \n",
    "    hidden_layer_sizes = tuple(layers)\n",
    "    \n",
    "    # 3. Activation function\n",
    "    activation = trial.suggest_categorical('activation', ['relu', 'tanh'])\n",
    "    \n",
    "    # 4. Solver (Algorithm for weight optimization)\n",
    "    solver = trial.suggest_categorical('solver', ['adam', 'sgd'])\n",
    "    \n",
    "    # 5. Regularization strength (helps prevent overfitting)\n",
    "    # --- CHANGED: Using suggest_float as per the warning ---\n",
    "    alpha = trial.suggest_float('alpha', 1e-5, 1e-1, log=True)\n",
    "    \n",
    "    # 6. Initial learning rate (only for 'sgd' or 'adam')\n",
    "    # --- CHANGED: Using suggest_float as per the warning ---\n",
    "    learning_rate_init = trial.suggest_float('learning_rate_init', 1e-4, 1e-2, log=True)\n",
    "\n",
    "    # --- Create the Model ---\n",
    "    model = MLPClassifier(\n",
    "        hidden_layer_sizes=hidden_layer_sizes,\n",
    "        activation=activation,\n",
    "        solver=solver,\n",
    "        alpha=alpha,\n",
    "        learning_rate_init=learning_rate_init,\n",
    "        max_iter=500,  # Give it enough time to converge\n",
    "        early_stopping=True, # Good practice for individual trials\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # --- Evaluate the Model ---\n",
    "    # We use 3-fold cross-validation on the training data.\n",
    "    # This gives a more stable score than a single train/validation split.\n",
    "    score = cross_val_score(model, X_train_scaled, y_train, cv=3, scoring='accuracy')\n",
    "    \n",
    "    # Return the mean accuracy from the cross-validation\n",
    "    return score.mean()\n",
    "\n",
    "# --- 5. Run the Optuna Study ---\n",
    "print(\"Starting Optuna hyperparameter search...\")\n",
    "\n",
    "# We want to MAXIMIZE accuracy\n",
    "study = optuna.create_study(direction='maximize')\n",
    "\n",
    "# Run 100 trials, and show the progress bar!\n",
    "study.optimize(\n",
    "    objective, \n",
    "    n_trials=100, \n",
    "    show_progress_bar=True  # This enables the progress bar\n",
    ")\n",
    "\n",
    "print(\"\\nSearch complete.\")\n",
    "print(f\"Best trial (accuracy): {study.best_value:.4f}\")\n",
    "print(\"Best hyperparameters found:\")\n",
    "print(study.best_params)\n",
    "\n",
    "# --- 6. Train the FINAL Model with the Best Params ---\n",
    "print(\"\\nTraining final model with best hyperparameters...\")\n",
    "\n",
    "# Get a copy of the best parameters\n",
    "best_params = study.best_params.copy()\n",
    "\n",
    "# --- Reconstruct the hidden_layer_sizes tuple from the best params ---\n",
    "# 1. Get the number of layers and REMOVE it from the dictionary\n",
    "n_layers = best_params.pop('n_layers')\n",
    "\n",
    "# 2. Build the layers list by REMOVING each n_units_l{i} key\n",
    "layers = []\n",
    "for i in range(n_layers):\n",
    "    layer_size = best_params.pop(f'n_units_l{i}')\n",
    "    layers.append(layer_size)\n",
    "\n",
    "# 3. Create the final tuple that MLPClassifier understands\n",
    "final_hidden_layer_sizes = tuple(layers)\n",
    "\n",
    "# Now, 'best_params' only contains keys that MLPClassifier accepts\n",
    "# (e.g., 'activation', 'solver', 'alpha')\n",
    "\n",
    "# Create a new MLP model using the reconstructed tuple and the rest of the params\n",
    "final_mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=final_hidden_layer_sizes, # Pass the tuple we just built\n",
    "    max_iter=1000, \n",
    "    early_stopping=True,\n",
    "    random_state=42,\n",
    "    **best_params # This unpacks the *cleaned* dictionary\n",
    ")\n",
    "\n",
    "# Train on the FULL scaled training set\n",
    "final_mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "# --- 7. Evaluate on the Test Set ---\n",
    "y_pred_final = final_mlp.predict(X_test_scaled)\n",
    "\n",
    "print(f\"\\nFinal Model Accuracy on Test Set: {accuracy_score(y_test, y_pred_final):.4f}\")\n",
    "print(\"\\nFinal Model Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_final,digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c0b114",
   "metadata": {},
   "source": [
    "### ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af450bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 4. Testing MLPClassifier with ADASYN ---\n",
      "Applying ADASYN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\natth\\anaconda3\\Lib\\site-packages\\imblearn\\over_sampling\\_adasyn.py:156: FutureWarning: The parameter `n_jobs` has been deprecated in 0.10 and will be removed in 0.12. You can pass an nearest neighbors estimator where `n_jobs` is already set instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\natth\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] The system cannot find the file specified\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"c:\\Users\\natth\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\natth\\anaconda3\\Lib\\subprocess.py\", line 548, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\natth\\anaconda3\\Lib\\subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"c:\\Users\\natth\\anaconda3\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New resampled label distribution:\n",
      "label\n",
      "1.0    9726\n",
      "0.0    9719\n",
      "Name: count, dtype: int64\n",
      "Model training complete.\n",
      "\n",
      "MLPClassifier (Tuned + ADASYN) Report:\n",
      "              precision    recall  f1-score     support\n",
      "0.0            0.842930  0.757450  0.797907  2416.00000\n",
      "1.0            0.708313  0.806689  0.754307  1764.00000\n",
      "accuracy       0.778230  0.778230  0.778230     0.77823\n",
      "macro avg      0.775621  0.782070  0.776107  4180.00000\n",
      "weighted avg   0.786120  0.778230  0.779507  4180.00000\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "print(\"\\n--- 4. Testing MLPClassifier with ADASYN ---\")\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    data_train_full = pandas.read_csv('./data/salary.train.processed.csv').set_index('id')\n",
    "    data_test_mlp = pandas.read_csv('./data/salary.test.processed.csv').set_index('id')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: ไม่พบไฟล์ salary.train.processed.csv กรุณาตรวจสอบ path\")\n",
    "    # exit() \n",
    "\n",
    "X_full = data_train_full.drop(['label'], axis='columns')\n",
    "y_full = data_train_full['label']\n",
    "X_test = data_test_mlp.drop(['label'], axis='columns')\n",
    "y_test = data_test_mlp['label']\n",
    "\n",
    "# --- Apply ADASYN ---\n",
    "print(\"Applying ADASYN...\")\n",
    "ada = ADASYN(random_state=42, n_jobs=-1)\n",
    "X_resampled, y_resampled = ada.fit_resample(X_full, y_full)\n",
    "print(f\"New resampled label distribution:\\n{y_resampled.value_counts()}\")\n",
    "\n",
    "# --- Define Parameters (Translated) ---\n",
    "best_mlp_params = {\n",
    "    'hidden_layer_sizes': (99,),\n",
    "    'activation': 'relu',\n",
    "    'solver': 'adam',\n",
    "    'alpha': 0.0009771293502957021,\n",
    "    'learning_rate_init': 0.0005471619343332291\n",
    "}\n",
    "\n",
    "# --- Create and Train Pipeline ---\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()), # Step 1: Scale\n",
    "    ('model', MLPClassifier(\n",
    "        **best_mlp_params,\n",
    "        # ⚠️ NO 'class_weight'\n",
    "        random_state=42,\n",
    "        max_iter=1000\n",
    "    ))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_resampled, y_resampled) # Train on ADASYN data\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# --- Evaluate ---\n",
    "y_pred = pipeline.predict(X_test)\n",
    "report = classification_report(y_test, y_pred, digits=6, output_dict=True)\n",
    "df_report = pandas.DataFrame(report).transpose()\n",
    "\n",
    "print(\"\\nMLPClassifier (Tuned + ADASYN) Report:\")\n",
    "print(df_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cfd3e7",
   "metadata": {},
   "source": [
    "### SMOTETomek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aab9a677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 3. Testing MLPClassifier with SMOTETomek ---\n",
      "Applying SMOTETomek...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\natth\\anaconda3\\Lib\\site-packages\\imblearn\\over_sampling\\_smote\\base.py:370: FutureWarning: The parameter `n_jobs` has been deprecated in 0.10 and will be removed in 0.12. You can pass an nearest neighbors estimator where `n_jobs` is already set instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New resampled label distribution:\n",
      "label\n",
      "1.0    8914\n",
      "0.0    8914\n",
      "Name: count, dtype: int64\n",
      "Model training complete.\n",
      "\n",
      "MLPClassifier (Tuned + SMOTETomek) Report:\n",
      "              precision    recall  f1-score      support\n",
      "0.0            0.838025  0.800911  0.819048  2416.000000\n",
      "1.0            0.742918  0.787982  0.764787  1764.000000\n",
      "accuracy       0.795455  0.795455  0.795455     0.795455\n",
      "macro avg      0.790472  0.794446  0.791917  4180.000000\n",
      "weighted avg   0.797889  0.795455  0.796149  4180.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "print(\"\\n--- 3. Testing MLPClassifier with SMOTETomek ---\")\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    data_train_full = pandas.read_csv('./data/salary.train.processed.csv').set_index('id')\n",
    "    data_test_mlp = pandas.read_csv('./data/salary.test.processed.csv').set_index('id')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: ไม่พบไฟล์ salary.train.processed.csv กรุณาตรวจสอบ path\")\n",
    "    # exit() \n",
    "\n",
    "X_full = data_train_full.drop(['label'], axis='columns')\n",
    "y_full = data_train_full['label']\n",
    "X_test = data_test_mlp.drop(['label'], axis='columns')\n",
    "y_test = data_test_mlp['label']\n",
    "\n",
    "# --- Apply SMOTETomek ---\n",
    "print(\"Applying SMOTETomek...\")\n",
    "smt = SMOTETomek(random_state=42, n_jobs=-1)\n",
    "X_resampled, y_resampled = smt.fit_resample(X_full, y_full)\n",
    "print(f\"New resampled label distribution:\\n{y_resampled.value_counts()}\")\n",
    "\n",
    "# --- Define Parameters (Translated) ---\n",
    "best_mlp_params = {\n",
    "    'hidden_layer_sizes': (99,),\n",
    "    'activation': 'relu',\n",
    "    'solver': 'adam',\n",
    "    'alpha': 0.0009771293502957021,\n",
    "    'learning_rate_init': 0.0005471619343332291\n",
    "}\n",
    "\n",
    "# --- Create and Train Pipeline ---\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()), # Step 1: Scale\n",
    "    ('model', MLPClassifier(\n",
    "        **best_mlp_params,\n",
    "        # ⚠️ NO 'class_weight'\n",
    "        random_state=42,\n",
    "        max_iter=1000\n",
    "    ))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_resampled, y_resampled) # Train on SMOTETomek data\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# --- Evaluate ---\n",
    "y_pred = pipeline.predict(X_test)\n",
    "report = classification_report(y_test, y_pred, digits=6, output_dict=True)\n",
    "df_report = pandas.DataFrame(report).transpose()\n",
    "\n",
    "print(\"\\nMLPClassifier (Tuned + SMOTETomek) Report:\")\n",
    "print(df_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe867d8c",
   "metadata": {},
   "source": [
    "### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76b68700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 2. Testing MLPClassifier with SMOTE ---\n",
      "Applying SMOTE...\n",
      "New resampled label distribution:\n",
      "label\n",
      "1.0    9719\n",
      "0.0    9719\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\natth\\anaconda3\\Lib\\site-packages\\imblearn\\over_sampling\\_smote\\base.py:370: FutureWarning: The parameter `n_jobs` has been deprecated in 0.10 and will be removed in 0.12. You can pass an nearest neighbors estimator where `n_jobs` is already set instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training complete.\n",
      "\n",
      "MLPClassifier (Tuned + SMOTE) Report:\n",
      "              precision    recall  f1-score      support\n",
      "0.0            0.839664  0.786838  0.812393  2416.000000\n",
      "1.0            0.731211  0.794218  0.761413  1764.000000\n",
      "accuracy       0.789952  0.789952  0.789952     0.789952\n",
      "macro avg      0.785438  0.790528  0.786903  4180.000000\n",
      "weighted avg   0.793896  0.789952  0.790879  4180.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "print(\"\\n--- 2. Testing MLPClassifier with SMOTE ---\")\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    data_train_full = pandas.read_csv('./data/salary.train.processed.csv').set_index('id')\n",
    "    data_test_mlp = pandas.read_csv('./data/salary.test.processed.csv').set_index('id')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: ไม่พบไฟล์ salary.train.processed.csv กรุณาตรวจสอบ path\")\n",
    "    # exit() \n",
    "\n",
    "X_full = data_train_full.drop(['label'], axis='columns')\n",
    "y_full = data_train_full['label']\n",
    "X_test = data_test_mlp.drop(['label'], axis='columns')\n",
    "y_test = data_test_mlp['label']\n",
    "\n",
    "# --- Apply SMOTE ---\n",
    "print(\"Applying SMOTE...\")\n",
    "smote = SMOTE(random_state=42, n_jobs=-1)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_full, y_full)\n",
    "print(f\"New resampled label distribution:\\n{y_resampled.value_counts()}\")\n",
    "\n",
    "# --- Define Parameters (Translated) ---\n",
    "best_mlp_params = {\n",
    "    'hidden_layer_sizes': (99,),\n",
    "    'activation': 'relu',\n",
    "    'solver': 'adam',\n",
    "    'alpha': 0.0009771293502957021,\n",
    "    'learning_rate_init': 0.0005471619343332291\n",
    "}\n",
    "\n",
    "# --- Create and Train Pipeline ---\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()), # Step 1: Scale\n",
    "    ('model', MLPClassifier(\n",
    "        **best_mlp_params,\n",
    "        # ⚠️ NO 'class_weight'\n",
    "        random_state=42,\n",
    "        max_iter=1000\n",
    "    ))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_resampled, y_resampled) # Train on SMOTEd data\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# --- Evaluate ---\n",
    "y_pred = pipeline.predict(X_test)\n",
    "report = classification_report(y_test, y_pred, digits=6, output_dict=True)\n",
    "df_report = pandas.DataFrame(report).transpose()\n",
    "\n",
    "print(\"\\nMLPClassifier (Tuned + SMOTE) Report:\")\n",
    "print(df_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cf3301",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
