{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "219fce59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "data_train = pd.read_csv('./data/salary.train.processed.csv').set_index('id')\n",
    "data_test = pd.read_csv('./data/salary.test.processed.csv').set_index('id')\n",
    "# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î target\n",
    "target = 'label'\n",
    "\n",
    "# ‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ù‡∏∂‡∏Å‡πÅ‡∏•‡∏∞‡∏ó‡∏î‡∏™‡∏≠‡∏ö\n",
    "X_train = data_train.drop(columns=[target])\n",
    "y_train = data_train[target]\n",
    "X_test = data_test.drop(columns=[target])\n",
    "y_test = data_test[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c74a22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in c:\\users\\natth\\anaconda3\\lib\\site-packages (4.5.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in c:\\users\\natth\\anaconda3\\lib\\site-packages (from optuna) (1.13.3)\n",
      "Requirement already satisfied: colorlog in c:\\users\\natth\\anaconda3\\lib\\site-packages (from optuna) (6.10.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\natth\\anaconda3\\lib\\site-packages (from optuna) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\natth\\anaconda3\\lib\\site-packages (from optuna) (24.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in c:\\users\\natth\\anaconda3\\lib\\site-packages (from optuna) (2.0.34)\n",
      "Requirement already satisfied: tqdm in c:\\users\\natth\\anaconda3\\lib\\site-packages (from optuna) (4.66.5)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\natth\\anaconda3\\lib\\site-packages (from optuna) (6.0.1)\n",
      "Requirement already satisfied: Mako in c:\\users\\natth\\anaconda3\\lib\\site-packages (from alembic>=1.5.0->optuna) (1.2.3)\n",
      "Requirement already satisfied: typing-extensions>=4 in c:\\users\\natth\\anaconda3\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.11.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\natth\\anaconda3\\lib\\site-packages (from sqlalchemy>=1.4.2->optuna) (3.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\natth\\anaconda3\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\natth\\anaconda3\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67bfeca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\natth\\anaconda3\\lib\\site-packages (3.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\natth\\anaconda3\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\natth\\anaconda3\\lib\\site-packages (from xgboost) (1.13.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ba9e39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-21 19:35:57,573] A new study created in memory with name: no-name-d2ce3609-1334-41cd-86c1-292aa5a0137d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Optuna study with Early Stopping... (This will take several minutes)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-21 19:36:01,144] Trial 0 finished with value: 0.7923960733249218 and parameters: {'learning_rate': 0.03467852056798425, 'max_depth': 6, 'min_child_weight': 5, 'subsample': 0.6647539487143682, 'colsample_bytree': 0.8001392158689938}. Best is trial 0 with value: 0.7923960733249218.\n",
      "[I 2025-10-21 19:36:05,429] Trial 1 finished with value: 0.7915544890599041 and parameters: {'learning_rate': 0.024233155106460475, 'max_depth': 9, 'min_child_weight': 9, 'subsample': 0.7830017567512916, 'colsample_bytree': 0.9442662901949559}. Best is trial 0 with value: 0.7923960733249218.\n",
      "[I 2025-10-21 19:36:06,951] Trial 2 finished with value: 0.7907780103367749 and parameters: {'learning_rate': 0.09131587491543129, 'max_depth': 10, 'min_child_weight': 7, 'subsample': 0.848748604574615, 'colsample_bytree': 0.9213469913676485}. Best is trial 0 with value: 0.7923960733249218.\n",
      "[I 2025-10-21 19:36:12,529] Trial 3 finished with value: 0.7918808345066847 and parameters: {'learning_rate': 0.013058789466952781, 'max_depth': 9, 'min_child_weight': 8, 'subsample': 0.7215501989304433, 'colsample_bytree': 0.9107686322617673}. Best is trial 0 with value: 0.7923960733249218.\n",
      "[I 2025-10-21 19:36:14,393] Trial 4 finished with value: 0.7917589775937248 and parameters: {'learning_rate': 0.10126117472419877, 'max_depth': 4, 'min_child_weight': 9, 'subsample': 0.7955429524721025, 'colsample_bytree': 0.8916916350762126}. Best is trial 0 with value: 0.7923960733249218.\n",
      "[I 2025-10-21 19:36:17,236] Trial 5 finished with value: 0.7922907091263928 and parameters: {'learning_rate': 0.0330829320870506, 'max_depth': 8, 'min_child_weight': 4, 'subsample': 0.9127565912414153, 'colsample_bytree': 0.9581068308179452}. Best is trial 0 with value: 0.7923960733249218.\n",
      "[I 2025-10-21 19:36:18,318] Trial 6 finished with value: 0.789984360705181 and parameters: {'learning_rate': 0.21484234751479367, 'max_depth': 7, 'min_child_weight': 4, 'subsample': 0.8964793934946298, 'colsample_bytree': 0.6463408685127393}. Best is trial 0 with value: 0.7923960733249218.\n",
      "[I 2025-10-21 19:36:22,720] Trial 7 finished with value: 0.7903744346802751 and parameters: {'learning_rate': 0.03909666665947569, 'max_depth': 3, 'min_child_weight': 7, 'subsample': 0.9479432605980962, 'colsample_bytree': 0.7585047305722141}. Best is trial 0 with value: 0.7923960733249218.\n",
      "[I 2025-10-21 19:36:23,625] Trial 8 finished with value: 0.7903305577838519 and parameters: {'learning_rate': 0.2399919174255505, 'max_depth': 7, 'min_child_weight': 9, 'subsample': 0.9849499018875935, 'colsample_bytree': 0.6800685741448855}. Best is trial 0 with value: 0.7923960733249218.\n",
      "[I 2025-10-21 19:36:28,203] Trial 9 finished with value: 0.7906312995409802 and parameters: {'learning_rate': 0.019476447830336637, 'max_depth': 8, 'min_child_weight': 3, 'subsample': 0.7014710334786911, 'colsample_bytree': 0.723874519310153}. Best is trial 0 with value: 0.7923960733249218.\n",
      "[I 2025-10-21 19:36:29,966] Trial 10 finished with value: 0.7892602442608888 and parameters: {'learning_rate': 0.07226216860410135, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 0.6063139289023953, 'colsample_bytree': 0.8345926497069291}. Best is trial 0 with value: 0.7923960733249218.\n",
      "[I 2025-10-21 19:36:33,170] Trial 11 finished with value: 0.7907857372680689 and parameters: {'learning_rate': 0.03906658149520676, 'max_depth': 6, 'min_child_weight': 5, 'subsample': 0.6218069833316685, 'colsample_bytree': 0.9876719284155768}. Best is trial 0 with value: 0.7923960733249218.\n",
      "[I 2025-10-21 19:36:37,326] Trial 12 finished with value: 0.7923339376321845 and parameters: {'learning_rate': 0.038404612009089456, 'max_depth': 5, 'min_child_weight': 2, 'subsample': 0.8933219533835295, 'colsample_bytree': 0.8444337993926546}. Best is trial 0 with value: 0.7923960733249218.\n",
      "[I 2025-10-21 19:36:46,398] Trial 13 finished with value: 0.7924267354842741 and parameters: {'learning_rate': 0.010920054769705293, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 0.6937799035657021, 'colsample_bytree': 0.8221830424198361}. Best is trial 13 with value: 0.7924267354842741.\n",
      "[I 2025-10-21 19:36:54,375] Trial 14 finished with value: 0.791960723349587 and parameters: {'learning_rate': 0.010216771991305034, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 0.6763433017845152, 'colsample_bytree': 0.7838879160848254}. Best is trial 13 with value: 0.7924267354842741.\n",
      "[I 2025-10-21 19:37:01,206] Trial 15 finished with value: 0.790785220860505 and parameters: {'learning_rate': 0.01946408059469687, 'max_depth': 3, 'min_child_weight': 6, 'subsample': 0.742082181069819, 'colsample_bytree': 0.839177817957261}. Best is trial 13 with value: 0.7924267354842741.\n",
      "[I 2025-10-21 19:37:02,594] Trial 16 finished with value: 0.790698203568711 and parameters: {'learning_rate': 0.13022363950029406, 'max_depth': 6, 'min_child_weight': 3, 'subsample': 0.6543427881777537, 'colsample_bytree': 0.6025668759976355}. Best is trial 13 with value: 0.7924267354842741.\n",
      "[I 2025-10-21 19:37:09,672] Trial 17 finished with value: 0.7904437790774957 and parameters: {'learning_rate': 0.014199809790672141, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 0.758532557607718, 'colsample_bytree': 0.7258179048837688}. Best is trial 13 with value: 0.7924267354842741.\n",
      "[I 2025-10-21 19:37:11,703] Trial 18 finished with value: 0.7897712836918928 and parameters: {'learning_rate': 0.05053961273907127, 'max_depth': 6, 'min_child_weight': 2, 'subsample': 0.6513633246578259, 'colsample_bytree': 0.8045937088254336}. Best is trial 13 with value: 0.7924267354842741.\n",
      "[I 2025-10-21 19:37:16,429] Trial 19 finished with value: 0.7923938810877728 and parameters: {'learning_rate': 0.0251922733059082, 'max_depth': 4, 'min_child_weight': 10, 'subsample': 0.6941948445189938, 'colsample_bytree': 0.8621864979080305}. Best is trial 13 with value: 0.7924267354842741.\n",
      "[I 2025-10-21 19:37:18,393] Trial 20 finished with value: 0.7903600193517599 and parameters: {'learning_rate': 0.06110888523386063, 'max_depth': 7, 'min_child_weight': 6, 'subsample': 0.8356100946576258, 'colsample_bytree': 0.7828174427191739}. Best is trial 13 with value: 0.7924267354842741.\n",
      "[I 2025-10-21 19:37:23,420] Trial 21 finished with value: 0.792389109368053 and parameters: {'learning_rate': 0.02430249877838665, 'max_depth': 4, 'min_child_weight': 10, 'subsample': 0.7019033022212395, 'colsample_bytree': 0.8712119062078134}. Best is trial 13 with value: 0.7924267354842741.\n",
      "[I 2025-10-21 19:37:30,108] Trial 22 finished with value: 0.7928808074744044 and parameters: {'learning_rate': 0.015176713006944701, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 0.6741520415753935, 'colsample_bytree': 0.8689247927552561}. Best is trial 22 with value: 0.7928808074744044.\n",
      "[I 2025-10-21 19:37:38,757] Trial 23 finished with value: 0.7928559225194287 and parameters: {'learning_rate': 0.011121833159774495, 'max_depth': 5, 'min_child_weight': 7, 'subsample': 0.6455904585313139, 'colsample_bytree': 0.8098255218219637}. Best is trial 22 with value: 0.7928808074744044.\n",
      "[I 2025-10-21 19:37:46,680] Trial 24 finished with value: 0.7920923692363014 and parameters: {'learning_rate': 0.010603368435806856, 'max_depth': 5, 'min_child_weight': 8, 'subsample': 0.6308529316061386, 'colsample_bytree': 0.7452495849400552}. Best is trial 22 with value: 0.7928808074744044.\n",
      "[I 2025-10-21 19:37:54,015] Trial 25 finished with value: 0.7922963228313288 and parameters: {'learning_rate': 0.014499747874601853, 'max_depth': 5, 'min_child_weight': 7, 'subsample': 0.7394142585163274, 'colsample_bytree': 0.8278303211645417}. Best is trial 22 with value: 0.7928808074744044.\n",
      "[I 2025-10-21 19:38:01,237] Trial 26 finished with value: 0.7907191438152088 and parameters: {'learning_rate': 0.017452547322309605, 'max_depth': 3, 'min_child_weight': 8, 'subsample': 0.6006999054811323, 'colsample_bytree': 0.8890555461625028}. Best is trial 22 with value: 0.7928808074744044.\n",
      "[I 2025-10-21 19:38:08,811] Trial 27 finished with value: 0.7910420736211798 and parameters: {'learning_rate': 0.012412837199148576, 'max_depth': 4, 'min_child_weight': 10, 'subsample': 0.6326981811952914, 'colsample_bytree': 0.8112329827214483}. Best is trial 22 with value: 0.7928808074744044.\n",
      "[I 2025-10-21 19:38:16,747] Trial 28 finished with value: 0.7919734927792642 and parameters: {'learning_rate': 0.010000426494609888, 'max_depth': 5, 'min_child_weight': 6, 'subsample': 0.7663790078137518, 'colsample_bytree': 0.7688624804112126}. Best is trial 22 with value: 0.7928808074744044.\n",
      "[I 2025-10-21 19:38:21,981] Trial 29 finished with value: 0.7930624999695053 and parameters: {'learning_rate': 0.016936740838045274, 'max_depth': 6, 'min_child_weight': 4, 'subsample': 0.6728417531696815, 'colsample_bytree': 0.8734308880973841}. Best is trial 29 with value: 0.7930624999695053.\n",
      "[I 2025-10-21 19:38:27,418] Trial 30 finished with value: 0.7911682714602124 and parameters: {'learning_rate': 0.01627973551033597, 'max_depth': 6, 'min_child_weight': 4, 'subsample': 0.6664172282057741, 'colsample_bytree': 0.8601812098168841}. Best is trial 29 with value: 0.7930624999695053.\n",
      "[I 2025-10-21 19:38:34,117] Trial 31 finished with value: 0.7930601404439598 and parameters: {'learning_rate': 0.012589621838586782, 'max_depth': 6, 'min_child_weight': 2, 'subsample': 0.6786626126467333, 'colsample_bytree': 0.8173139120451615}. Best is trial 29 with value: 0.7930624999695053.\n",
      "[I 2025-10-21 19:38:37,781] Trial 32 finished with value: 0.7913786117920195 and parameters: {'learning_rate': 0.027446158327732395, 'max_depth': 6, 'min_child_weight': 3, 'subsample': 0.6720154308554972, 'colsample_bytree': 0.9349278547130051}. Best is trial 29 with value: 0.7930624999695053.\n",
      "[I 2025-10-21 19:38:41,515] Trial 33 finished with value: 0.7927915031002954 and parameters: {'learning_rate': 0.020487108113280115, 'max_depth': 7, 'min_child_weight': 2, 'subsample': 0.7202616795055872, 'colsample_bytree': 0.8911230009413394}. Best is trial 29 with value: 0.7930624999695053.\n",
      "[I 2025-10-21 19:38:46,360] Trial 34 finished with value: 0.7900040124474824 and parameters: {'learning_rate': 0.01607464605088807, 'max_depth': 8, 'min_child_weight': 4, 'subsample': 0.6396915846437079, 'colsample_bytree': 0.7965900612243906}. Best is trial 29 with value: 0.7930624999695053.\n",
      "[I 2025-10-21 19:38:49,757] Trial 35 finished with value: 0.7923493211887882 and parameters: {'learning_rate': 0.02944464627475678, 'max_depth': 6, 'min_child_weight': 9, 'subsample': 0.8212905610397949, 'colsample_bytree': 0.965536908278162}. Best is trial 29 with value: 0.7930624999695053.\n",
      "[I 2025-10-21 19:38:57,881] Trial 36 finished with value: 0.7919796912180953 and parameters: {'learning_rate': 0.012869394913649686, 'max_depth': 6, 'min_child_weight': 5, 'subsample': 0.6793566484992225, 'colsample_bytree': 0.9145325822158697}. Best is trial 29 with value: 0.7930624999695053.\n",
      "[I 2025-10-21 19:39:03,807] Trial 37 finished with value: 0.7905826070496356 and parameters: {'learning_rate': 0.012875138580365535, 'max_depth': 10, 'min_child_weight': 8, 'subsample': 0.7193007501168707, 'colsample_bytree': 0.877847154292503}. Best is trial 29 with value: 0.7930624999695053.\n",
      "[I 2025-10-21 19:39:08,288] Trial 38 finished with value: 0.7914238625345829 and parameters: {'learning_rate': 0.02111428093340108, 'max_depth': 7, 'min_child_weight': 7, 'subsample': 0.7743819666182797, 'colsample_bytree': 0.8551431740690869}. Best is trial 29 with value: 0.7930624999695053.\n",
      "[I 2025-10-21 19:39:15,733] Trial 39 finished with value: 0.7902662797938543 and parameters: {'learning_rate': 0.016306862437808982, 'max_depth': 4, 'min_child_weight': 3, 'subsample': 0.8037815812659432, 'colsample_bytree': 0.9108343781402182}. Best is trial 29 with value: 0.7930624999695053.\n",
      "[I 2025-10-21 19:39:19,791] Trial 40 finished with value: 0.7907105157871452 and parameters: {'learning_rate': 0.03146114482650392, 'max_depth': 8, 'min_child_weight': 4, 'subsample': 0.7375442783169767, 'colsample_bytree': 0.9404276782293726}. Best is trial 29 with value: 0.7930624999695053.\n",
      "[I 2025-10-21 19:39:25,134] Trial 41 finished with value: 0.7917576748116008 and parameters: {'learning_rate': 0.021007370459504127, 'max_depth': 7, 'min_child_weight': 2, 'subsample': 0.7090080460599055, 'colsample_bytree': 0.8951458818541776}. Best is trial 29 with value: 0.7930624999695053.\n",
      "[I 2025-10-21 19:39:31,439] Trial 42 finished with value: 0.7913193831470625 and parameters: {'learning_rate': 0.018326476043466426, 'max_depth': 9, 'min_child_weight': 2, 'subsample': 0.6495896466752175, 'colsample_bytree': 0.8911290081964609}. Best is trial 29 with value: 0.7930624999695053.\n",
      "[I 2025-10-21 19:39:37,458] Trial 43 finished with value: 0.7904825866707825 and parameters: {'learning_rate': 0.022305899102401827, 'max_depth': 7, 'min_child_weight': 3, 'subsample': 0.6211173136893852, 'colsample_bytree': 0.8197360029886606}. Best is trial 29 with value: 0.7930624999695053.\n",
      "[I 2025-10-21 19:39:47,343] Trial 44 finished with value: 0.7921642261702604 and parameters: {'learning_rate': 0.01165465059784203, 'max_depth': 6, 'min_child_weight': 2, 'subsample': 0.6843891382025264, 'colsample_bytree': 0.8440091259365091}. Best is trial 29 with value: 0.7930624999695053.\n",
      "[I 2025-10-21 19:39:53,928] Trial 45 finished with value: 0.7918037185501717 and parameters: {'learning_rate': 0.01458422381027772, 'max_depth': 7, 'min_child_weight': 9, 'subsample': 0.7287445702529977, 'colsample_bytree': 0.9276449383278742}. Best is trial 29 with value: 0.7930624999695053.\n",
      "[I 2025-10-21 19:39:55,233] Trial 46 finished with value: 0.792025645313847 and parameters: {'learning_rate': 0.1532272923714732, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 0.7172634478357682, 'colsample_bytree': 0.9543710155884086}. Best is trial 29 with value: 0.7930624999695053.\n",
      "[I 2025-10-21 19:39:57,968] Trial 47 finished with value: 0.7911971045514243 and parameters: {'learning_rate': 0.047718319026835, 'max_depth': 8, 'min_child_weight': 4, 'subsample': 0.6602112739886183, 'colsample_bytree': 0.8799354706217842}. Best is trial 29 with value: 0.7930624999695053.\n",
      "[I 2025-10-21 19:40:07,021] Trial 48 finished with value: 0.791397438456475 and parameters: {'learning_rate': 0.01452823419189209, 'max_depth': 5, 'min_child_weight': 1, 'subsample': 0.614994043362349, 'colsample_bytree': 0.9001416617182922}. Best is trial 29 with value: 0.7930624999695053.\n",
      "[I 2025-10-21 19:40:15,041] Trial 49 finished with value: 0.7910572416763207 and parameters: {'learning_rate': 0.01191076174433366, 'max_depth': 9, 'min_child_weight': 3, 'subsample': 0.8654699893236967, 'colsample_bytree': 0.9996215400685867}. Best is trial 29 with value: 0.7930624999695053.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study complete!\n",
      "\n",
      "Best trial:\n",
      "  Value (Mean F1): 0.7931\n",
      "  Best Params: \n",
      "{'learning_rate': 0.016936740838045274, 'max_depth': 6, 'min_child_weight': 4, 'subsample': 0.6728417531696815, 'colsample_bytree': 0.8734308880973841}\n",
      "\n",
      "Optuna-Tuned XGBoost (w/ Early Stopping) Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.84      0.85      2416\n",
      "         1.0       0.79      0.81      0.80      1764\n",
      "\n",
      "    accuracy                           0.83      4180\n",
      "   macro avg       0.83      0.83      0.83      4180\n",
      "weighted avg       0.83      0.83      0.83      4180\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Step 1: Define the Advanced Objective Function ---\n",
    "def objective_with_es(trial):\n",
    "    \"\"\"\n",
    "    Objective function with manual Cross-Validation and Early Stopping.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Define the search space for the parameters\n",
    "    param = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'logloss',\n",
    "        'random_state': 42,\n",
    "        'use_label_encoder': False,\n",
    "        'n_estimators': 1000, \n",
    "        \n",
    "        # --- THIS IS THE FIX ---\n",
    "        'early_stopping_rounds': 50, # MOVED HERE\n",
    "        \n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0)\n",
    "    }\n",
    "    \n",
    "    # 2. Set up 3-Fold Cross-Validation\n",
    "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "\n",
    "    # 3. Manually run the CV loop\n",
    "    for train_idx, val_idx in skf.split(X_train, y_train):\n",
    "        X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        \n",
    "        model = XGBClassifier(**param)\n",
    "        \n",
    "        # --- THIS IS THE FIX ---\n",
    "        # We pass eval_set, but 'early_stopping_rounds' is already in the model\n",
    "        model.fit(\n",
    "            X_train_fold, \n",
    "            y_train_fold,\n",
    "            eval_set=[(X_val_fold, y_val_fold)],\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        preds = model.predict(X_val_fold)\n",
    "        f1 = f1_score(y_val_fold, preds, average='binary')\n",
    "        scores.append(f1)\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "# --- Step 2: Create and Run the Study ---\n",
    "print(\"Starting Optuna study with Early Stopping... (This will take several minutes)\")\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective_with_es, n_trials=50) # 50 trials\n",
    "print(\"Study complete!\")\n",
    "\n",
    "# --- Step 3: Get Best Params ---\n",
    "print(\"\\nBest trial:\")\n",
    "print(f\"  Value (Mean F1): {study.best_value:.4f}\")\n",
    "print(\"  Best Params: \")\n",
    "# We need to remove the key we manually added\n",
    "best_params_from_study = {k: v for k, v in study.best_params.items() if k != 'early_stopping_rounds'}\n",
    "print(best_params_from_study)\n",
    "\n",
    "\n",
    "# --- Step 4: Train the FINAL Model (Also Fixed) ---\n",
    "X_train_final, X_val_final, y_train_final, y_val_final = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "# Create the final model with the fix\n",
    "final_xgb = XGBClassifier(\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    n_estimators=1000,\n",
    "    early_stopping_rounds=50, # --- FIX APPLIED HERE ---\n",
    "    **best_params_from_study  # Use best params from Optuna\n",
    ")\n",
    "\n",
    "# Train it with the fix\n",
    "final_xgb.fit(\n",
    "    X_train_final, \n",
    "    y_train_final,\n",
    "    eval_set=[(X_val_final, y_val_final)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# --- Step 5: Evaluate the Optuna-Tuned Model on the TEST set ---\n",
    "y_pred_optuna_es = final_xgb.predict(X_test)\n",
    "\n",
    "print(\"\\nOptuna-Tuned XGBoost (w/ Early Stopping) Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_optuna_es))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc272c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {\n",
    "    'learning_rate': 0.016936740838045274, \n",
    "    'max_depth': 6, \n",
    "    'min_child_weight':  4, \n",
    "    'subsample': 0.6728417531696815, \n",
    "    'colsample_bytree': 0.8734308880973841\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ef57e5",
   "metadata": {},
   "source": [
    "### Class Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c231e687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Testing XGBoost with Class Weight ---\n",
      "Using scale_pos_weight: 1.3882\n",
      "Model training complete.\n",
      "\n",
      "XGBoost (Tuned + Class Weight) Report:\n",
      "              precision    recall  f1-score      support\n",
      "0.0            0.886023  0.781871  0.830695  2416.000000\n",
      "1.0            0.742676  0.862245  0.798006  1764.000000\n",
      "accuracy       0.815789  0.815789  0.815789     0.815789\n",
      "macro avg      0.814349  0.822058  0.814351  4180.000000\n",
      "weighted avg   0.825529  0.815789  0.816900  4180.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import xgboost as xgb\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"--- 1. Testing XGBoost with Class Weight ---\")\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    data_train_full = pandas.read_csv('./data/salary.train.processed.csv').set_index('id')\n",
    "    data_test_xgb = pandas.read_csv('./data/salary.test.processed.csv').set_index('id')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå salary.train.processed.csv ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö path\")\n",
    "    # exit() \n",
    "\n",
    "X_full = data_train_full.drop(['label'], axis='columns')\n",
    "y_full = data_train_full['label']\n",
    "X_test = data_test_xgb.drop(['label'], axis='columns')\n",
    "y_test = data_test_xgb['label']\n",
    "\n",
    "# --- Calculate Weight ---\n",
    "scale_pos_weight = len(y_full[y_full == 0]) / len(y_full[y_full == 1])\n",
    "print(f\"Using scale_pos_weight: {scale_pos_weight:.4f}\")\n",
    "\n",
    "# --- Define Parameters ---\n",
    "best_xgb_params = best_params\n",
    "\n",
    "# --- Create and Train Model ---\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    **best_xgb_params,\n",
    "    scale_pos_weight=scale_pos_weight, # üëà Add weight\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_full, y_full) # Train on original data\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# --- Evaluate ---\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "report = classification_report(y_test, y_pred, digits=6, output_dict=True)\n",
    "df_report = pandas.DataFrame(report).transpose()\n",
    "\n",
    "print(\"\\nXGBoost (Tuned + Class Weight) Report:\")\n",
    "print(df_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1488c3",
   "metadata": {},
   "source": [
    "### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "751afeaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 2. Testing XGBoost with SMOTE ---\n",
      "Applying SMOTE...\n",
      "New resampled label distribution:\n",
      "label\n",
      "1.0    9719\n",
      "0.0    9719\n",
      "Name: count, dtype: int64\n",
      "Model training complete.\n",
      "\n",
      "XGBoost (Tuned + SMOTE) Report:\n",
      "              precision    recall  f1-score      support\n",
      "0.0            0.884255  0.781043  0.829451  2416.000000\n",
      "1.0            0.741447  0.859977  0.796325  1764.000000\n",
      "accuracy       0.814354  0.814354  0.814354     0.814354\n",
      "macro avg      0.812851  0.820510  0.812888  4180.000000\n",
      "weighted avg   0.823988  0.814354  0.815471  4180.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import xgboost as xgb\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "print(\"\\n--- 2. Testing XGBoost with SMOTE ---\")\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    data_train_full = pandas.read_csv('./data/salary.train.processed.csv').set_index('id')\n",
    "    data_test_xgb = pandas.read_csv('./data/salary.test.processed.csv').set_index('id')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå salary.train.processed.csv ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö path\")\n",
    "    # exit() \n",
    "\n",
    "X_full = data_train_full.drop(['label'], axis='columns')\n",
    "y_full = data_train_full['label']\n",
    "X_test = data_test_xgb.drop(['label'], axis='columns')\n",
    "y_test = data_test_xgb['label']\n",
    "\n",
    "# --- Apply SMOTE ---\n",
    "print(\"Applying SMOTE...\")\n",
    "smote = SMOTE(random_state=42, n_jobs=-1)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_full, y_full)\n",
    "print(f\"New resampled label distribution:\\n{y_resampled.value_counts()}\")\n",
    "\n",
    "# --- Define Parameters ---\n",
    "best_xgb_params = best_params\n",
    "\n",
    "# --- Create and Train Model ---\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    **best_xgb_params,\n",
    "    # ‚ö†Ô∏è NO 'scale_pos_weight'\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_resampled, y_resampled) # Train on SMOTEd data\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# --- Evaluate ---\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "report = classification_report(y_test, y_pred, digits=6, output_dict=True)\n",
    "df_report = pandas.DataFrame(report).transpose()\n",
    "\n",
    "print(\"\\nXGBoost (Tuned + SMOTE) Report:\")\n",
    "print(df_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60c1463",
   "metadata": {},
   "source": [
    "### SMOTETomek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3555419b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 3. Testing XGBoost with SMOTETomek ---\n",
      "Applying SMOTETomek...\n",
      "New resampled label distribution:\n",
      "label\n",
      "1.0    8914\n",
      "0.0    8914\n",
      "Name: count, dtype: int64\n",
      "Model training complete.\n",
      "\n",
      "XGBoost (Tuned + SMOTETomek) Report:\n",
      "              precision    recall  f1-score      support\n",
      "0.0            0.883721  0.786424  0.832238  2416.000000\n",
      "1.0            0.745813  0.858277  0.798102  1764.000000\n",
      "accuracy       0.816746  0.816746  0.816746     0.816746\n",
      "macro avg      0.814767  0.822350  0.815170  4180.000000\n",
      "weighted avg   0.825522  0.816746  0.817833  4180.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import xgboost as xgb\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "print(\"\\n--- 3. Testing XGBoost with SMOTETomek ---\")\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    data_train_full = pandas.read_csv('./data/salary.train.processed.csv').set_index('id')\n",
    "    data_test_xgb = pandas.read_csv('./data/salary.test.processed.csv').set_index('id')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå salary.train.processed.csv ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö path\")\n",
    "    # exit() \n",
    "\n",
    "X_full = data_train_full.drop(['label'], axis='columns')\n",
    "y_full = data_train_full['label']\n",
    "X_test = data_test_xgb.drop(['label'], axis='columns')\n",
    "y_test = data_test_xgb['label']\n",
    "\n",
    "# --- Apply SMOTETomek ---\n",
    "print(\"Applying SMOTETomek...\")\n",
    "smt = SMOTETomek(random_state=42, n_jobs=-1)\n",
    "X_resampled, y_resampled = smt.fit_resample(X_full, y_full)\n",
    "print(f\"New resampled label distribution:\\n{y_resampled.value_counts()}\")\n",
    "\n",
    "# --- Define Parameters ---\n",
    "best_xgb_params = best_params\n",
    "\n",
    "# --- Create and Train Model ---\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    **best_xgb_params,\n",
    "    # ‚ö†Ô∏è NO 'scale_pos_weight'\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_resampled, y_resampled) # Train on SMOTETomek data\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# --- Evaluate ---\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "report = classification_report(y_test, y_pred, digits=6, output_dict=True)\n",
    "df_report = pandas.DataFrame(report).transpose()\n",
    "\n",
    "print(\"\\nXGBoost (Tuned + SMOTETomek) Report:\")\n",
    "print(df_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9e8f9e",
   "metadata": {},
   "source": [
    "### ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3705b63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 4. Testing XGBoost with ADASYN ---\n",
      "Applying ADASYN...\n",
      "New resampled label distribution:\n",
      "label\n",
      "1.0    9726\n",
      "0.0    9719\n",
      "Name: count, dtype: int64\n",
      "Model training complete.\n",
      "\n",
      "XGBoost (Tuned + ADASYN) Report:\n",
      "              precision    recall  f1-score      support\n",
      "0.0            0.891398  0.767798  0.824994  2416.000000\n",
      "1.0            0.732730  0.871882  0.796272  1764.000000\n",
      "accuracy       0.811722  0.811722  0.811722     0.811722\n",
      "macro avg      0.812064  0.819840  0.810633  4180.000000\n",
      "weighted avg   0.824439  0.811722  0.812873  4180.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import xgboost as xgb\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "print(\"\\n--- 4. Testing XGBoost with ADASYN ---\")\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    data_train_full = pandas.read_csv('./data/salary.train.processed.csv').set_index('id')\n",
    "    data_test_xgb = pandas.read_csv('./data/salary.test.processed.csv').set_index('id')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå salary.train.processed.csv ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö path\")\n",
    "    # exit() \n",
    "\n",
    "X_full = data_train_full.drop(['label'], axis='columns')\n",
    "y_full = data_train_full['label']\n",
    "X_test = data_test_xgb.drop(['label'], axis='columns')\n",
    "y_test = data_test_xgb['label']\n",
    "\n",
    "# --- Apply ADASYN ---\n",
    "print(\"Applying ADASYN...\")\n",
    "ada = ADASYN(random_state=42, n_jobs=-1)\n",
    "X_resampled, y_resampled = ada.fit_resample(X_full, y_full)\n",
    "print(f\"New resampled label distribution:\\n{y_resampled.value_counts()}\")\n",
    "\n",
    "# --- Define Parameters ---\n",
    "best_xgb_params = best_params\n",
    "\n",
    "# --- Create and Train Model ---\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    **best_xgb_params,\n",
    "    # ‚ö†Ô∏è NO 'scale_pos_weight'\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_resampled, y_resampled) # Train on ADASYN data\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# --- Evaluate ---\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "report = classification_report(y_test, y_pred, digits=6, output_dict=True)\n",
    "df_report = pandas.DataFrame(report).transpose()\n",
    "\n",
    "print(\"\\nXGBoost (Tuned + ADASYN) Report:\")\n",
    "print(df_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f54cb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
