{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "219fce59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "data_train = pd.read_csv('./data/salary.train.processed.csv').set_index('id')\n",
    "data_test = pd.read_csv('./data/salary.test.processed.csv').set_index('id')\n",
    "# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î target\n",
    "target = 'label'\n",
    "\n",
    "# ‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ù‡∏∂‡∏Å‡πÅ‡∏•‡∏∞‡∏ó‡∏î‡∏™‡∏≠‡∏ö\n",
    "X_train = data_train.drop(columns=[target])\n",
    "y_train = data_train[target]\n",
    "X_test = data_test.drop(columns=[target])\n",
    "y_test = data_test[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c74a22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in c:\\users\\natth\\anaconda3\\lib\\site-packages (4.5.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in c:\\users\\natth\\anaconda3\\lib\\site-packages (from optuna) (1.13.3)\n",
      "Requirement already satisfied: colorlog in c:\\users\\natth\\anaconda3\\lib\\site-packages (from optuna) (6.10.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\natth\\anaconda3\\lib\\site-packages (from optuna) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\natth\\anaconda3\\lib\\site-packages (from optuna) (24.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in c:\\users\\natth\\anaconda3\\lib\\site-packages (from optuna) (2.0.34)\n",
      "Requirement already satisfied: tqdm in c:\\users\\natth\\anaconda3\\lib\\site-packages (from optuna) (4.66.5)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\natth\\anaconda3\\lib\\site-packages (from optuna) (6.0.1)\n",
      "Requirement already satisfied: Mako in c:\\users\\natth\\anaconda3\\lib\\site-packages (from alembic>=1.5.0->optuna) (1.2.3)\n",
      "Requirement already satisfied: typing-extensions>=4 in c:\\users\\natth\\anaconda3\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.11.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\natth\\anaconda3\\lib\\site-packages (from sqlalchemy>=1.4.2->optuna) (3.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\natth\\anaconda3\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\natth\\anaconda3\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67bfeca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\natth\\anaconda3\\lib\\site-packages (3.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\natth\\anaconda3\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\natth\\anaconda3\\lib\\site-packages (from xgboost) (1.13.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02a855f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "XGBoost Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0   0.858180  0.831540  0.844650      2416\n",
      "         1.0   0.778684  0.811791  0.794893      1764\n",
      "\n",
      "    accuracy                       0.823206      4180\n",
      "   macro avg   0.818432  0.821666  0.819772      4180\n",
      "weighted avg   0.824632  0.823206  0.823652      4180\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    train_df = pandas.read_csv('./data/salary.train.processed.csv', index_col='id')\n",
    "    test_df = pandas.read_csv('./data/salary.test.processed.csv', index_col='id')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Could not find the processed CSV files.\")\n",
    "    raise\n",
    "\n",
    "X_train = train_df.drop('label', axis=1)\n",
    "y_train = train_df['label']\n",
    "X_test = test_df.drop('label', axis=1)\n",
    "y_test = test_df['label']\n",
    "# --- End of Data Loading ---\n",
    "\n",
    "# --- Train-Validation Split ---\n",
    "X_train_final, X_val_final, y_train_final, y_val_final = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "# --- Define XGBoost Model with Default Parameters ---\n",
    "xgb_model = XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    use_label_encoder=False,\n",
    "    random_state=42,\n",
    "    n_estimators=1000,\n",
    "    early_stopping_rounds=50,\n",
    "    learning_rate=0.1,  # ‡∏Ñ‡πà‡∏≤ default\n",
    "    max_depth=6,        # ‡∏Ñ‡πà‡∏≤ default\n",
    "    min_child_weight=1, # ‡∏Ñ‡πà‡∏≤ default\n",
    "    subsample=1.0,      # ‡∏Ñ‡πà‡∏≤ default\n",
    "    colsample_bytree=1.0  # ‡∏Ñ‡πà‡∏≤ default\n",
    ")\n",
    "\n",
    "# --- Train the Model with Early Stopping ---\n",
    "xgb_model.fit(\n",
    "    X_train_final,\n",
    "    y_train_final,\n",
    "    eval_set=[(X_val_final, y_val_final)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# --- Evaluate on Test Set ---\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "print(\"\\nXGBoost Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_xgb, digits=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ba9e39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-22 06:44:57,907] A new study created in memory with name: no-name-da3ba00a-7ad3-4ea6-95ef-cf5c92bf4062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Optuna study with Early Stopping... (This will take several minutes)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-22 06:45:00,437] Trial 0 finished with value: 0.7899229277888272 and parameters: {'learning_rate': 0.03747553201879854, 'max_depth': 10, 'min_child_weight': 6, 'subsample': 0.9945720759070314, 'colsample_bytree': 0.8281161569147458}. Best is trial 0 with value: 0.7899229277888272.\n",
      "[I 2025-10-22 06:45:01,775] Trial 1 finished with value: 0.7909559391006252 and parameters: {'learning_rate': 0.13816489836748594, 'max_depth': 5, 'min_child_weight': 4, 'subsample': 0.9706146674831886, 'colsample_bytree': 0.9633703784494705}. Best is trial 1 with value: 0.7909559391006252.\n",
      "[I 2025-10-22 06:45:07,234] Trial 2 finished with value: 0.7885567827087069 and parameters: {'learning_rate': 0.013558447735419735, 'max_depth': 10, 'min_child_weight': 9, 'subsample': 0.9512517865035529, 'colsample_bytree': 0.6557998803383025}. Best is trial 1 with value: 0.7909559391006252.\n",
      "[I 2025-10-22 06:45:08,822] Trial 3 finished with value: 0.7866435362231391 and parameters: {'learning_rate': 0.055987727995026104, 'max_depth': 10, 'min_child_weight': 9, 'subsample': 0.6307011368824545, 'colsample_bytree': 0.7432368171424076}. Best is trial 1 with value: 0.7909559391006252.\n",
      "[I 2025-10-22 06:45:10,756] Trial 4 finished with value: 0.7898593017836303 and parameters: {'learning_rate': 0.04815002211871403, 'max_depth': 10, 'min_child_weight': 3, 'subsample': 0.9325356464643659, 'colsample_bytree': 0.9916435784401109}. Best is trial 1 with value: 0.7909559391006252.\n",
      "[I 2025-10-22 06:45:15,636] Trial 5 finished with value: 0.7881478959637068 and parameters: {'learning_rate': 0.02448689135377512, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 0.6033342840093274, 'colsample_bytree': 0.800683406651269}. Best is trial 1 with value: 0.7909559391006252.\n",
      "[I 2025-10-22 06:45:16,430] Trial 6 finished with value: 0.7843722279920198 and parameters: {'learning_rate': 0.28668720694424354, 'max_depth': 8, 'min_child_weight': 1, 'subsample': 0.7823146870241406, 'colsample_bytree': 0.9484803623232198}. Best is trial 1 with value: 0.7909559391006252.\n",
      "[I 2025-10-22 06:45:22,623] Trial 7 finished with value: 0.7902905570891868 and parameters: {'learning_rate': 0.010991582093648443, 'max_depth': 9, 'min_child_weight': 6, 'subsample': 0.8219296454516108, 'colsample_bytree': 0.7508550839640776}. Best is trial 1 with value: 0.7909559391006252.\n",
      "[I 2025-10-22 06:45:23,829] Trial 8 finished with value: 0.7884460548556547 and parameters: {'learning_rate': 0.0805360649951239, 'max_depth': 10, 'min_child_weight': 10, 'subsample': 0.7514737353547979, 'colsample_bytree': 0.7669886075592879}. Best is trial 1 with value: 0.7909559391006252.\n",
      "[I 2025-10-22 06:45:24,654] Trial 9 finished with value: 0.7890962306974308 and parameters: {'learning_rate': 0.1931622014143557, 'max_depth': 4, 'min_child_weight': 4, 'subsample': 0.7904691145149061, 'colsample_bytree': 0.9944439750703267}. Best is trial 1 with value: 0.7909559391006252.\n",
      "[I 2025-10-22 06:45:25,829] Trial 10 finished with value: 0.7874938457379308 and parameters: {'learning_rate': 0.12082485593519236, 'max_depth': 6, 'min_child_weight': 1, 'subsample': 0.8790629602146542, 'colsample_bytree': 0.890896934629593}. Best is trial 1 with value: 0.7909559391006252.\n",
      "[I 2025-10-22 06:45:33,486] Trial 11 finished with value: 0.7895144447509564 and parameters: {'learning_rate': 0.010004932718708151, 'max_depth': 6, 'min_child_weight': 7, 'subsample': 0.8636285741463879, 'colsample_bytree': 0.6820192067891225}. Best is trial 1 with value: 0.7909559391006252.\n",
      "[I 2025-10-22 06:45:34,496] Trial 12 finished with value: 0.7893033242723607 and parameters: {'learning_rate': 0.12424926072804335, 'max_depth': 8, 'min_child_weight': 7, 'subsample': 0.7126235240917466, 'colsample_bytree': 0.8667890699635074}. Best is trial 1 with value: 0.7909559391006252.\n",
      "[I 2025-10-22 06:45:38,220] Trial 13 finished with value: 0.7903551957922489 and parameters: {'learning_rate': 0.021533356969908286, 'max_depth': 5, 'min_child_weight': 3, 'subsample': 0.8568579568900682, 'colsample_bytree': 0.7138178944330785}. Best is trial 1 with value: 0.7909559391006252.\n",
      "[I 2025-10-22 06:45:41,760] Trial 14 finished with value: 0.7903107629024074 and parameters: {'learning_rate': 0.023377355162616364, 'max_depth': 5, 'min_child_weight': 3, 'subsample': 0.8844256589143838, 'colsample_bytree': 0.6262937137388846}. Best is trial 1 with value: 0.7909559391006252.\n",
      "[I 2025-10-22 06:45:46,358] Trial 15 finished with value: 0.7909658460858023 and parameters: {'learning_rate': 0.022196447946570035, 'max_depth': 5, 'min_child_weight': 3, 'subsample': 0.9776669580347536, 'colsample_bytree': 0.7079206784134698}. Best is trial 15 with value: 0.7909658460858023.\n",
      "[I 2025-10-22 06:45:48,600] Trial 16 finished with value: 0.7900258725331617 and parameters: {'learning_rate': 0.08506327574635239, 'max_depth': 3, 'min_child_weight': 2, 'subsample': 0.9614210753237178, 'colsample_bytree': 0.9155187264788445}. Best is trial 15 with value: 0.7909658460858023.\n",
      "[I 2025-10-22 06:45:51,326] Trial 17 finished with value: 0.7914614664485703 and parameters: {'learning_rate': 0.03482540590102811, 'max_depth': 5, 'min_child_weight': 4, 'subsample': 0.9177407404987542, 'colsample_bytree': 0.8438277292242202}. Best is trial 17 with value: 0.7914614664485703.\n",
      "[I 2025-10-22 06:45:54,400] Trial 18 finished with value: 0.7908400327560695 and parameters: {'learning_rate': 0.03307660912782304, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 0.9118212097418024, 'colsample_bytree': 0.8317339046774624}. Best is trial 17 with value: 0.7914614664485703.\n",
      "[I 2025-10-22 06:45:58,692] Trial 19 finished with value: 0.790535153180612 and parameters: {'learning_rate': 0.017145648168980742, 'max_depth': 7, 'min_child_weight': 2, 'subsample': 0.9922109829202843, 'colsample_bytree': 0.7065637806459166}. Best is trial 17 with value: 0.7914614664485703.\n",
      "[I 2025-10-22 06:46:01,073] Trial 20 finished with value: 0.7896576823201285 and parameters: {'learning_rate': 0.03337655820602682, 'max_depth': 7, 'min_child_weight': 4, 'subsample': 0.9143010948149825, 'colsample_bytree': 0.6204161455516594}. Best is trial 17 with value: 0.7914614664485703.\n",
      "[I 2025-10-22 06:46:02,472] Trial 21 finished with value: 0.7917780141819364 and parameters: {'learning_rate': 0.07013912503570967, 'max_depth': 5, 'min_child_weight': 4, 'subsample': 0.957101124383825, 'colsample_bytree': 0.9306076105289269}. Best is trial 21 with value: 0.7917780141819364.\n",
      "[I 2025-10-22 06:46:04,516] Trial 22 finished with value: 0.7909472194534901 and parameters: {'learning_rate': 0.058790578442719774, 'max_depth': 4, 'min_child_weight': 2, 'subsample': 0.9289868931687799, 'colsample_bytree': 0.85356217288392}. Best is trial 21 with value: 0.7917780141819364.\n",
      "[I 2025-10-22 06:46:06,556] Trial 23 finished with value: 0.7891358202594122 and parameters: {'learning_rate': 0.04232319435434922, 'max_depth': 6, 'min_child_weight': 4, 'subsample': 0.9914928511545409, 'colsample_bytree': 0.9213310149199837}. Best is trial 21 with value: 0.7917780141819364.\n",
      "[I 2025-10-22 06:46:08,008] Trial 24 finished with value: 0.7903287552397437 and parameters: {'learning_rate': 0.06979065152179895, 'max_depth': 5, 'min_child_weight': 5, 'subsample': 0.8222742893722998, 'colsample_bytree': 0.7889578578021917}. Best is trial 21 with value: 0.7917780141819364.\n",
      "[I 2025-10-22 06:46:11,220] Trial 25 finished with value: 0.7913602429063705 and parameters: {'learning_rate': 0.028672859501246196, 'max_depth': 4, 'min_child_weight': 3, 'subsample': 0.9502975188499952, 'colsample_bytree': 0.8852419409056815}. Best is trial 21 with value: 0.7917780141819364.\n",
      "[I 2025-10-22 06:46:12,740] Trial 26 finished with value: 0.7914939755770519 and parameters: {'learning_rate': 0.09285792258907805, 'max_depth': 4, 'min_child_weight': 7, 'subsample': 0.8963174071217528, 'colsample_bytree': 0.8808055628888486}. Best is trial 21 with value: 0.7917780141819364.\n",
      "[I 2025-10-22 06:46:14,388] Trial 27 finished with value: 0.789206307663028 and parameters: {'learning_rate': 0.09814325231911016, 'max_depth': 3, 'min_child_weight': 8, 'subsample': 0.8937886184929968, 'colsample_bytree': 0.9216286966643547}. Best is trial 21 with value: 0.7917780141819364.\n",
      "[I 2025-10-22 06:46:15,232] Trial 28 finished with value: 0.790069183956294 and parameters: {'learning_rate': 0.18115668507590532, 'max_depth': 4, 'min_child_weight': 7, 'subsample': 0.8417883055339277, 'colsample_bytree': 0.8391555475885071}. Best is trial 21 with value: 0.7917780141819364.\n",
      "[I 2025-10-22 06:46:16,658] Trial 29 finished with value: 0.7893278410622742 and parameters: {'learning_rate': 0.06519615517027717, 'max_depth': 6, 'min_child_weight': 6, 'subsample': 0.9035741477618272, 'colsample_bytree': 0.8017527309300758}. Best is trial 21 with value: 0.7917780141819364.\n",
      "[I 2025-10-22 06:46:19,694] Trial 30 finished with value: 0.7899295137712556 and parameters: {'learning_rate': 0.0428870867905776, 'max_depth': 3, 'min_child_weight': 8, 'subsample': 0.6990682038987722, 'colsample_bytree': 0.886804906934243}. Best is trial 21 with value: 0.7917780141819364.\n",
      "[I 2025-10-22 06:46:22,876] Trial 31 finished with value: 0.7918717107055021 and parameters: {'learning_rate': 0.030249662719892927, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 0.9485625883367089, 'colsample_bytree': 0.878146717805932}. Best is trial 31 with value: 0.7918717107055021.\n",
      "[I 2025-10-22 06:46:25,032] Trial 32 finished with value: 0.7906542702208599 and parameters: {'learning_rate': 0.04979173176221344, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 0.9338493219424435, 'colsample_bytree': 0.955092614942405}. Best is trial 31 with value: 0.7918717107055021.\n",
      "[I 2025-10-22 06:46:26,256] Trial 33 finished with value: 0.7896834721570359 and parameters: {'learning_rate': 0.0986584789031044, 'max_depth': 5, 'min_child_weight': 6, 'subsample': 0.9579529220098832, 'colsample_bytree': 0.8607594215404324}. Best is trial 31 with value: 0.7918717107055021.\n",
      "[I 2025-10-22 06:46:28,599] Trial 34 finished with value: 0.7908954465645665 and parameters: {'learning_rate': 0.03959305837787809, 'max_depth': 5, 'min_child_weight': 4, 'subsample': 0.9963219553602001, 'colsample_bytree': 0.9034375516884469}. Best is trial 31 with value: 0.7918717107055021.\n",
      "[I 2025-10-22 06:46:33,613] Trial 35 finished with value: 0.7902830816290564 and parameters: {'learning_rate': 0.01835333206436824, 'max_depth': 4, 'min_child_weight': 8, 'subsample': 0.9415670029582398, 'colsample_bytree': 0.9395160153118931}. Best is trial 31 with value: 0.7918717107055021.\n",
      "[I 2025-10-22 06:46:37,600] Trial 36 finished with value: 0.7894288210837527 and parameters: {'learning_rate': 0.03017147169513609, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 0.9724894807117934, 'colsample_bytree': 0.8270397799461058}. Best is trial 31 with value: 0.7918717107055021.\n",
      "[I 2025-10-22 06:46:38,473] Trial 37 finished with value: 0.7923414063859683 and parameters: {'learning_rate': 0.16863298232095916, 'max_depth': 5, 'min_child_weight': 7, 'subsample': 0.8743916337587808, 'colsample_bytree': 0.9728472649390595}. Best is trial 37 with value: 0.7923414063859683.\n",
      "[I 2025-10-22 06:46:39,321] Trial 38 finished with value: 0.7896236413439534 and parameters: {'learning_rate': 0.15425713848310307, 'max_depth': 6, 'min_child_weight': 7, 'subsample': 0.868991718361273, 'colsample_bytree': 0.9734826272647272}. Best is trial 37 with value: 0.7923414063859683.\n",
      "[I 2025-10-22 06:46:40,096] Trial 39 finished with value: 0.7870433102577401 and parameters: {'learning_rate': 0.2983891879540468, 'max_depth': 7, 'min_child_weight': 10, 'subsample': 0.8382495779899608, 'colsample_bytree': 0.9769432142823776}. Best is trial 37 with value: 0.7923414063859683.\n",
      "[I 2025-10-22 06:46:40,878] Trial 40 finished with value: 0.7895506107367756 and parameters: {'learning_rate': 0.20123395127158572, 'max_depth': 4, 'min_child_weight': 9, 'subsample': 0.8921191247853362, 'colsample_bytree': 0.9363740631950712}. Best is trial 37 with value: 0.7923414063859683.\n",
      "[I 2025-10-22 06:46:41,597] Trial 41 finished with value: 0.7899916800895368 and parameters: {'learning_rate': 0.25140772798293504, 'max_depth': 5, 'min_child_weight': 6, 'subsample': 0.9213644490262319, 'colsample_bytree': 0.8697081045540679}. Best is trial 37 with value: 0.7923414063859683.\n",
      "[I 2025-10-22 06:46:43,202] Trial 42 finished with value: 0.790461815923269 and parameters: {'learning_rate': 0.07420524740355149, 'max_depth': 5, 'min_child_weight': 4, 'subsample': 0.942322591533546, 'colsample_bytree': 0.8181943842402568}. Best is trial 37 with value: 0.7923414063859683.\n",
      "[I 2025-10-22 06:46:45,192] Trial 43 finished with value: 0.7910463626923082 and parameters: {'learning_rate': 0.05560083692909439, 'max_depth': 4, 'min_child_weight': 7, 'subsample': 0.9700411173457694, 'colsample_bytree': 0.9984300045345034}. Best is trial 37 with value: 0.7923414063859683.\n",
      "[I 2025-10-22 06:46:46,389] Trial 44 finished with value: 0.7903194340070994 and parameters: {'learning_rate': 0.10794978451050466, 'max_depth': 5, 'min_child_weight': 5, 'subsample': 0.9083051910316261, 'colsample_bytree': 0.8999209303697911}. Best is trial 37 with value: 0.7923414063859683.\n",
      "[I 2025-10-22 06:46:47,311] Trial 45 finished with value: 0.7887467140905468 and parameters: {'learning_rate': 0.13644899740378094, 'max_depth': 6, 'min_child_weight': 6, 'subsample': 0.7587658101554461, 'colsample_bytree': 0.9735492219672082}. Best is trial 37 with value: 0.7923414063859683.\n",
      "[I 2025-10-22 06:46:50,191] Trial 46 finished with value: 0.788617864745001 and parameters: {'learning_rate': 0.047033995429816365, 'max_depth': 3, 'min_child_weight': 8, 'subsample': 0.8770433288737525, 'colsample_bytree': 0.8485237106292693}. Best is trial 37 with value: 0.7923414063859683.\n",
      "[I 2025-10-22 06:46:53,504] Trial 47 finished with value: 0.7898935080101234 and parameters: {'learning_rate': 0.026984265612926304, 'max_depth': 4, 'min_child_weight': 9, 'subsample': 0.8527193165019329, 'colsample_bytree': 0.8785417130406588}. Best is trial 37 with value: 0.7923414063859683.\n",
      "[I 2025-10-22 06:46:54,740] Trial 48 finished with value: 0.7920222282169588 and parameters: {'learning_rate': 0.08427449271352538, 'max_depth': 6, 'min_child_weight': 5, 'subsample': 0.9543884403807118, 'colsample_bytree': 0.9426232578594411}. Best is trial 37 with value: 0.7923414063859683.\n",
      "[I 2025-10-22 06:46:56,117] Trial 49 finished with value: 0.7906482027692646 and parameters: {'learning_rate': 0.08886117157196168, 'max_depth': 6, 'min_child_weight': 7, 'subsample': 0.9785076697101043, 'colsample_bytree': 0.9551207148221409}. Best is trial 37 with value: 0.7923414063859683.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study complete!\n",
      "\n",
      "Best trial:\n",
      "  Value (Mean F1): 0.7923\n",
      "  Best Params: \n",
      "{'learning_rate': 0.16863298232095916, 'max_depth': 5, 'min_child_weight': 7, 'subsample': 0.8743916337587808, 'colsample_bytree': 0.9728472649390595}\n",
      "\n",
      "Successfully stored parameters in 'best_params' variable.\n",
      "\n",
      "Optuna-Tuned XGBoost (w/ Early Stopping) Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0   0.857263  0.842715  0.849927      2416\n",
      "         1.0   0.789474  0.807823  0.798543      1764\n",
      "\n",
      "    accuracy                       0.827990      4180\n",
      "   macro avg   0.823368  0.825269  0.824235      4180\n",
      "weighted avg   0.828655  0.827990  0.828242      4180\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import optuna\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Load Data (Assuming X_train, y_train, X_test, y_test are loaded) ---\n",
    "# (You must load your data here first)\n",
    "try:\n",
    "    train_df = pandas.read_csv('./data/salary.train.processed.csv', index_col='id')\n",
    "    test_df = pandas.read_csv('./data/salary.test.processed.csv', index_col='id')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Could not find the processed CSV files.\")\n",
    "    raise\n",
    "\n",
    "X_train = train_df.drop('label', axis=1)\n",
    "y_train = train_df['label']\n",
    "X_test = test_df.drop('label', axis=1)\n",
    "y_test = test_df['label']\n",
    "# --- End of Data Loading ---\n",
    "\n",
    "\n",
    "# --- Step 1: Define the Advanced Objective Function ---\n",
    "def objective_with_es(trial):\n",
    "    \"\"\"\n",
    "    Objective function with manual Cross-Validation and Early Stopping.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Define the search space for the parameters\n",
    "    param = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'logloss',\n",
    "        'random_state': 42,\n",
    "        'use_label_encoder': False,\n",
    "        'n_estimators': 1000, \n",
    "        \n",
    "        # --- THIS IS THE FIX ---\n",
    "        'early_stopping_rounds': 50, # MOVED HERE\n",
    "        \n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0)\n",
    "    }\n",
    "    \n",
    "    # 2. Set up 3-Fold Cross-Validation\n",
    "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "\n",
    "    # 3. Manually run the CV loop\n",
    "    for train_idx, val_idx in skf.split(X_train, y_train):\n",
    "        X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        \n",
    "        model = XGBClassifier(**param)\n",
    "        \n",
    "        # --- THIS IS THE FIX ---\n",
    "        # We pass eval_set, but 'early_stopping_rounds' is already in the model\n",
    "        model.fit(\n",
    "            X_train_fold, \n",
    "            y_train_fold,\n",
    "            eval_set=[(X_val_fold, y_val_fold)],\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        preds = model.predict(X_val_fold)\n",
    "        f1 = sklearn.metrics.f1_score(y_val_fold, preds, average='binary')\n",
    "        scores.append(f1)\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "# --- Step 2: Create and Run the Study ---\n",
    "print(\"Starting Optuna study with Early Stopping... (This will take several minutes)\")\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective_with_es, n_trials=50) # 50 trials\n",
    "print(\"Study complete!\")\n",
    "\n",
    "# --- Step 3: Get Best Params ---\n",
    "print(\"\\nBest trial:\")\n",
    "print(f\"  Value (Mean F1): {study.best_value:.4f}\")\n",
    "print(\"  Best Params: \")\n",
    "# We need to remove the key we manually added\n",
    "best_params = {k: v for k, v in study.best_params.items() if k != 'early_stopping_rounds'} # üëà *** Renamed variable ***\n",
    "print(best_params) # üëà *** Print new variable ***\n",
    "print(f\"\\nSuccessfully stored parameters in 'best_params' variable.\")\n",
    "\n",
    "\n",
    "# --- Step 4: Train the FINAL Model (Also Fixed) ---\n",
    "X_train_final, X_val_final, y_train_final, y_val_final = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "# Create the final model with the fix\n",
    "final_xgb = XGBClassifier(\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    n_estimators=1000,\n",
    "    early_stopping_rounds=50, # --- FIX APPLIED HERE ---\n",
    "    **best_params  # üëà *** Use new variable ***\n",
    ")\n",
    "\n",
    "# Train it with the fix\n",
    "final_xgb.fit(\n",
    "    X_train_final, \n",
    "    y_train_final,\n",
    "    eval_set=[(X_val_final, y_val_final)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# --- Step 5: Evaluate the Optuna-Tuned Model on the TEST set ---\n",
    "y_pred_optuna_es = final_xgb.predict(X_test)\n",
    "\n",
    "print(\"\\nOptuna-Tuned XGBoost (w/ Early Stopping) Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_optuna_es, digits=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fbff22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.16863298232095916, 'max_depth': 5, 'min_child_weight': 7, 'subsample': 0.8743916337587808, 'colsample_bytree': 0.9728472649390595}\n"
     ]
    }
   ],
   "source": [
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757ccc1d",
   "metadata": {},
   "source": [
    "best_params = {\n",
    "    'learning_rate': 0.016936740838045274, \n",
    "    'max_depth': 6, \n",
    "    'min_child_weight':  4, \n",
    "    'subsample': 0.6728417531696815, \n",
    "    'colsample_bytree': 0.8734308880973841\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ef57e5",
   "metadata": {},
   "source": [
    "### Class Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c231e687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Testing XGBoost with Class Weight ---\n",
      "Using scale_pos_weight: 1.3882\n",
      "Model training complete.\n",
      "\n",
      "XGBoost (Tuned + Class Weight) Report:\n",
      "              precision    recall  f1-score      support\n",
      "0.0            0.879413  0.793874  0.834457  2416.000000\n",
      "1.0            0.750875  0.850907  0.797768  1764.000000\n",
      "accuracy       0.817943  0.817943  0.817943     0.817943\n",
      "macro avg      0.815144  0.822391  0.816112  4180.000000\n",
      "weighted avg   0.825169  0.817943  0.818974  4180.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import xgboost as xgb\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"--- 1. Testing XGBoost with Class Weight ---\")\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    data_train_full = pandas.read_csv('./data/salary.train.processed.csv').set_index('id')\n",
    "    data_test_xgb = pandas.read_csv('./data/salary.test.processed.csv').set_index('id')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå salary.train.processed.csv ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö path\")\n",
    "    # exit() \n",
    "\n",
    "X_full = data_train_full.drop(['label'], axis='columns')\n",
    "y_full = data_train_full['label']\n",
    "X_test = data_test_xgb.drop(['label'], axis='columns')\n",
    "y_test = data_test_xgb['label']\n",
    "\n",
    "# --- Calculate Weight ---\n",
    "scale_pos_weight = len(y_full[y_full == 0]) / len(y_full[y_full == 1])\n",
    "print(f\"Using scale_pos_weight: {scale_pos_weight:.4f}\")\n",
    "\n",
    "# --- Define Parameters ---\n",
    "best_xgb_params = best_params\n",
    "\n",
    "# --- Create and Train Model ---\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    **best_xgb_params,\n",
    "    scale_pos_weight=scale_pos_weight, # üëà Add weight\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_full, y_full) # Train on original data\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# --- Evaluate ---\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "report = classification_report(y_test, y_pred, digits=6, output_dict=True)\n",
    "df_report = pandas.DataFrame(report).transpose()\n",
    "\n",
    "print(\"\\nXGBoost (Tuned + Class Weight) Report:\")\n",
    "print(df_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1488c3",
   "metadata": {},
   "source": [
    "### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "751afeaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 2. Testing XGBoost with SMOTE ---\n",
      "Applying SMOTE...\n",
      "New resampled label distribution:\n",
      "label\n",
      "1.0    9719\n",
      "0.0    9719\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"c:\\Users\\natth\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\natth\\anaconda3\\Lib\\subprocess.py\", line 548, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\natth\\anaconda3\\Lib\\subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"c:\\Users\\natth\\anaconda3\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training complete.\n",
      "\n",
      "XGBoost (Tuned + SMOTE) Report:\n",
      "              precision    recall  f1-score      support\n",
      "0.0            0.870313  0.816639  0.842622  2416.000000\n",
      "1.0            0.768427  0.833333  0.799565  1764.000000\n",
      "accuracy       0.823684  0.823684  0.823684     0.823684\n",
      "macro avg      0.819370  0.824986  0.821094  4180.000000\n",
      "weighted avg   0.827316  0.823684  0.824452  4180.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import xgboost as xgb\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "print(\"\\n--- 2. Testing XGBoost with SMOTE ---\")\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    data_train_full = pandas.read_csv('./data/salary.train.processed.csv').set_index('id')\n",
    "    data_test_xgb = pandas.read_csv('./data/salary.test.processed.csv').set_index('id')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå salary.train.processed.csv ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö path\")\n",
    "    # exit() \n",
    "\n",
    "X_full = data_train_full.drop(['label'], axis='columns')\n",
    "y_full = data_train_full['label']\n",
    "X_test = data_test_xgb.drop(['label'], axis='columns')\n",
    "y_test = data_test_xgb['label']\n",
    "\n",
    "# --- Apply SMOTE ---\n",
    "print(\"Applying SMOTE...\")\n",
    "smote = SMOTE(random_state=42, n_jobs=-1)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_full, y_full)\n",
    "print(f\"New resampled label distribution:\\n{y_resampled.value_counts()}\")\n",
    "\n",
    "# --- Define Parameters ---\n",
    "best_xgb_params = best_params\n",
    "\n",
    "# --- Create and Train Model ---\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    **best_xgb_params,\n",
    "    # ‚ö†Ô∏è NO 'scale_pos_weight'\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_resampled, y_resampled) # Train on SMOTEd data\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# --- Evaluate ---\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "report = classification_report(y_test, y_pred, digits=6, output_dict=True)\n",
    "df_report = pandas.DataFrame(report).transpose()\n",
    "\n",
    "print(\"\\nXGBoost (Tuned + SMOTE) Report:\")\n",
    "print(df_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60c1463",
   "metadata": {},
   "source": [
    "### SMOTETomek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3555419b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 3. Testing XGBoost with SMOTETomek ---\n",
      "Applying SMOTETomek...\n",
      "New resampled label distribution:\n",
      "label\n",
      "1.0    9220\n",
      "0.0    9220\n",
      "Name: count, dtype: int64\n",
      "Model training complete.\n",
      "\n",
      "XGBoost (Tuned + SMOTETomek) Report:\n",
      "              precision    recall  f1-score      support\n",
      "0.0            0.869565  0.819536  0.843810  2416.000000\n",
      "1.0            0.770888  0.831633  0.800109  1764.000000\n",
      "accuracy       0.824641  0.824641  0.824641     0.824641\n",
      "macro avg      0.820227  0.825585  0.821960  4180.000000\n",
      "weighted avg   0.827923  0.824641  0.825368  4180.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import xgboost as xgb\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "print(\"\\n--- 3. Testing XGBoost with SMOTETomek ---\")\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    data_train_full = pandas.read_csv('./data/salary.train.processed.csv').set_index('id')\n",
    "    data_test_xgb = pandas.read_csv('./data/salary.test.processed.csv').set_index('id')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå salary.train.processed.csv ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö path\")\n",
    "    # exit() \n",
    "\n",
    "X_full = data_train_full.drop(['label'], axis='columns')\n",
    "y_full = data_train_full['label']\n",
    "X_test = data_test_xgb.drop(['label'], axis='columns')\n",
    "y_test = data_test_xgb['label']\n",
    "\n",
    "# --- Apply SMOTETomek ---\n",
    "print(\"Applying SMOTETomek...\")\n",
    "smt = SMOTETomek(random_state=42, n_jobs=-1)\n",
    "X_resampled, y_resampled = smt.fit_resample(X_full, y_full)\n",
    "print(f\"New resampled label distribution:\\n{y_resampled.value_counts()}\")\n",
    "\n",
    "# --- Define Parameters ---\n",
    "best_xgb_params = best_params\n",
    "\n",
    "# --- Create and Train Model ---\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    **best_xgb_params,\n",
    "    # ‚ö†Ô∏è NO 'scale_pos_weight'\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_resampled, y_resampled) # Train on SMOTETomek data\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# --- Evaluate ---\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "report = classification_report(y_test, y_pred, digits=6, output_dict=True)\n",
    "df_report = pandas.DataFrame(report).transpose()\n",
    "\n",
    "print(\"\\nXGBoost (Tuned + SMOTETomek) Report:\")\n",
    "print(df_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9e8f9e",
   "metadata": {},
   "source": [
    "### ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3705b63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 4. Testing XGBoost with ADASYN ---\n",
      "Applying ADASYN...\n",
      "New resampled label distribution:\n",
      "label\n",
      "0.0    9719\n",
      "1.0    9698\n",
      "Name: count, dtype: int64\n",
      "Model training complete.\n",
      "\n",
      "XGBoost (Tuned + ADASYN) Report:\n",
      "              precision    recall  f1-score      support\n",
      "0.0            0.877708  0.805050  0.839810  2416.000000\n",
      "1.0            0.760183  0.846372  0.800966  1764.000000\n",
      "accuracy       0.822488  0.822488  0.822488     0.822488\n",
      "macro avg      0.818945  0.825711  0.820388  4180.000000\n",
      "weighted avg   0.828111  0.822488  0.823417  4180.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import xgboost as xgb\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "print(\"\\n--- 4. Testing XGBoost with ADASYN ---\")\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    data_train_full = pandas.read_csv('./data/salary.train.processed.csv').set_index('id')\n",
    "    data_test_xgb = pandas.read_csv('./data/salary.test.processed.csv').set_index('id')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå salary.train.processed.csv ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö path\")\n",
    "    # exit() \n",
    "\n",
    "X_full = data_train_full.drop(['label'], axis='columns')\n",
    "y_full = data_train_full['label']\n",
    "X_test = data_test_xgb.drop(['label'], axis='columns')\n",
    "y_test = data_test_xgb['label']\n",
    "\n",
    "# --- Apply ADASYN ---\n",
    "print(\"Applying ADASYN...\")\n",
    "ada = ADASYN(random_state=42, n_jobs=-1)\n",
    "X_resampled, y_resampled = ada.fit_resample(X_full, y_full)\n",
    "print(f\"New resampled label distribution:\\n{y_resampled.value_counts()}\")\n",
    "\n",
    "# --- Define Parameters ---\n",
    "best_xgb_params = best_params\n",
    "\n",
    "# --- Create and Train Model ---\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    **best_xgb_params,\n",
    "    # ‚ö†Ô∏è NO 'scale_pos_weight'\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_resampled, y_resampled) # Train on ADASYN data\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# --- Evaluate ---\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "report = classification_report(y_test, y_pred, digits=6, output_dict=True)\n",
    "df_report = pandas.DataFrame(report).transpose()\n",
    "\n",
    "print(\"\\nXGBoost (Tuned + ADASYN) Report:\")\n",
    "print(df_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f54cb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
