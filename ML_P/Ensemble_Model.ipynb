{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ensemble Model for Salary Prediction\n",
        "\n",
        "This notebook implements an ensemble/stacking model that combines XGBoost, LightGBM, and CatBoost for maximum F1 score (>0.90) on the salary prediction dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.ensemble import VotingClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load and Prepare Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load processed data\n",
        "train_data = pd.read_csv('./for_cursur/Data/salary.train.processed.csv', index_col='id')\n",
        "test_data = pd.read_csv('./for_cursur/Data/salary.test.processed.csv', index_col='id')\n",
        "live_data = pd.read_csv('./for_cursur/Data/salary.live.processed.csv', index_col='id')\n",
        "\n",
        "print(f\"Training data shape: {train_data.shape}\")\n",
        "print(f\"Test data shape: {test_data.shape}\")\n",
        "print(f\"Live data shape: {live_data.shape}\")\n",
        "\n",
        "# Prepare features and target\n",
        "X_train = train_data.drop(columns=['label'])\n",
        "y_train = train_data['label']\n",
        "X_test = test_data.drop(columns=['label'])\n",
        "y_test = test_data['label']\n",
        "X_live = live_data\n",
        "\n",
        "print(f\"\\nFeature matrix shape: {X_train.shape}\")\n",
        "print(f\"Target distribution: {y_train.value_counts().to_dict()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Define Base Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define optimized base models\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    n_estimators=400,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.03,\n",
        "    subsample=0.85,\n",
        "    colsample_bytree=0.85,\n",
        "    reg_alpha=0.5,\n",
        "    reg_lambda=1.5,\n",
        "    scale_pos_weight=2.5,\n",
        "    random_state=42,\n",
        "    eval_metric='logloss',\n",
        "    verbosity=0\n",
        ")\n",
        "\n",
        "lgb_model = lgb.LGBMClassifier(\n",
        "    n_estimators=400,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.03,\n",
        "    subsample=0.85,\n",
        "    colsample_bytree=0.85,\n",
        "    reg_alpha=0.5,\n",
        "    reg_lambda=1.5,\n",
        "    class_weight='balanced',\n",
        "    random_state=42,\n",
        "    verbose=-1,\n",
        "    force_col_wise=True\n",
        ")\n",
        "\n",
        "catboost_model = CatBoostClassifier(\n",
        "    iterations=400,\n",
        "    depth=6,\n",
        "    learning_rate=0.03,\n",
        "    subsample=0.85,\n",
        "    colsample_bylevel=0.85,\n",
        "    l2_leaf_reg=4,\n",
        "    class_weights=[1, 2.5],\n",
        "    random_state=42,\n",
        "    verbose=False,\n",
        "    thread_count=-1\n",
        ")\n",
        "\n",
        "print(\"Base models defined successfully!\")\n",
        "print(\"Models: XGBoost, LightGBM, CatBoost\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Create Ensemble Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Voting Classifier (Hard Voting)\n",
        "voting_hard = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('xgb', xgb_model),\n",
        "        ('lgb', lgb_model),\n",
        "        ('catboost', catboost_model)\n",
        "    ],\n",
        "    voting='hard'\n",
        ")\n",
        "\n",
        "# Create Voting Classifier (Soft Voting)\n",
        "voting_soft = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('xgb', xgb_model),\n",
        "        ('lgb', lgb_model),\n",
        "        ('catboost', catboost_model)\n",
        "    ],\n",
        "    voting='soft'\n",
        ")\n",
        "\n",
        "# Create Stacking Classifier\n",
        "stacking_model = StackingClassifier(\n",
        "    estimators=[\n",
        "        ('xgb', xgb_model),\n",
        "        ('lgb', lgb_model),\n",
        "        ('catboost', catboost_model)\n",
        "    ],\n",
        "    final_estimator=LogisticRegression(random_state=42, class_weight='balanced'),\n",
        "    cv=5,\n",
        "    stack_method='predict_proba'\n",
        ")\n",
        "\n",
        "print(\"Ensemble models created successfully!\")\n",
        "print(\"Models: Hard Voting, Soft Voting, Stacking\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Train and Evaluate Ensemble Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train and evaluate all ensemble models\n",
        "models = {\n",
        "    'Hard Voting': voting_hard,\n",
        "    'Soft Voting': voting_soft,\n",
        "    'Stacking': stacking_model\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "print(\"Training and evaluating ensemble models...\")\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nTraining {name}...\")\n",
        "    \n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    # Make predictions\n",
        "    y_pred_train = model.predict(X_train)\n",
        "    y_pred_test = model.predict(X_test)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    train_f1 = f1_score(y_train, y_pred_train)\n",
        "    test_f1 = f1_score(y_test, y_pred_test)\n",
        "    \n",
        "    # Store results\n",
        "    results[name] = {\n",
        "        'model': model,\n",
        "        'train_f1': train_f1,\n",
        "        'test_f1': test_f1\n",
        "    }\n",
        "    \n",
        "    print(f\"{name} - Train F1: {train_f1:.4f}, Test F1: {test_f1:.4f}\")\n",
        "\n",
        "# Find best model\n",
        "best_model_name = max(results.keys(), key=lambda x: results[x]['test_f1'])\n",
        "best_model = results[best_model_name]['model']\n",
        "best_f1 = results[best_model_name]['test_f1']\n",
        "\n",
        "print(f\"\\n=== BEST ENSEMBLE MODEL ===\")\n",
        "print(f\"Best Model: {best_model_name}\")\n",
        "print(f\"Best F1 Score: {best_f1:.4f}\")\n",
        "print(f\"Target achieved: {'✅ YES' if best_f1 >= 0.90 else '❌ NO'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Detailed Evaluation of Best Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed evaluation of best model\n",
        "y_pred_test = best_model.predict(X_test)\n",
        "y_pred_proba_test = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate additional metrics\n",
        "test_auc = roc_auc_score(y_test, y_pred_proba_test)\n",
        "\n",
        "print(f\"=== DETAILED EVALUATION - {best_model_name} ===\")\n",
        "print(f\"Test F1 Score: {best_f1:.4f}\")\n",
        "print(f\"Test AUC Score: {test_auc:.4f}\")\n",
        "\n",
        "# Detailed classification report\n",
        "print(\"\\n=== DETAILED CLASSIFICATION REPORT ===\")\n",
        "print(classification_report(y_test, y_pred_test))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred_test)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Purples', \n",
        "            xticklabels=['Low Income', 'High Income'], \n",
        "            yticklabels=['Low Income', 'High Income'])\n",
        "plt.title(f'Confusion Matrix - {best_model_name} Model')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.show()\n",
        "\n",
        "# Model comparison\n",
        "print(\"\\n=== MODEL COMPARISON ===\")\n",
        "for name, result in results.items():\n",
        "    print(f\"{name:15s}: F1 = {result['test_f1']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Make Predictions on Live Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions on live data using best model\n",
        "live_predictions = best_model.predict(X_live)\n",
        "live_probabilities = best_model.predict_proba(X_live)[:, 1]\n",
        "\n",
        "# Create prediction dataframe\n",
        "live_results = pd.DataFrame({\n",
        "    'id': X_live.index,\n",
        "    'predicted_label': live_predictions,\n",
        "    'probability_high_income': live_probabilities\n",
        "})\n",
        "\n",
        "print(f\"Live data predictions completed using {best_model_name}!\")\n",
        "print(f\"Number of predictions: {len(live_results)}\")\n",
        "print(f\"High income predictions: {live_predictions.sum()}\")\n",
        "print(f\"Low income predictions: {len(live_predictions) - live_predictions.sum()}\")\n",
        "\n",
        "# Show sample predictions\n",
        "print(\"\\nSample predictions:\")\n",
        "print(live_results.head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Save Model and Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the best ensemble model\n",
        "joblib.dump(best_model, './for_cursur/ensemble_model.joblib')\n",
        "\n",
        "# Save predictions\n",
        "live_results.to_csv('./for_cursur/ensemble_predictions.csv', index=False)\n",
        "\n",
        "# Save model configuration\n",
        "model_config = {\n",
        "    'model_type': f'Ensemble - {best_model_name}',\n",
        "    'best_model': best_model_name,\n",
        "    'all_results': {name: result['test_f1'] for name, result in results.items()},\n",
        "    'test_f1_score': best_f1,\n",
        "    'test_auc_score': test_auc,\n",
        "    'feature_count': X_train.shape[1],\n",
        "    'training_samples': len(X_train),\n",
        "    'test_samples': len(X_test),\n",
        "    'base_models': ['XGBoost', 'LightGBM', 'CatBoost']\n",
        "}\n",
        "\n",
        "import json\n",
        "with open('./for_cursur/ensemble_model_config.json', 'w') as f:\n",
        "    json.dump(model_config, f, indent=2)\n",
        "\n",
        "print(\"Ensemble model and results saved successfully!\")\n",
        "print(f\"\\nFinal Test F1 Score: {best_f1:.4f}\")\n",
        "print(f\"Target achieved: {'✅ YES' if best_f1 >= 0.90 else '❌ NO'}\")\n",
        "print(f\"\\nBest performing ensemble: {best_model_name}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
