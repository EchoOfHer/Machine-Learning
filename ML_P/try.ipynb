{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fa846b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 'high_salary.csv'\n",
      "------------------------------\n",
      "Target variable 'label' distribution:\n",
      "label\n",
      "0.0    0.580622\n",
      "1.0    0.419378\n",
      "Name: proportion, dtype: float64\n",
      "------------------------------\n",
      "Numerical features: ['age-group', 'fnlwgt', 'education-num', 'capitalgain', 'capitalloss', 'hoursperweek']\n",
      "Categorical features: ['workclass', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\n",
      "------------------------------\n",
      "Training set size: 16720 samples\n",
      "Testing set size: 4180 samples\n",
      "------------------------------\n",
      "Training the model...\n",
      "Model training complete.\n",
      "------------------------------\n",
      "Evaluating model on the test set...\n",
      "Model Accuracy: 0.8031\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.83      0.83      2427\n",
      "         1.0       0.77      0.76      0.77      1753\n",
      "\n",
      "    accuracy                           0.80      4180\n",
      "   macro avg       0.80      0.80      0.80      4180\n",
      "weighted avg       0.80      0.80      0.80      4180\n",
      "\n",
      "------------------------------\n",
      "Process finished.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# --- 1. Load Data ---\n",
    "try:\n",
    "    df = pd.read_csv('./data/high_salary.csv')\n",
    "    print(\"Successfully loaded 'high_salary.csv'\")\n",
    "    print(\"-\" * 30)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'high_salary.csv' not found. Please make sure it's in the same directory.\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. Define Features (X) and Target (y) ---\n",
    "\n",
    "# Drop identifier columns and redundant columns\n",
    "# 'education' is dropped in favor of 'education-num'\n",
    "# 'native-country-code' is dropped in favor of 'native-country'\n",
    "try:\n",
    "    columns_to_drop = ['id', 'social-security-number', 'house-number', 'education', 'native-country-code']\n",
    "    X = df.drop(columns=columns_to_drop + ['label'])\n",
    "    y = df['label']\n",
    "    \n",
    "    # Check for class balance\n",
    "    print(\"Target variable 'label' distribution:\")\n",
    "    print(y.value_counts(normalize=True))\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "except KeyError as e:\n",
    "    print(f\"Error: A required column is missing. {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 3. Identify Feature Types ---\n",
    "# Identify numerical and categorical columns\n",
    "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"Numerical features: {numerical_features}\")\n",
    "print(f\"Categorical features: {categorical_features}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- 4. Split Data ---\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set size: {X_test.shape[0]} samples\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- 5. Create Preprocessing Pipelines ---\n",
    "\n",
    "# Pipeline for numerical features:\n",
    "# 1. Impute missing values with the median\n",
    "# 2. Scale features\n",
    "num_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Pipeline for categorical features:\n",
    "# 1. Impute missing values with the most frequent value\n",
    "# 2. One-hot encode the categories\n",
    "cat_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# --- 6. Combine Pipelines with ColumnTransformer ---\n",
    "# Create a preprocessor that applies the correct pipeline to each column type\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_pipeline, numerical_features),\n",
    "        ('cat', cat_pipeline, categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# --- 7. Create and Train the Final Model ---\n",
    "# Use a RandomForestClassifier as the model\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "\n",
    "# Create the full pipeline: preprocess, then model\n",
    "full_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "print(\"Training the model...\")\n",
    "full_pipeline.fit(X_train, y_train)\n",
    "print(\"Model training complete.\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- 8. Evaluate the Model ---\n",
    "print(\"Evaluating model on the test set...\")\n",
    "# Make predictions on the test set\n",
    "y_pred = full_pipeline.predict(X_test)\n",
    "\n",
    "# Calculate and print accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "\n",
    "# Print a detailed classification report\n",
    "# Note: '1.0' likely means 'high salary' and '0.0' means 'low salary'\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(\"Process finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a00a9160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n",
      "Error: 'high_salary.csv' not found. Please make sure it's in the same directory.\n",
      "Target variable 'label' distribution (before split):\n",
      "label\n",
      "0.0    0.580622\n",
      "1.0    0.419378\n",
      "Name: proportion, dtype: float64\n",
      "------------------------------\n",
      "Numerical features: ['age-group', 'fnlwgt', 'education-num', 'capitalgain', 'capitalloss', 'hoursperweek']\n",
      "Categorical features: ['workclass', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\n",
      "------------------------------\n",
      "Training set size: 16720 samples\n",
      "Testing set size: 4180 samples\n",
      "------------------------------\n",
      "Starting hyperparameter tuning with GridSearchCV...\n",
      "This may take several minutes...\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "Hyperparameter tuning complete.\n",
      "------------------------------\n",
      "Best parameters found: {'model__class_weight': None, 'model__max_depth': 20, 'model__min_samples_leaf': 2, 'model__n_estimators': 100}\n",
      "Best macro-F1 score during cross-validation: 0.8177\n",
      "------------------------------\n",
      "Evaluating the best model on the unseen test set...\n",
      "Final Model Accuracy: 0.8237\n",
      "\n",
      "Final Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.83      0.85      2427\n",
      "         1.0       0.77      0.82      0.80      1753\n",
      "\n",
      "    accuracy                           0.82      4180\n",
      "   macro avg       0.82      0.82      0.82      4180\n",
      "weighted avg       0.83      0.82      0.82      4180\n",
      "\n",
      "\n",
      "---> F1-Score for class '1.0' (high-salary): 0.7956 <---\n",
      "------------------------------\n",
      "Process finished.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
    "\n",
    "print(\"Libraries imported successfully.\")\n",
    "\n",
    "# --- 1. Load Data ---\n",
    "try:\n",
    "    df = pd.read_csv('high_salary.csv')\n",
    "    print(\"Successfully loaded 'high_salary.csv'\")\n",
    "    print(\"-\" * 30)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'high_salary.csv' not found. Please make sure it's in the same directory.\")\n",
    "    \n",
    "if 'df' in locals():\n",
    "    \n",
    "    # --- 2. Define Features (X) and Target (y) ---\n",
    "    try:\n",
    "        # Drop identifier columns and redundant/leaky columns\n",
    "        columns_to_drop = ['id', 'social-security-number', 'house-number', 'education', 'native-country-code']\n",
    "        X = df.drop(columns=columns_to_drop + ['label'])\n",
    "        y = df['label']\n",
    "        \n",
    "        print(\"Target variable 'label' distribution (before split):\")\n",
    "        print(y.value_counts(normalize=True))\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: A required column is missing. {e}\")\n",
    "\n",
    "    # --- 3. Identify Feature Types ---\n",
    "    numerical_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "    print(f\"Numerical features: {numerical_features}\")\n",
    "    print(f\"Categorical features: {categorical_features}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # --- 4. Split Data ---\n",
    "    # Stratify=y ensures both train and test sets have a similar class distribution\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "    print(f\"Testing set size: {X_test.shape[0]} samples\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # --- 5. Create Preprocessing Pipelines ---\n",
    "    \n",
    "    # Pipeline for numerical features:\n",
    "    # 1. Impute missing values with the median\n",
    "    # 2. Scale features\n",
    "    num_pipeline = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    # Pipeline for categorical features:\n",
    "    # 1. Impute missing values with the most frequent value\n",
    "    # 2. One-hot encode the categories\n",
    "    cat_pipeline = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ])\n",
    "\n",
    "    # --- 6. Combine Pipelines with ColumnTransformer ---\n",
    "    # Create a preprocessor that applies the correct pipeline to each column type\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', num_pipeline, numerical_features),\n",
    "            ('cat', cat_pipeline, categorical_features)\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "\n",
    "    # --- 7. Create Model Pipeline ---\n",
    "    # This pipeline will first preprocess the data, then train the RF model\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', RandomForestClassifier(random_state=42, n_jobs=-1))\n",
    "    ])\n",
    "\n",
    "    # --- 8. Define Hyperparameter Grid for Tuning ---\n",
    "    # This grid tells GridSearchCV which parameters to test\n",
    "    param_grid = {\n",
    "        'model__n_estimators': [100, 200],      # Number of trees\n",
    "        'model__max_depth': [10, 20, 30],       # Max depth of trees\n",
    "        'model__min_samples_leaf': [2, 4],    # Min samples at a leaf node\n",
    "        'model__class_weight': ['balanced', None] # Key parameter to fight imbalance\n",
    "    }\n",
    "\n",
    "    # We will score based on 'f1_macro' to balance both classes' F1-scores\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid=param_grid,\n",
    "        cv=3, # 3-fold cross-validation\n",
    "        scoring='f1_macro', \n",
    "        n_jobs=-1, # Use all available cores\n",
    "        verbose=2  # Show progress\n",
    "    )\n",
    "\n",
    "    # --- 9. Train the Grid Search ---\n",
    "    print(\"Starting hyperparameter tuning with GridSearchCV...\")\n",
    "    print(\"This may take several minutes...\")\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(\"Hyperparameter tuning complete.\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # --- 10. Show Best Parameters and Evaluate ---\n",
    "    print(f\"Best parameters found: {grid_search.best_params_}\")\n",
    "    print(f\"Best macro-F1 score during cross-validation: {grid_search.best_score_:.4f}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    print(\"Evaluating the best model on the unseen test set...\")\n",
    "    # Get the best model found by the grid search\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # --- 11. Display Final Report ---\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Final Model Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    print(\"\\nFinal Classification Report:\")\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    print(report)\n",
    "\n",
    "    # Print the specific F1-score for the high-salary class\n",
    "    f1_class_1 = f1_score(y_test, y_pred, pos_label=1.0)\n",
    "    print(f\"\\n---> F1-Score for class '1.0' (high-salary): {f1_class_1:.4f} <---\")\n",
    "    print(\"-\" * 30)\n",
    "    print(\"Process finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca86cdf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
