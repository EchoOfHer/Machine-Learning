{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Comparison and Selection\n",
        "\n",
        "This notebook compares all trained models and selects the best performing one for salary prediction with F1 score > 0.90.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "import joblib\n",
        "from sklearn.metrics import f1_score, roc_auc_score, classification_report, confusion_matrix\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Data and Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load test data\n",
        "test_data = pd.read_csv('./for_cursur/Data/salary.test.processed.csv', index_col='id')\n",
        "live_data = pd.read_csv('./for_cursur/Data/salary.live.processed.csv', index_col='id')\n",
        "\n",
        "X_test = test_data.drop(columns=['label'])\n",
        "y_test = test_data['label']\n",
        "X_live = live_data\n",
        "\n",
        "print(f\"Test data shape: {X_test.shape}\")\n",
        "print(f\"Live data shape: {X_live.shape}\")\n",
        "print(f\"Test target distribution: {y_test.value_counts().to_dict()}\")\n",
        "\n",
        "# Load all model configurations\n",
        "model_configs = {}\n",
        "model_files = [\n",
        "    ('XGBoost', './for_cursur/xgb_model_config.json'),\n",
        "    ('LightGBM', './for_cursur/lgb_model_config.json'),\n",
        "    ('CatBoost', './for_cursur/catboost_model_config.json'),\n",
        "    ('Random Forest', './for_cursur/rf_model_config.json'),\n",
        "    ('Ensemble', './for_cursur/ensemble_model_config.json')\n",
        "]\n",
        "\n",
        "print(\"\\nLoading model configurations...\")\n",
        "for model_name, config_file in model_files:\n",
        "    try:\n",
        "        with open(config_file, 'r') as f:\n",
        "            config = json.load(f)\n",
        "            model_configs[model_name] = config\n",
        "            print(f\"âœ… {model_name}: F1 = {config.get('test_f1_score', 'N/A'):.4f}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"âŒ {model_name}: Config file not found\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ {model_name}: Error loading config - {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Model Performance Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comparison dataframe\n",
        "comparison_data = []\n",
        "for model_name, config in model_configs.items():\n",
        "    comparison_data.append({\n",
        "        'Model': model_name,\n",
        "        'F1_Score': config.get('test_f1_score', 0),\n",
        "        'AUC_Score': config.get('test_auc_score', 0),\n",
        "        'CV_Mean_F1': config.get('cv_mean_f1', 0),\n",
        "        'Feature_Count': config.get('feature_count', 0),\n",
        "        'Training_Samples': config.get('training_samples', 0),\n",
        "        'Test_Samples': config.get('test_samples', 0),\n",
        "        'Target_Achieved': config.get('test_f1_score', 0) >= 0.90\n",
        "    })\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "comparison_df = comparison_df.sort_values('F1_Score', ascending=False)\n",
        "\n",
        "print(\"=== MODEL PERFORMANCE COMPARISON ===\")\n",
        "print(comparison_df.to_string(index=False, float_format='%.4f'))\n",
        "\n",
        "# Find best model\n",
        "best_model = comparison_df.iloc[0]\n",
        "print(f\"\\nðŸ† BEST MODEL: {best_model['Model']}\")\n",
        "print(f\"   F1 Score: {best_model['F1_Score']:.4f}\")\n",
        "print(f\"   AUC Score: {best_model['AUC_Score']:.4f}\")\n",
        "print(f\"   Target Achieved: {'âœ… YES' if best_model['Target_Achieved'] else 'âŒ NO'}\")\n",
        "\n",
        "# Count models that achieved target\n",
        "target_achieved = comparison_df['Target_Achieved'].sum()\n",
        "print(f\"\\nðŸ“Š Models achieving F1 > 0.90: {target_achieved}/{len(comparison_df)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Performance Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create performance comparison plots\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# F1 Score comparison\n",
        "axes[0, 0].bar(comparison_df['Model'], comparison_df['F1_Score'], color='skyblue', alpha=0.7)\n",
        "axes[0, 0].axhline(y=0.90, color='red', linestyle='--', label='Target F1 = 0.90')\n",
        "axes[0, 0].set_title('F1 Score Comparison', fontsize=14, fontweight='bold')\n",
        "axes[0, 0].set_ylabel('F1 Score')\n",
        "axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# AUC Score comparison\n",
        "axes[0, 1].bar(comparison_df['Model'], comparison_df['AUC_Score'], color='lightgreen', alpha=0.7)\n",
        "axes[0, 1].set_title('AUC Score Comparison', fontsize=14, fontweight='bold')\n",
        "axes[0, 1].set_ylabel('AUC Score')\n",
        "axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# F1 vs AUC scatter plot\n",
        "colors = ['red' if not achieved else 'green' for achieved in comparison_df['Target_Achieved']]\n",
        "axes[1, 0].scatter(comparison_df['F1_Score'], comparison_df['AUC_Score'], \n",
        "                   c=colors, s=100, alpha=0.7)\n",
        "axes[1, 0].axvline(x=0.90, color='red', linestyle='--', alpha=0.5, label='Target F1 = 0.90')\n",
        "axes[1, 0].set_xlabel('F1 Score')\n",
        "axes[1, 0].set_ylabel('AUC Score')\n",
        "axes[1, 0].set_title('F1 Score vs AUC Score', fontsize=14, fontweight='bold')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Add model labels to scatter plot\n",
        "for i, model in enumerate(comparison_df['Model']):\n",
        "    axes[1, 0].annotate(model, (comparison_df.iloc[i]['F1_Score'], comparison_df.iloc[i]['AUC_Score']),\n",
        "                       xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
        "\n",
        "# Target achievement pie chart\n",
        "target_counts = comparison_df['Target_Achieved'].value_counts()\n",
        "labels = ['Target NOT Achieved', 'Target Achieved'] if False in target_counts.index else ['Target Achieved']\n",
        "sizes = [target_counts.get(False, 0), target_counts.get(True, 0)] if False in target_counts.index else [target_counts.get(True, 0)]\n",
        "colors_pie = ['lightcoral', 'lightgreen'] if len(sizes) == 2 else ['lightgreen']\n",
        "\n",
        "axes[1, 1].pie(sizes, labels=labels, colors=colors_pie, autopct='%1.1f%%', startangle=90)\n",
        "axes[1, 1].set_title('Target Achievement Distribution', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print detailed performance summary\n",
        "print(\"\\n=== DETAILED PERFORMANCE SUMMARY ===\")\n",
        "for _, row in comparison_df.iterrows():\n",
        "    status = \"âœ… TARGET ACHIEVED\" if row['Target_Achieved'] else \"âŒ Target not achieved\"\n",
        "    print(f\"{row['Model']:15s}: F1={row['F1_Score']:.4f}, AUC={row['AUC_Score']:.4f} - {status}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Load Best Model and Make Final Predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the best model\n",
        "best_model_name = best_model['Model']\n",
        "print(f\"Loading best model: {best_model_name}\")\n",
        "\n",
        "# Load the appropriate model file\n",
        "model_file_map = {\n",
        "    'XGBoost': './for_cursur/xgb_model.joblib',\n",
        "    'LightGBM': './for_cursur/lgb_model.joblib',\n",
        "    'CatBoost': './for_cursur/catboost_model.cbm',\n",
        "    'Random Forest': './for_cursur/rf_model.joblib',\n",
        "    'Ensemble': './for_cursur/ensemble_model.joblib'\n",
        "}\n",
        "\n",
        "try:\n",
        "    if best_model_name == 'CatBoost':\n",
        "        from catboost import CatBoostClassifier\n",
        "        best_model_loaded = CatBoostClassifier()\n",
        "        best_model_loaded.load_model(model_file_map[best_model_name])\n",
        "    else:\n",
        "        best_model_loaded = joblib.load(model_file_map[best_model_name])\n",
        "    \n",
        "    print(f\"âœ… Successfully loaded {best_model_name} model\")\n",
        "    \n",
        "    # Make predictions on live data\n",
        "    live_predictions = best_model_loaded.predict(X_live)\n",
        "    live_probabilities = best_model_loaded.predict_proba(X_live)[:, 1]\n",
        "    \n",
        "    # Create final prediction dataframe\n",
        "    final_predictions = pd.DataFrame({\n",
        "        'id': X_live.index,\n",
        "        'predicted_label': live_predictions,\n",
        "        'probability_high_income': live_probabilities,\n",
        "        'model_used': best_model_name\n",
        "    })\n",
        "    \n",
        "    print(f\"\\nðŸŽ¯ FINAL PREDICTIONS COMPLETED!\")\n",
        "    print(f\"   Model Used: {best_model_name}\")  \n",
        "    print(f\"   F1 Score: {best_model['F1_Score']:.4f}\")\n",
        "    print(f\"   Number of predictions: {len(final_predictions)}\")\n",
        "    print(f\"   High income predictions: {live_predictions.sum()}\")\n",
        "    print(f\"   Low income predictions: {len(live_predictions) - live_predictions.sum()}\")\n",
        "    \n",
        "    # Save final predictions\n",
        "    final_predictions.to_csv('./for_cursur/final_predictions.csv', index=False)\n",
        "    print(f\"\\nðŸ’¾ Final predictions saved to: ./for_cursur/final_predictions.csv\")\n",
        "    \n",
        "    # Show sample predictions\n",
        "    print(f\"\\nðŸ“‹ Sample predictions:\")\n",
        "    print(final_predictions.head(10))\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error loading model: {e}\")\n",
        "    print(\"Please ensure all model files are properly trained and saved.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Summary and Recommendations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final summary\n",
        "print(\"=\" * 60)\n",
        "print(\"ðŸŽ¯ SALARY PREDICTION MODEL SELECTION SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\nðŸ“Š TOTAL MODELS TESTED: {len(comparison_df)}\")\n",
        "print(f\"âœ… MODELS ACHIEVING F1 > 0.90: {target_achieved}\")\n",
        "print(f\"ðŸ† BEST MODEL: {best_model['Model']}\")\n",
        "print(f\"ðŸ“ˆ BEST F1 SCORE: {best_model['F1_Score']:.4f}\")\n",
        "print(f\"ðŸ“ˆ BEST AUC SCORE: {best_model['AUC_Score']:.4f}\")\n",
        "\n",
        "print(f\"\\nðŸ” MODEL RANKINGS:\")\n",
        "for i, (_, row) in enumerate(comparison_df.iterrows(), 1):\n",
        "    status = \"âœ…\" if row['Target_Achieved'] else \"âŒ\"\n",
        "    print(f\"   {i}. {row['Model']:15s} - F1: {row['F1_Score']:.4f} {status}\")\n",
        "\n",
        "print(f\"\\nðŸ’¡ RECOMMENDATIONS:\")\n",
        "if best_model['Target_Achieved']:\n",
        "    print(f\"   âœ… SUCCESS! {best_model['Model']} achieved your target F1 > 0.90\")\n",
        "    print(f\"   ðŸš€ Use {best_model['Model']} for production predictions\")\n",
        "    print(f\"   ðŸ“ Final predictions saved in: ./for_cursur/final_predictions.csv\")\n",
        "else:\n",
        "    print(f\"   âš ï¸  No model achieved F1 > 0.90\")\n",
        "    print(f\"   ðŸ”§ Consider:\")\n",
        "    print(f\"      - More feature engineering\")\n",
        "    print(f\"      - Different hyperparameter tuning\")\n",
        "    print(f\"      - Ensemble methods\")\n",
        "    print(f\"      - More training data\")\n",
        "\n",
        "print(f\"\\nðŸ“‹ NEXT STEPS:\")\n",
        "print(f\"   1. Run individual model notebooks to train all models\")\n",
        "print(f\"   2. Run this comparison notebook to select the best model\")\n",
        "print(f\"   3. Use final_predictions.csv for your salary predictions\")\n",
        "print(f\"   4. Monitor model performance in production\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
