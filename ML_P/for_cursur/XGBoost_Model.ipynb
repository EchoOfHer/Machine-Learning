{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# XGBoost Model for Salary Prediction\n",
        "\n",
        "This notebook implements an XGBoost model optimized for high F1 score (>0.90) on the salary prediction dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score, roc_auc_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load and Prepare Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data shape: (16720, 24)\n",
            "Test data shape: (4180, 24)\n",
            "Live data shape: (6967, 23)\n",
            "\n",
            "Target distribution in training data:\n",
            "label\n",
            "0.0    9719\n",
            "1.0    7001\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Class balance: 0.419\n"
          ]
        }
      ],
      "source": [
        "# Load processed data\n",
        "train_data = pd.read_csv('./Data/salary.train.processed.csv', index_col='id')\n",
        "test_data = pd.read_csv('./Data/salary.test.processed.csv', index_col='id')\n",
        "live_data = pd.read_csv('./Data/salary.live.processed.csv', index_col='id')\n",
        "\n",
        "print(f\"Training data shape: {train_data.shape}\")\n",
        "print(f\"Test data shape: {test_data.shape}\")\n",
        "print(f\"Live data shape: {live_data.shape}\")\n",
        "\n",
        "# Check target distribution\n",
        "print(\"\\nTarget distribution in training data:\")\n",
        "print(train_data['label'].value_counts())\n",
        "print(f\"\\nClass balance: {train_data['label'].mean():.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Prepare Features and Target\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature matrix shape: (16720, 23)\n",
            "Features: ['marital-status_married-civ-spouse', 'relationship_husband', 'marital-status_never-married', 'education-num', 'capitalgain', 'age-group', 'relationship_own-child', 'hoursperweek', 'sex_male', 'relationship_not-in-family', 'occupation_prof-specialty', 'occupation_other-service', 'relationship_unmarried', 'marital-status_divorced', 'capitalloss', 'occupation_exec-managerial', 'workclass_self-emp-inc', 'relationship_wife', 'workclass_private', 'race_black', 'race_white', 'relationship_other-relative', 'occupation_handlers-cleaners']\n",
            "\n",
            "Class weights: {0: 0.8601707994649656, 1: 1.1941151264105128}\n"
          ]
        }
      ],
      "source": [
        "# Prepare features and target\n",
        "X_train = train_data.drop(columns=['label'])\n",
        "y_train = train_data['label']\n",
        "X_test = test_data.drop(columns=['label'])\n",
        "y_test = test_data['label']\n",
        "X_live = live_data\n",
        "\n",
        "print(f\"Feature matrix shape: {X_train.shape}\")\n",
        "print(f\"Features: {list(X_train.columns)}\")\n",
        "\n",
        "# Calculate class weights for imbalanced data\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
        "print(f\"\\nClass weights: {class_weight_dict}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Hyperparameter Tuning for XGBoost\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing parameter combinations...\n",
            "Testing combination 1/5...\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "\nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n5 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\natth\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\natth\\anaconda3\\Lib\\site-packages\\xgboost\\core.py\", line 774, in inner_f\n    return func(**kwargs)\n           ^^^^^^^^^^^^^^\n  File \"c:\\Users\\natth\\anaconda3\\Lib\\site-packages\\xgboost\\sklearn.py\", line 1803, in fit\n    self._Booster = train(\n                    ^^^^^^\n  File \"c:\\Users\\natth\\anaconda3\\Lib\\site-packages\\xgboost\\core.py\", line 774, in inner_f\n    return func(**kwargs)\n           ^^^^^^^^^^^^^^\n  File \"c:\\Users\\natth\\anaconda3\\Lib\\site-packages\\xgboost\\training.py\", line 200, in train\n    if cb_container.after_iteration(bst, i, dtrain, evals):\n       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\natth\\anaconda3\\Lib\\site-packages\\xgboost\\callback.py\", line 269, in after_iteration\n    ret = any(c.after_iteration(model, epoch, self.history) for c in self.callbacks)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\natth\\anaconda3\\Lib\\site-packages\\xgboost\\callback.py\", line 269, in <genexpr>\n    ret = any(c.after_iteration(model, epoch, self.history) for c in self.callbacks)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\natth\\anaconda3\\Lib\\site-packages\\xgboost\\callback.py\", line 461, in after_iteration\n    raise ValueError(msg)\nValueError: Must have at least 1 validation dataset for early stopping.\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[4], line 43\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, params \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(test_params):\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting combination \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(test_params)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 43\u001b[0m     mean_score, std_score \u001b[38;5;241m=\u001b[39m evaluate_xgb_params(params, X_train, y_train)\n\u001b[0;32m     44\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend((params, mean_score, std_score))\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mean_score \u001b[38;5;241m>\u001b[39m best_score:\n",
            "Cell \u001b[1;32mIn[4], line 12\u001b[0m, in \u001b[0;36mevaluate_xgb_params\u001b[1;34m(params, X, y, cv_folds)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Evaluate XGBoost parameters using cross-validation\"\"\"\u001b[39;00m\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBClassifier(\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m      6\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     verbosity\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     10\u001b[0m )\n\u001b[1;32m---> 12\u001b[0m cv_scores \u001b[38;5;241m=\u001b[39m cross_val_score(\n\u001b[0;32m     13\u001b[0m     model, X, y, \n\u001b[0;32m     14\u001b[0m     cv\u001b[38;5;241m=\u001b[39mStratifiedKFold(n_splits\u001b[38;5;241m=\u001b[39mcv_folds, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m),\n\u001b[0;32m     15\u001b[0m     scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     16\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     17\u001b[0m )\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_scores\u001b[38;5;241m.\u001b[39mmean(), cv_scores\u001b[38;5;241m.\u001b[39mstd()\n",
            "File \u001b[1;32mc:\\Users\\natth\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\natth\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:712\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    710\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[1;32m--> 712\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m cross_validate(\n\u001b[0;32m    713\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m    714\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m    715\u001b[0m     y\u001b[38;5;241m=\u001b[39my,\n\u001b[0;32m    716\u001b[0m     groups\u001b[38;5;241m=\u001b[39mgroups,\n\u001b[0;32m    717\u001b[0m     scoring\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m: scorer},\n\u001b[0;32m    718\u001b[0m     cv\u001b[38;5;241m=\u001b[39mcv,\n\u001b[0;32m    719\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[0;32m    720\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    721\u001b[0m     fit_params\u001b[38;5;241m=\u001b[39mfit_params,\n\u001b[0;32m    722\u001b[0m     params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    723\u001b[0m     pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch,\n\u001b[0;32m    724\u001b[0m     error_score\u001b[38;5;241m=\u001b[39merror_score,\n\u001b[0;32m    725\u001b[0m )\n\u001b[0;32m    726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
            "File \u001b[1;32mc:\\Users\\natth\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\natth\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:443\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[0;32m    422\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[0;32m    423\u001b[0m results \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    424\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    425\u001b[0m         clone(estimator),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m indices\n\u001b[0;32m    441\u001b[0m )\n\u001b[1;32m--> 443\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    445\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(scoring):\n",
            "File \u001b[1;32mc:\\Users\\natth\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:529\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[0;32m    523\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    524\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    525\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    526\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    527\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    528\u001b[0m     )\n\u001b[1;32m--> 529\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    532\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    533\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    534\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    538\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    539\u001b[0m     )\n",
            "\u001b[1;31mValueError\u001b[0m: \nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n5 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\natth\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\natth\\anaconda3\\Lib\\site-packages\\xgboost\\core.py\", line 774, in inner_f\n    return func(**kwargs)\n           ^^^^^^^^^^^^^^\n  File \"c:\\Users\\natth\\anaconda3\\Lib\\site-packages\\xgboost\\sklearn.py\", line 1803, in fit\n    self._Booster = train(\n                    ^^^^^^\n  File \"c:\\Users\\natth\\anaconda3\\Lib\\site-packages\\xgboost\\core.py\", line 774, in inner_f\n    return func(**kwargs)\n           ^^^^^^^^^^^^^^\n  File \"c:\\Users\\natth\\anaconda3\\Lib\\site-packages\\xgboost\\training.py\", line 200, in train\n    if cb_container.after_iteration(bst, i, dtrain, evals):\n       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\natth\\anaconda3\\Lib\\site-packages\\xgboost\\callback.py\", line 269, in after_iteration\n    ret = any(c.after_iteration(model, epoch, self.history) for c in self.callbacks)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\natth\\anaconda3\\Lib\\site-packages\\xgboost\\callback.py\", line 269, in <genexpr>\n    ret = any(c.after_iteration(model, epoch, self.history) for c in self.callbacks)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\natth\\anaconda3\\Lib\\site-packages\\xgboost\\callback.py\", line 461, in after_iteration\n    raise ValueError(msg)\nValueError: Must have at least 1 validation dataset for early stopping.\n"
          ]
        }
      ],
      "source": [
        "# Manual hyperparameter tuning (since GridSearchCV can be slow)\n",
        "def evaluate_xgb_params(params, X, y, cv_folds=5):\n",
        "    \"\"\"Evaluate XGBoost parameters using cross-validation\"\"\"\n",
        "    model = xgb.XGBClassifier(\n",
        "        **params,\n",
        "        random_state=42,\n",
        "        eval_metric='logloss',\n",
        "        early_stopping_rounds=50,\n",
        "        verbosity=0\n",
        "    )\n",
        "    \n",
        "    cv_scores = cross_val_score(\n",
        "        model, X, y, \n",
        "        cv=StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42),\n",
        "        scoring='f1',\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    \n",
        "    return cv_scores.mean(), cv_scores.std()\n",
        "\n",
        "# Test different parameter combinations\n",
        "best_score = 0\n",
        "best_params = None\n",
        "results = []\n",
        "\n",
        "# Test key parameter combinations optimized for F1 score\n",
        "test_params = [\n",
        "    {'n_estimators': 300, 'max_depth': 6, 'learning_rate': 0.05, 'subsample': 0.9, \n",
        "     'colsample_bytree': 0.9, 'reg_alpha': 0.1, 'reg_lambda': 1.5, 'scale_pos_weight': 2},\n",
        "    {'n_estimators': 500, 'max_depth': 8, 'learning_rate': 0.01, 'subsample': 0.8, \n",
        "     'colsample_bytree': 0.8, 'reg_alpha': 1, 'reg_lambda': 2, 'scale_pos_weight': 3},\n",
        "    {'n_estimators': 200, 'max_depth': 4, 'learning_rate': 0.1, 'subsample': 1.0, \n",
        "     'colsample_bytree': 1.0, 'reg_alpha': 0, 'reg_lambda': 1, 'scale_pos_weight': 1},\n",
        "    {'n_estimators': 400, 'max_depth': 6, 'learning_rate': 0.03, 'subsample': 0.85, \n",
        "     'colsample_bytree': 0.85, 'reg_alpha': 0.5, 'reg_lambda': 1.5, 'scale_pos_weight': 2.5},\n",
        "    {'n_estimators': 600, 'max_depth': 7, 'learning_rate': 0.02, 'subsample': 0.9, \n",
        "     'colsample_bytree': 0.9, 'reg_alpha': 0.2, 'reg_lambda': 1.8, 'scale_pos_weight': 2.2}\n",
        "]\n",
        "\n",
        "print(\"Testing parameter combinations...\")\n",
        "for i, params in enumerate(test_params):\n",
        "    print(f\"Testing combination {i+1}/{len(test_params)}...\")\n",
        "    mean_score, std_score = evaluate_xgb_params(params, X_train, y_train)\n",
        "    results.append((params, mean_score, std_score))\n",
        "    \n",
        "    if mean_score > best_score:\n",
        "        best_score = mean_score\n",
        "        best_params = params\n",
        "    \n",
        "    print(f\"F1 Score: {mean_score:.4f} (+/- {std_score:.4f})\")\n",
        "\n",
        "print(f\"\\nBest F1 Score: {best_score:.4f}\")\n",
        "print(f\"Best Parameters: {best_params}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Train Final XGBoost Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train final model with best parameters\n",
        "final_model = xgb.XGBClassifier(\n",
        "    **best_params,\n",
        "    random_state=42,\n",
        "    eval_metric='logloss',\n",
        "    early_stopping_rounds=50,\n",
        "    verbosity=0\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "final_model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Model trained successfully!\")\n",
        "print(f\"Number of features used: {final_model.n_features_in_}\")\n",
        "print(f\"Number of estimators: {final_model.n_estimators}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions\n",
        "y_pred_train = final_model.predict(X_train)\n",
        "y_pred_test = final_model.predict(X_test)\n",
        "y_pred_proba_test = final_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate metrics\n",
        "train_f1 = f1_score(y_train, y_pred_train)\n",
        "test_f1 = f1_score(y_test, y_pred_test)\n",
        "test_auc = roc_auc_score(y_test, y_pred_proba_test)\n",
        "\n",
        "print(\"=== MODEL PERFORMANCE ===\")\n",
        "print(f\"Training F1 Score: {train_f1:.4f}\")\n",
        "print(f\"Test F1 Score: {test_f1:.4f}\")\n",
        "print(f\"Test AUC Score: {test_auc:.4f}\")\n",
        "\n",
        "# Detailed classification report\n",
        "print(\"\\n=== DETAILED CLASSIFICATION REPORT ===\")\n",
        "print(classification_report(y_test, y_pred_test))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred_test)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=['Low Income', 'High Income'], \n",
        "            yticklabels=['Low Income', 'High Income'])\n",
        "plt.title('Confusion Matrix - XGBoost Model')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Make Predictions on Live Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions on live data\n",
        "live_predictions = final_model.predict(X_live)\n",
        "live_probabilities = final_model.predict_proba(X_live)[:, 1]\n",
        "\n",
        "# Create prediction dataframe\n",
        "live_results = pd.DataFrame({\n",
        "    'id': X_live.index,\n",
        "    'predicted_label': live_predictions,\n",
        "    'probability_high_income': live_probabilities\n",
        "})\n",
        "\n",
        "print(f\"Live data predictions completed!\")\n",
        "print(f\"Number of predictions: {len(live_results)}\")\n",
        "print(f\"High income predictions: {live_predictions.sum()}\")\n",
        "print(f\"Low income predictions: {len(live_predictions) - live_predictions.sum()}\")\n",
        "\n",
        "# Show sample predictions\n",
        "print(\"\\nSample predictions:\")\n",
        "print(live_results.head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Save Model and Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the trained model\n",
        "joblib.dump(final_model, './for_cursur/xgb_model.joblib')\n",
        "\n",
        "# Save predictions\n",
        "live_results.to_csv('./for_cursur/xgb_predictions.csv', index=False)\n",
        "\n",
        "# Save model configuration\n",
        "model_config = {\n",
        "    'model_type': 'XGBoost',\n",
        "    'parameters': best_params,\n",
        "    'test_f1_score': test_f1,\n",
        "    'test_auc_score': test_auc,\n",
        "    'cv_mean_f1': best_score,\n",
        "    'feature_count': final_model.n_features_in_,\n",
        "    'training_samples': len(X_train),\n",
        "    'test_samples': len(X_test)\n",
        "}\n",
        "\n",
        "import json\n",
        "with open('./for_cursur/xgb_model_config.json', 'w') as f:\n",
        "    json.dump(model_config, f, indent=2)\n",
        "\n",
        "print(\"Model and results saved successfully!\")\n",
        "print(f\"\\nFinal Test F1 Score: {test_f1:.4f}\")\n",
        "print(f\"Target achieved: {'✅ YES' if test_f1 >= 0.90 else '❌ NO'}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
