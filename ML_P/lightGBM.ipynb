{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4816981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "data_train = pd.read_csv('./data/salary.train.processed.csv').set_index('id')\n",
    "data_test = pd.read_csv('./data/salary.test.processed.csv').set_index('id')\n",
    "# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î target\n",
    "target = 'label'\n",
    "\n",
    "# ‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ù‡∏∂‡∏Å‡πÅ‡∏•‡∏∞‡∏ó‡∏î‡∏™‡∏≠‡∏ö\n",
    "X_train = data_train.drop(columns=[target])\n",
    "y_train = data_train[target]\n",
    "X_test = data_test.drop(columns=[target])\n",
    "y_test = data_test[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "492eb3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in c:\\users\\natth\\anaconda3\\lib\\site-packages (4.5.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in c:\\users\\natth\\anaconda3\\lib\\site-packages (from optuna) (1.13.3)\n",
      "Requirement already satisfied: colorlog in c:\\users\\natth\\anaconda3\\lib\\site-packages (from optuna) (6.10.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\natth\\anaconda3\\lib\\site-packages (from optuna) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\natth\\anaconda3\\lib\\site-packages (from optuna) (24.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in c:\\users\\natth\\anaconda3\\lib\\site-packages (from optuna) (2.0.34)\n",
      "Requirement already satisfied: tqdm in c:\\users\\natth\\anaconda3\\lib\\site-packages (from optuna) (4.66.5)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\natth\\anaconda3\\lib\\site-packages (from optuna) (6.0.1)\n",
      "Requirement already satisfied: Mako in c:\\users\\natth\\anaconda3\\lib\\site-packages (from alembic>=1.5.0->optuna) (1.2.3)\n",
      "Requirement already satisfied: typing-extensions>=4 in c:\\users\\natth\\anaconda3\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.11.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\natth\\anaconda3\\lib\\site-packages (from sqlalchemy>=1.4.2->optuna) (3.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\natth\\anaconda3\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\natth\\anaconda3\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a260cad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in c:\\users\\natth\\anaconda3\\lib\\site-packages (4.6.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\natth\\anaconda3\\lib\\site-packages (from lightgbm) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\natth\\anaconda3\\lib\\site-packages (from lightgbm) (1.13.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lightgbm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88f26698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LightGBM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0   0.858539  0.836507  0.847379      2416\n",
      "         1.0   0.783680  0.811224  0.797214      1764\n",
      "\n",
      "    accuracy                       0.825837      4180\n",
      "   macro avg   0.821109  0.823866  0.822297      4180\n",
      "weighted avg   0.826948  0.825837  0.826209      4180\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import lightgbm as lgbm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    train_df = pandas.read_csv('./data/salary.train.processed.csv', index_col='id')\n",
    "    test_df = pandas.read_csv('./data/salary.test.processed.csv', index_col='id')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Could not find the processed CSV files.\")\n",
    "    raise\n",
    "\n",
    "X_train = train_df.drop('label', axis=1)\n",
    "y_train = train_df['label']\n",
    "X_test = test_df.drop('label', axis=1)\n",
    "y_test = test_df['label']\n",
    "# --- End of Data Loading ---\n",
    "\n",
    "# --- Train-Validation Split ---\n",
    "X_train_final, X_val_final, y_train_final, y_val_final = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "# --- Define LightGBM Model with Default Parameters ---\n",
    "lgbm_model = lgbm.LGBMClassifier(\n",
    "    objective='binary',\n",
    "    metric='binary_logloss',\n",
    "    random_state=42,\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.1,  # ‡∏Ñ‡πà‡∏≤ default\n",
    "    num_leaves=31,      # ‡∏Ñ‡πà‡∏≤ default\n",
    "    max_depth=-1,       # ‡∏Ñ‡πà‡∏≤ default (‡πÑ‡∏°‡πà‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏∂‡∏Å)\n",
    "    min_child_samples=20,  # ‡∏Ñ‡πà‡∏≤ default\n",
    "    subsample=1.0,      # ‡∏Ñ‡πà‡∏≤ default\n",
    "    colsample_bytree=1.0,  # ‡∏Ñ‡πà‡∏≤ default\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "# --- Train the Model with Early Stopping ---\n",
    "lgbm_model.fit(\n",
    "    X_train_final,\n",
    "    y_train_final,\n",
    "    eval_set=[(X_val_final, y_val_final)],\n",
    "    eval_metric='logloss',\n",
    "    callbacks=[lgbm.early_stopping(50, verbose=False)]\n",
    ")\n",
    "\n",
    "# --- Evaluate on Test Set ---\n",
    "y_pred_lgbm = lgbm_model.predict(X_test)\n",
    "\n",
    "print(\"\\nLightGBM Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_lgbm, digits=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24ac8e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-22 06:40:26,329] A new study created in memory with name: lgbm_salary_tuning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Optuna study: 'lgbm_salary_tuning'\n",
      "Running 200 new trials...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-22 06:40:26,848] Trial 0 finished with value: 0.7905935909105978 and parameters: {'learning_rate': 0.13087099233680682, 'num_leaves': 24, 'max_depth': 6, 'min_child_samples': 9, 'subsample': 0.9340840184473755, 'colsample_bytree': 0.8102533462424166}. Best is trial 0 with value: 0.7905935909105978.\n",
      "[I 2025-10-22 06:40:29,930] Trial 1 finished with value: 0.7875580630145745 and parameters: {'learning_rate': 0.016043146631172546, 'num_leaves': 44, 'max_depth': 3, 'min_child_samples': 16, 'subsample': 0.819539102780078, 'colsample_bytree': 0.6909344043303839}. Best is trial 0 with value: 0.7905935909105978.\n",
      "[I 2025-10-22 06:40:34,878] Trial 2 finished with value: 0.791800765921803 and parameters: {'learning_rate': 0.010205717272192534, 'num_leaves': 20, 'max_depth': 6, 'min_child_samples': 11, 'subsample': 0.7938575279368162, 'colsample_bytree': 0.8247383883688079}. Best is trial 2 with value: 0.791800765921803.\n",
      "[I 2025-10-22 06:40:37,743] Trial 3 finished with value: 0.7904615048585092 and parameters: {'learning_rate': 0.016711392530532485, 'num_leaves': 24, 'max_depth': 8, 'min_child_samples': 30, 'subsample': 0.7256862015845724, 'colsample_bytree': 0.9351840975412901}. Best is trial 2 with value: 0.791800765921803.\n",
      "[I 2025-10-22 06:40:39,683] Trial 4 finished with value: 0.7904437915154646 and parameters: {'learning_rate': 0.026400771413599793, 'num_leaves': 48, 'max_depth': 8, 'min_child_samples': 5, 'subsample': 0.9668877975131884, 'colsample_bytree': 0.8442581911279092}. Best is trial 2 with value: 0.791800765921803.\n",
      "[I 2025-10-22 06:40:41,200] Trial 5 finished with value: 0.7892173782179593 and parameters: {'learning_rate': 0.03569409719762338, 'num_leaves': 31, 'max_depth': 5, 'min_child_samples': 22, 'subsample': 0.6695561561904795, 'colsample_bytree': 0.8030862016287226}. Best is trial 2 with value: 0.791800765921803.\n",
      "[I 2025-10-22 06:40:42,292] Trial 6 finished with value: 0.7888438732898863 and parameters: {'learning_rate': 0.04654081406634691, 'num_leaves': 15, 'max_depth': 4, 'min_child_samples': 13, 'subsample': 0.9157157142006024, 'colsample_bytree': 0.8499634427454636}. Best is trial 2 with value: 0.791800765921803.\n",
      "[I 2025-10-22 06:40:43,224] Trial 7 finished with value: 0.790904621076169 and parameters: {'learning_rate': 0.13982598955555806, 'num_leaves': 33, 'max_depth': 10, 'min_child_samples': 8, 'subsample': 0.8277602548120038, 'colsample_bytree': 0.7046623781494767}. Best is trial 2 with value: 0.791800765921803.\n",
      "[I 2025-10-22 06:40:43,736] Trial 8 finished with value: 0.7891924345922993 and parameters: {'learning_rate': 0.229077891470972, 'num_leaves': 16, 'max_depth': 9, 'min_child_samples': 28, 'subsample': 0.6870650942547908, 'colsample_bytree': 0.890187474340828}. Best is trial 2 with value: 0.791800765921803.\n",
      "[I 2025-10-22 06:40:45,261] Trial 9 finished with value: 0.7900398324617305 and parameters: {'learning_rate': 0.030835824687936542, 'num_leaves': 24, 'max_depth': 7, 'min_child_samples': 16, 'subsample': 0.6450970974957618, 'colsample_bytree': 0.9326080058696937}. Best is trial 2 with value: 0.791800765921803.\n",
      "[I 2025-10-22 06:40:48,821] Trial 10 finished with value: 0.7900291647897598 and parameters: {'learning_rate': 0.011320268759016105, 'num_leaves': 12, 'max_depth': 6, 'min_child_samples': 22, 'subsample': 0.7360234710284631, 'colsample_bytree': 0.6070669806794334}. Best is trial 2 with value: 0.791800765921803.\n",
      "[I 2025-10-22 06:40:49,437] Trial 11 finished with value: 0.7885108677400697 and parameters: {'learning_rate': 0.10116716032058401, 'num_leaves': 34, 'max_depth': 10, 'min_child_samples': 10, 'subsample': 0.832323633911522, 'colsample_bytree': 0.7206211388770888}. Best is trial 2 with value: 0.791800765921803.\n",
      "[I 2025-10-22 06:40:50,148] Trial 12 finished with value: 0.7892308537298852 and parameters: {'learning_rate': 0.08015701712509904, 'num_leaves': 39, 'max_depth': 10, 'min_child_samples': 5, 'subsample': 0.8727734283543468, 'colsample_bytree': 0.7290245559623891}. Best is trial 2 with value: 0.791800765921803.\n",
      "[I 2025-10-22 06:40:50,578] Trial 13 finished with value: 0.7907692040997274 and parameters: {'learning_rate': 0.29448295313960976, 'num_leaves': 21, 'max_depth': 7, 'min_child_samples': 10, 'subsample': 0.760672302986922, 'colsample_bytree': 0.6598956651454646}. Best is trial 2 with value: 0.791800765921803.\n",
      "[I 2025-10-22 06:40:51,207] Trial 14 finished with value: 0.7907704366794382 and parameters: {'learning_rate': 0.17566937346353806, 'num_leaves': 36, 'max_depth': 5, 'min_child_samples': 13, 'subsample': 0.7781352969627583, 'colsample_bytree': 0.9960745028284625}. Best is trial 2 with value: 0.791800765921803.\n",
      "[I 2025-10-22 06:40:52,154] Trial 15 finished with value: 0.7907359569927505 and parameters: {'learning_rate': 0.07317545078932124, 'num_leaves': 28, 'max_depth': 8, 'min_child_samples': 8, 'subsample': 0.8450286738623388, 'colsample_bytree': 0.7634168015354467}. Best is trial 2 with value: 0.791800765921803.\n",
      "[I 2025-10-22 06:40:52,699] Trial 16 finished with value: 0.7920625571468835 and parameters: {'learning_rate': 0.13910944588082466, 'num_leaves': 42, 'max_depth': 5, 'min_child_samples': 13, 'subsample': 0.8753688787537255, 'colsample_bytree': 0.648935684085874}. Best is trial 16 with value: 0.7920625571468835.\n",
      "[I 2025-10-22 06:40:53,698] Trial 17 finished with value: 0.788775621374817 and parameters: {'learning_rate': 0.055145838459847295, 'num_leaves': 41, 'max_depth': 4, 'min_child_samples': 19, 'subsample': 0.9970917992976974, 'colsample_bytree': 0.631897214302083}. Best is trial 16 with value: 0.7920625571468835.\n",
      "[I 2025-10-22 06:40:56,251] Trial 18 finished with value: 0.78979455544131 and parameters: {'learning_rate': 0.01594673904560537, 'num_leaves': 49, 'max_depth': 5, 'min_child_samples': 14, 'subsample': 0.8903061472754512, 'colsample_bytree': 0.7564313192797807}. Best is trial 16 with value: 0.7920625571468835.\n",
      "[I 2025-10-22 06:40:58,300] Trial 19 finished with value: 0.7886236363113269 and parameters: {'learning_rate': 0.024119029340821444, 'num_leaves': 18, 'max_depth': 3, 'min_child_samples': 18, 'subsample': 0.6031506881143963, 'colsample_bytree': 0.6586466408274894}. Best is trial 16 with value: 0.7920625571468835.\n",
      "[I 2025-10-22 06:41:00,794] Trial 20 finished with value: 0.7906036169950029 and parameters: {'learning_rate': 0.010357738598118534, 'num_leaves': 10, 'max_depth': 6, 'min_child_samples': 21, 'subsample': 0.7889209327338096, 'colsample_bytree': 0.8556576486211878}. Best is trial 16 with value: 0.7920625571468835.\n",
      "[I 2025-10-22 06:41:01,304] Trial 21 finished with value: 0.7893848297223881 and parameters: {'learning_rate': 0.13113813638220723, 'num_leaves': 28, 'max_depth': 4, 'min_child_samples': 7, 'subsample': 0.8750298858692439, 'colsample_bytree': 0.6945812075198371}. Best is trial 16 with value: 0.7920625571468835.\n",
      "[I 2025-10-22 06:41:01,849] Trial 22 finished with value: 0.7891387266096653 and parameters: {'learning_rate': 0.1592329495163575, 'num_leaves': 43, 'max_depth': 7, 'min_child_samples': 12, 'subsample': 0.8149182183483948, 'colsample_bytree': 0.6002480378603308}. Best is trial 16 with value: 0.7920625571468835.\n",
      "[I 2025-10-22 06:41:02,437] Trial 23 finished with value: 0.7886585895841801 and parameters: {'learning_rate': 0.10133821723507182, 'num_leaves': 36, 'max_depth': 9, 'min_child_samples': 11, 'subsample': 0.8381784427361397, 'colsample_bytree': 0.7618118689118674}. Best is trial 16 with value: 0.7920625571468835.\n",
      "[I 2025-10-22 06:41:02,836] Trial 24 finished with value: 0.7886403599203219 and parameters: {'learning_rate': 0.23389038300993042, 'num_leaves': 33, 'max_depth': 5, 'min_child_samples': 7, 'subsample': 0.926730566860701, 'colsample_bytree': 0.6686376579614871}. Best is trial 16 with value: 0.7920625571468835.\n",
      "[I 2025-10-22 06:41:04,034] Trial 25 finished with value: 0.7909585741862206 and parameters: {'learning_rate': 0.062092892646704104, 'num_leaves': 46, 'max_depth': 6, 'min_child_samples': 15, 'subsample': 0.7384284114970999, 'colsample_bytree': 0.7247422768834846}. Best is trial 16 with value: 0.7920625571468835.\n",
      "[I 2025-10-22 06:41:05,098] Trial 26 finished with value: 0.791540142568549 and parameters: {'learning_rate': 0.05048460751796938, 'num_leaves': 46, 'max_depth': 6, 'min_child_samples': 15, 'subsample': 0.723712150646842, 'colsample_bytree': 0.7441530349931365}. Best is trial 16 with value: 0.7920625571468835.\n",
      "[I 2025-10-22 06:41:06,188] Trial 27 finished with value: 0.7899247197973654 and parameters: {'learning_rate': 0.04073232664239562, 'num_leaves': 50, 'max_depth': 5, 'min_child_samples': 25, 'subsample': 0.7073632257949256, 'colsample_bytree': 0.7797893733092099}. Best is trial 16 with value: 0.7920625571468835.\n",
      "[I 2025-10-22 06:41:22,082] Trial 28 finished with value: 0.7906540718863958 and parameters: {'learning_rate': 0.021987046120595176, 'num_leaves': 39, 'max_depth': 7, 'min_child_samples': 19, 'subsample': 0.8603371254883657, 'colsample_bytree': 0.8324432686277782}. Best is trial 16 with value: 0.7920625571468835.\n",
      "[I 2025-10-22 06:41:23,129] Trial 29 finished with value: 0.7890484074939742 and parameters: {'learning_rate': 0.09960287746905441, 'num_leaves': 46, 'max_depth': 6, 'min_child_samples': 17, 'subsample': 0.763062971097846, 'colsample_bytree': 0.8097067479197961}. Best is trial 16 with value: 0.7920625571468835.\n",
      "[I 2025-10-22 06:41:28,677] Trial 30 finished with value: 0.7895617518711774 and parameters: {'learning_rate': 0.012495834829642479, 'num_leaves': 27, 'max_depth': 4, 'min_child_samples': 12, 'subsample': 0.7980174433486774, 'colsample_bytree': 0.8723278689618479}. Best is trial 16 with value: 0.7920625571468835.\n",
      "[I 2025-10-22 06:41:30,303] Trial 31 finished with value: 0.7921281025523323 and parameters: {'learning_rate': 0.05499827908972434, 'num_leaves': 46, 'max_depth': 6, 'min_child_samples': 15, 'subsample': 0.7158845056558323, 'colsample_bytree': 0.7494954453917052}. Best is trial 31 with value: 0.7921281025523323.\n",
      "[I 2025-10-22 06:41:31,540] Trial 32 finished with value: 0.7910776144777257 and parameters: {'learning_rate': 0.04744423443816679, 'num_leaves': 44, 'max_depth': 6, 'min_child_samples': 15, 'subsample': 0.6483684868276018, 'colsample_bytree': 0.791543570128399}. Best is trial 31 with value: 0.7921281025523323.\n",
      "[I 2025-10-22 06:41:32,550] Trial 33 finished with value: 0.7902640697606446 and parameters: {'learning_rate': 0.0779359410595461, 'num_leaves': 41, 'max_depth': 5, 'min_child_samples': 14, 'subsample': 0.7542675398901427, 'colsample_bytree': 0.7398893963111836}. Best is trial 31 with value: 0.7921281025523323.\n",
      "[I 2025-10-22 06:41:35,187] Trial 34 finished with value: 0.7904501827393148 and parameters: {'learning_rate': 0.01663953343442827, 'num_leaves': 47, 'max_depth': 6, 'min_child_samples': 17, 'subsample': 0.7107740825619321, 'colsample_bytree': 0.8189399295563465}. Best is trial 31 with value: 0.7921281025523323.\n",
      "[I 2025-10-22 06:41:37,122] Trial 35 finished with value: 0.7907296296974144 and parameters: {'learning_rate': 0.019730782978831986, 'num_leaves': 44, 'max_depth': 7, 'min_child_samples': 11, 'subsample': 0.6939845120571488, 'colsample_bytree': 0.9034564076103215}. Best is trial 31 with value: 0.7921281025523323.\n",
      "[I 2025-10-22 06:41:38,377] Trial 36 finished with value: 0.7916301093386472 and parameters: {'learning_rate': 0.03300302834876719, 'num_leaves': 39, 'max_depth': 8, 'min_child_samples': 15, 'subsample': 0.667743104207797, 'colsample_bytree': 0.785891630690493}. Best is trial 31 with value: 0.7921281025523323.\n",
      "[I 2025-10-22 06:41:39,504] Trial 37 finished with value: 0.7903015199771178 and parameters: {'learning_rate': 0.038269902489418146, 'num_leaves': 39, 'max_depth': 8, 'min_child_samples': 13, 'subsample': 0.6220194398460348, 'colsample_bytree': 0.7818885407583023}. Best is trial 31 with value: 0.7921281025523323.\n",
      "[I 2025-10-22 06:41:40,961] Trial 38 finished with value: 0.7913281570812712 and parameters: {'learning_rate': 0.0270443756181006, 'num_leaves': 36, 'max_depth': 8, 'min_child_samples': 9, 'subsample': 0.6619133575319733, 'colsample_bytree': 0.6358154357287715}. Best is trial 31 with value: 0.7921281025523323.\n",
      "[I 2025-10-22 06:41:43,059] Trial 39 finished with value: 0.7910013051044964 and parameters: {'learning_rate': 0.03160639998563795, 'num_leaves': 42, 'max_depth': 9, 'min_child_samples': 16, 'subsample': 0.9627526475368491, 'colsample_bytree': 0.8388480566248384}. Best is trial 31 with value: 0.7921281025523323.\n",
      "[I 2025-10-22 06:41:46,209] Trial 40 finished with value: 0.7914738255373442 and parameters: {'learning_rate': 0.013006466895764842, 'num_leaves': 22, 'max_depth': 7, 'min_child_samples': 20, 'subsample': 0.6819635659367338, 'colsample_bytree': 0.8225313429165793}. Best is trial 31 with value: 0.7921281025523323.\n",
      "[I 2025-10-22 06:41:47,096] Trial 41 finished with value: 0.7901425130659847 and parameters: {'learning_rate': 0.06002285078088182, 'num_leaves': 45, 'max_depth': 5, 'min_child_samples': 14, 'subsample': 0.7222139156282577, 'colsample_bytree': 0.7442648395812218}. Best is trial 31 with value: 0.7921281025523323.\n",
      "[I 2025-10-22 06:41:48,105] Trial 42 finished with value: 0.7915482669874486 and parameters: {'learning_rate': 0.04753545690184944, 'num_leaves': 48, 'max_depth': 6, 'min_child_samples': 16, 'subsample': 0.901741838875164, 'colsample_bytree': 0.7052621116972293}. Best is trial 31 with value: 0.7921281025523323.\n",
      "[I 2025-10-22 06:41:49,778] Trial 43 finished with value: 0.7920171609658091 and parameters: {'learning_rate': 0.043266343667828396, 'num_leaves': 48, 'max_depth': 6, 'min_child_samples': 16, 'subsample': 0.9106181771566441, 'colsample_bytree': 0.6895796443683581}. Best is trial 31 with value: 0.7921281025523323.\n",
      "[I 2025-10-22 06:41:51,935] Trial 44 finished with value: 0.7921670827786587 and parameters: {'learning_rate': 0.03150536095889577, 'num_leaves': 49, 'max_depth': 7, 'min_child_samples': 12, 'subsample': 0.9366008653992742, 'colsample_bytree': 0.6683861699705071}. Best is trial 44 with value: 0.7921670827786587.\n",
      "[I 2025-10-22 06:41:53,787] Trial 45 finished with value: 0.790646531601659 and parameters: {'learning_rate': 0.040906645655958085, 'num_leaves': 50, 'max_depth': 7, 'min_child_samples': 12, 'subsample': 0.963652347217036, 'colsample_bytree': 0.6794060891321025}. Best is trial 44 with value: 0.7921670827786587.\n",
      "[I 2025-10-22 06:41:56,752] Trial 46 finished with value: 0.7904006205252982 and parameters: {'learning_rate': 0.020178445160505824, 'num_leaves': 48, 'max_depth': 5, 'min_child_samples': 10, 'subsample': 0.9420719948481715, 'colsample_bytree': 0.6310758561655714}. Best is trial 44 with value: 0.7921670827786587.\n",
      "[I 2025-10-22 06:41:58,395] Trial 47 finished with value: 0.7905992865916621 and parameters: {'learning_rate': 0.02657696159654196, 'num_leaves': 18, 'max_depth': 6, 'min_child_samples': 11, 'subsample': 0.9402057373771604, 'colsample_bytree': 0.6454438360672565}. Best is trial 44 with value: 0.7921670827786587.\n",
      "[I 2025-10-22 06:41:59,166] Trial 48 finished with value: 0.7902462496970623 and parameters: {'learning_rate': 0.11258276296424735, 'num_leaves': 50, 'max_depth': 4, 'min_child_samples': 13, 'subsample': 0.9071916820474194, 'colsample_bytree': 0.7113324721266658}. Best is trial 44 with value: 0.7921670827786587.\n",
      "[I 2025-10-22 06:42:00,518] Trial 49 finished with value: 0.7901178995914427 and parameters: {'learning_rate': 0.06668813280042657, 'num_leaves': 48, 'max_depth': 7, 'min_child_samples': 23, 'subsample': 0.9930853723851091, 'colsample_bytree': 0.6831937994528441}. Best is trial 44 with value: 0.7921670827786587.\n",
      "[I 2025-10-22 06:42:01,352] Trial 50 finished with value: 0.7899472409395875 and parameters: {'learning_rate': 0.08806127736534199, 'num_leaves': 15, 'max_depth': 3, 'min_child_samples': 18, 'subsample': 0.8877204112776679, 'colsample_bytree': 0.6243525689399148}. Best is trial 44 with value: 0.7921670827786587.\n",
      "[I 2025-10-22 06:42:03,482] Trial 51 finished with value: 0.7920976663829808 and parameters: {'learning_rate': 0.031059993307571754, 'num_leaves': 41, 'max_depth': 8, 'min_child_samples': 14, 'subsample': 0.9234781859780244, 'colsample_bytree': 0.6528289114167698}. Best is trial 44 with value: 0.7921670827786587.\n",
      "[I 2025-10-22 06:42:05,521] Trial 52 finished with value: 0.7922134070306397 and parameters: {'learning_rate': 0.03476497162363416, 'num_leaves': 42, 'max_depth': 8, 'min_child_samples': 9, 'subsample': 0.9236319050702, 'colsample_bytree': 0.6676577563556789}. Best is trial 52 with value: 0.7922134070306397.\n",
      "[I 2025-10-22 06:42:07,136] Trial 53 finished with value: 0.7912172045161346 and parameters: {'learning_rate': 0.03812556794230763, 'num_leaves': 41, 'max_depth': 8, 'min_child_samples': 9, 'subsample': 0.9214246815253011, 'colsample_bytree': 0.6514939721143472}. Best is trial 52 with value: 0.7922134070306397.\n",
      "[I 2025-10-22 06:42:09,418] Trial 54 finished with value: 0.792368541695605 and parameters: {'learning_rate': 0.02987652103139488, 'num_leaves': 43, 'max_depth': 9, 'min_child_samples': 13, 'subsample': 0.9427267069548426, 'colsample_bytree': 0.6149996370426882}. Best is trial 54 with value: 0.792368541695605.\n",
      "[I 2025-10-22 06:42:11,998] Trial 55 finished with value: 0.7920383873052813 and parameters: {'learning_rate': 0.027764827530474996, 'num_leaves': 43, 'max_depth': 9, 'min_child_samples': 6, 'subsample': 0.9473047668292376, 'colsample_bytree': 0.6208504779610814}. Best is trial 54 with value: 0.792368541695605.\n",
      "[I 2025-10-22 06:42:13,883] Trial 56 finished with value: 0.7919244423734387 and parameters: {'learning_rate': 0.03404951441693848, 'num_leaves': 37, 'max_depth': 9, 'min_child_samples': 9, 'subsample': 0.98232251888634, 'colsample_bytree': 0.6686532197409302}. Best is trial 54 with value: 0.792368541695605.\n",
      "[I 2025-10-22 06:42:16,498] Trial 57 finished with value: 0.7908831857006672 and parameters: {'learning_rate': 0.02932103587999459, 'num_leaves': 45, 'max_depth': 8, 'min_child_samples': 12, 'subsample': 0.9534172855645707, 'colsample_bytree': 0.6102255973745481}. Best is trial 54 with value: 0.792368541695605.\n",
      "[I 2025-10-22 06:42:19,519] Trial 58 finished with value: 0.791861641793282 and parameters: {'learning_rate': 0.02318909139260638, 'num_leaves': 38, 'max_depth': 10, 'min_child_samples': 13, 'subsample': 0.9306916464457271, 'colsample_bytree': 0.6505229055625974}. Best is trial 54 with value: 0.792368541695605.\n",
      "[I 2025-10-22 06:42:20,210] Trial 59 finished with value: 0.7889540947153969 and parameters: {'learning_rate': 0.19131694275860459, 'num_leaves': 42, 'max_depth': 8, 'min_child_samples': 30, 'subsample': 0.9758658069785181, 'colsample_bytree': 0.6732202648710026}. Best is trial 54 with value: 0.792368541695605.\n",
      "[I 2025-10-22 06:42:21,469] Trial 60 finished with value: 0.7911109040543961 and parameters: {'learning_rate': 0.05291313014289547, 'num_leaves': 31, 'max_depth': 9, 'min_child_samples': 14, 'subsample': 0.8590270790000212, 'colsample_bytree': 0.6128413424616315}. Best is trial 54 with value: 0.792368541695605.\n",
      "[I 2025-10-22 06:42:25,137] Trial 61 finished with value: 0.79096334461537 and parameters: {'learning_rate': 0.028504821532919878, 'num_leaves': 43, 'max_depth': 9, 'min_child_samples': 6, 'subsample': 0.9535609092860131, 'colsample_bytree': 0.6193086790857463}. Best is trial 54 with value: 0.792368541695605.\n",
      "[I 2025-10-22 06:42:28,254] Trial 62 finished with value: 0.7917357950199472 and parameters: {'learning_rate': 0.019540290723366827, 'num_leaves': 41, 'max_depth': 9, 'min_child_samples': 5, 'subsample': 0.8914857606936727, 'colsample_bytree': 0.6446679733158855}. Best is trial 54 with value: 0.792368541695605.\n",
      "[I 2025-10-22 06:42:32,160] Trial 63 finished with value: 0.7913215046477684 and parameters: {'learning_rate': 0.024301091882612893, 'num_leaves': 43, 'max_depth': 10, 'min_child_samples': 7, 'subsample': 0.94756829009838, 'colsample_bytree': 0.6617384185495224}. Best is trial 54 with value: 0.792368541695605.\n",
      "[I 2025-10-22 06:42:34,841] Trial 64 finished with value: 0.7907527545134706 and parameters: {'learning_rate': 0.03318135776293058, 'num_leaves': 45, 'max_depth': 8, 'min_child_samples': 11, 'subsample': 0.920894659435458, 'colsample_bytree': 0.6018545295952547}. Best is trial 54 with value: 0.792368541695605.\n",
      "[I 2025-10-22 06:42:36,873] Trial 65 finished with value: 0.7920418019334822 and parameters: {'learning_rate': 0.036391065149658015, 'num_leaves': 47, 'max_depth': 9, 'min_child_samples': 10, 'subsample': 0.980092104034633, 'colsample_bytree': 0.6974948564487644}. Best is trial 54 with value: 0.792368541695605.\n",
      "[I 2025-10-22 06:42:39,086] Trial 66 finished with value: 0.7916066347691241 and parameters: {'learning_rate': 0.0367790780141678, 'num_leaves': 47, 'max_depth': 9, 'min_child_samples': 8, 'subsample': 0.9778638054877822, 'colsample_bytree': 0.6956158560287826}. Best is trial 54 with value: 0.792368541695605.\n",
      "[I 2025-10-22 06:42:40,591] Trial 67 finished with value: 0.7914645453427985 and parameters: {'learning_rate': 0.04242930078203226, 'num_leaves': 40, 'max_depth': 7, 'min_child_samples': 12, 'subsample': 0.8133012865098792, 'colsample_bytree': 0.6384353423831781}. Best is trial 54 with value: 0.792368541695605.\n",
      "[I 2025-10-22 06:42:41,081] Trial 68 finished with value: 0.7897004437292942 and parameters: {'learning_rate': 0.28949242849145346, 'num_leaves': 34, 'max_depth': 10, 'min_child_samples': 10, 'subsample': 0.9316367520581302, 'colsample_bytree': 0.7156382100115002}. Best is trial 54 with value: 0.792368541695605.\n",
      "[I 2025-10-22 06:42:42,542] Trial 69 finished with value: 0.7915628522860706 and parameters: {'learning_rate': 0.05698524752149667, 'num_leaves': 46, 'max_depth': 8, 'min_child_samples': 10, 'subsample': 0.9900019036860697, 'colsample_bytree': 0.7279469726418781}. Best is trial 54 with value: 0.792368541695605.\n",
      "[I 2025-10-22 06:42:44,844] Trial 70 finished with value: 0.7923337225702607 and parameters: {'learning_rate': 0.02526666865299062, 'num_leaves': 49, 'max_depth': 9, 'min_child_samples': 14, 'subsample': 0.8998950654239868, 'colsample_bytree': 0.7010501934111254}. Best is trial 54 with value: 0.792368541695605.\n",
      "[I 2025-10-22 06:42:47,131] Trial 71 finished with value: 0.7913480535538718 and parameters: {'learning_rate': 0.024342240167786033, 'num_leaves': 49, 'max_depth': 9, 'min_child_samples': 14, 'subsample': 0.8783287713498268, 'colsample_bytree': 0.6993807316066581}. Best is trial 54 with value: 0.792368541695605.\n",
      "[I 2025-10-22 06:42:49,044] Trial 72 finished with value: 0.7897443841750986 and parameters: {'learning_rate': 0.029645813698545128, 'num_leaves': 44, 'max_depth': 9, 'min_child_samples': 13, 'subsample': 0.8958198423924831, 'colsample_bytree': 0.661490826325472}. Best is trial 54 with value: 0.792368541695605.\n",
      "[I 2025-10-22 06:42:51,638] Trial 73 finished with value: 0.7912905142759183 and parameters: {'learning_rate': 0.021894906118399834, 'num_leaves': 47, 'max_depth': 8, 'min_child_samples': 15, 'subsample': 0.8544347300227342, 'colsample_bytree': 0.6802540205738372}. Best is trial 54 with value: 0.792368541695605.\n",
      "[I 2025-10-22 06:42:55,774] Trial 74 finished with value: 0.7906433640222014 and parameters: {'learning_rate': 0.015147097427462976, 'num_leaves': 49, 'max_depth': 10, 'min_child_samples': 11, 'subsample': 0.9154367986042424, 'colsample_bytree': 0.6870747297862553}. Best is trial 54 with value: 0.792368541695605.\n",
      "[I 2025-10-22 06:42:57,364] Trial 75 finished with value: 0.7922762484294235 and parameters: {'learning_rate': 0.035086091850169164, 'num_leaves': 45, 'max_depth': 8, 'min_child_samples': 14, 'subsample': 0.968273839439611, 'colsample_bytree': 0.6551887809966327}. Best is trial 54 with value: 0.792368541695605.\n",
      "[I 2025-10-22 06:42:59,181] Trial 76 finished with value: 0.7907280826562552 and parameters: {'learning_rate': 0.03239987159615159, 'num_leaves': 45, 'max_depth': 7, 'min_child_samples': 15, 'subsample': 0.879882592683793, 'colsample_bytree': 0.6558610835918234}. Best is trial 54 with value: 0.792368541695605.\n",
      "[I 2025-10-22 06:43:01,059] Trial 77 finished with value: 0.7911927273162361 and parameters: {'learning_rate': 0.025466592915114772, 'num_leaves': 42, 'max_depth': 8, 'min_child_samples': 13, 'subsample': 0.959810429977464, 'colsample_bytree': 0.6348741058810488}. Best is trial 54 with value: 0.792368541695605.\n",
      "[I 2025-10-22 06:43:02,320] Trial 78 finished with value: 0.789580729776339 and parameters: {'learning_rate': 0.04499826797001578, 'num_leaves': 40, 'max_depth': 7, 'min_child_samples': 17, 'subsample': 0.9334360538034734, 'colsample_bytree': 0.6683300552606471}. Best is trial 54 with value: 0.792368541695605.\n",
      "[I 2025-10-22 06:43:03,431] Trial 79 finished with value: 0.7887172447996158 and parameters: {'learning_rate': 0.0507915719491616, 'num_leaves': 46, 'max_depth': 8, 'min_child_samples': 14, 'subsample': 0.8673320548411104, 'colsample_bytree': 0.9642425220967046}. Best is trial 54 with value: 0.792368541695605.\n",
      "[I 2025-10-22 06:43:05,835] Trial 80 finished with value: 0.7922880408418839 and parameters: {'learning_rate': 0.020781273657703256, 'num_leaves': 44, 'max_depth': 8, 'min_child_samples': 16, 'subsample': 0.9036174608743374, 'colsample_bytree': 0.6425204338183309}. Best is trial 54 with value: 0.792368541695605.\n",
      "[I 2025-10-22 06:43:09,307] Trial 81 finished with value: 0.7912009051895893 and parameters: {'learning_rate': 0.014649833026492178, 'num_leaves': 44, 'max_depth': 8, 'min_child_samples': 15, 'subsample': 0.9026416169663248, 'colsample_bytree': 0.6417019068907738}. Best is trial 54 with value: 0.792368541695605.\n",
      "[I 2025-10-22 06:43:11,936] Trial 82 finished with value: 0.7908343789501865 and parameters: {'learning_rate': 0.01811506471211051, 'num_leaves': 50, 'max_depth': 8, 'min_child_samples': 18, 'subsample': 0.9155848030885397, 'colsample_bytree': 0.7696864237994254}. Best is trial 54 with value: 0.792368541695605.\n",
      "[I 2025-10-22 06:43:14,143] Trial 83 finished with value: 0.7911029225663233 and parameters: {'learning_rate': 0.021542570224575303, 'num_leaves': 44, 'max_depth': 7, 'min_child_samples': 16, 'subsample': 0.9686563695083604, 'colsample_bytree': 0.6255728889807148}. Best is trial 54 with value: 0.792368541695605.\n",
      "[I 2025-10-22 06:43:15,794] Trial 84 finished with value: 0.7902307518171167 and parameters: {'learning_rate': 0.031111913741079664, 'num_leaves': 42, 'max_depth': 8, 'min_child_samples': 17, 'subsample': 0.7815147890382316, 'colsample_bytree': 0.6550956362700852}. Best is trial 54 with value: 0.792368541695605.\n",
      "[I 2025-10-22 06:43:16,293] Trial 85 finished with value: 0.7914489521670647 and parameters: {'learning_rate': 0.1439572958228936, 'num_leaves': 40, 'max_depth': 5, 'min_child_samples': 14, 'subsample': 0.9260187614402665, 'colsample_bytree': 0.6769609911779468}. Best is trial 54 with value: 0.792368541695605.\n",
      "[I 2025-10-22 06:43:19,125] Trial 86 finished with value: 0.7918261198372605 and parameters: {'learning_rate': 0.018455878533786406, 'num_leaves': 49, 'max_depth': 7, 'min_child_samples': 12, 'subsample': 0.9430141537834754, 'colsample_bytree': 0.666603894127213}. Best is trial 54 with value: 0.792368541695605.\n",
      "[I 2025-10-22 06:43:21,527] Trial 87 finished with value: 0.7911376149528943 and parameters: {'learning_rate': 0.02594336665032023, 'num_leaves': 45, 'max_depth': 9, 'min_child_samples': 16, 'subsample': 0.8367169869695872, 'colsample_bytree': 0.738766261579681}. Best is trial 54 with value: 0.792368541695605.\n",
      "[I 2025-10-22 06:43:22,560] Trial 88 finished with value: 0.7879179202289975 and parameters: {'learning_rate': 0.0395935883383974, 'num_leaves': 47, 'max_depth': 4, 'min_child_samples': 13, 'subsample': 0.8854599775200371, 'colsample_bytree': 0.6142527526584621}. Best is trial 54 with value: 0.792368541695605.\n",
      "[I 2025-10-22 06:43:23,982] Trial 89 finished with value: 0.7909482875161752 and parameters: {'learning_rate': 0.03457550211448256, 'num_leaves': 43, 'max_depth': 8, 'min_child_samples': 19, 'subsample': 0.8971294941972728, 'colsample_bytree': 0.7081948502859127}. Best is trial 54 with value: 0.792368541695605.\n",
      "[I 2025-10-22 06:43:25,017] Trial 90 finished with value: 0.7931061061465842 and parameters: {'learning_rate': 0.06883376056802966, 'num_leaves': 46, 'max_depth': 6, 'min_child_samples': 14, 'subsample': 0.9091917489759059, 'colsample_bytree': 0.6281804943827396}. Best is trial 90 with value: 0.7931061061465842.\n",
      "[I 2025-10-22 06:43:25,980] Trial 91 finished with value: 0.7902346921411908 and parameters: {'learning_rate': 0.0677772634148016, 'num_leaves': 46, 'max_depth': 6, 'min_child_samples': 14, 'subsample': 0.9069539624706546, 'colsample_bytree': 0.6297552008444686}. Best is trial 90 with value: 0.7931061061465842.\n",
      "[I 2025-10-22 06:43:26,594] Trial 92 finished with value: 0.7902690946914621 and parameters: {'learning_rate': 0.10914345255380689, 'num_leaves': 48, 'max_depth': 6, 'min_child_samples': 15, 'subsample': 0.9366479835803565, 'colsample_bytree': 0.6513858867085437}. Best is trial 90 with value: 0.7931061061465842.\n",
      "[I 2025-10-22 06:43:28,101] Trial 93 finished with value: 0.79096440433654 and parameters: {'learning_rate': 0.03020145098159823, 'num_leaves': 38, 'max_depth': 6, 'min_child_samples': 13, 'subsample': 0.9227481936042935, 'colsample_bytree': 0.6434616249567162}. Best is trial 90 with value: 0.7931061061465842.\n",
      "[I 2025-10-22 06:43:28,865] Trial 94 finished with value: 0.7913583249808719 and parameters: {'learning_rate': 0.08402546774344455, 'num_leaves': 41, 'max_depth': 7, 'min_child_samples': 12, 'subsample': 0.8684777016133948, 'colsample_bytree': 0.6005839275320088}. Best is trial 90 with value: 0.7931061061465842.\n",
      "[I 2025-10-22 06:43:30,910] Trial 95 finished with value: 0.7904207924404044 and parameters: {'learning_rate': 0.022952270157443534, 'num_leaves': 45, 'max_depth': 5, 'min_child_samples': 16, 'subsample': 0.9561918703346796, 'colsample_bytree': 0.6197609267691321}. Best is trial 90 with value: 0.7931061061465842.\n",
      "[I 2025-10-22 06:43:31,746] Trial 96 finished with value: 0.7926300793212636 and parameters: {'learning_rate': 0.06458710441412525, 'num_leaves': 25, 'max_depth': 8, 'min_child_samples': 11, 'subsample': 0.8459919306972206, 'colsample_bytree': 0.6296503637895912}. Best is trial 90 with value: 0.7931061061465842.\n",
      "[I 2025-10-22 06:43:32,439] Trial 97 finished with value: 0.7909381204796113 and parameters: {'learning_rate': 0.0739807172606027, 'num_leaves': 27, 'max_depth': 8, 'min_child_samples': 11, 'subsample': 0.8445833186381428, 'colsample_bytree': 0.6299777676052906}. Best is trial 90 with value: 0.7931061061465842.\n",
      "[I 2025-10-22 06:43:33,135] Trial 98 finished with value: 0.7906958928341569 and parameters: {'learning_rate': 0.06569911910538086, 'num_leaves': 23, 'max_depth': 7, 'min_child_samples': 15, 'subsample': 0.9680432869360421, 'colsample_bytree': 0.6127495493378727}. Best is trial 90 with value: 0.7931061061465842.\n",
      "[I 2025-10-22 06:43:34,012] Trial 99 finished with value: 0.7901048636691509 and parameters: {'learning_rate': 0.059504965910816526, 'num_leaves': 26, 'max_depth': 9, 'min_child_samples': 12, 'subsample': 0.9112449452866352, 'colsample_bytree': 0.6381057296482373}. Best is trial 90 with value: 0.7931061061465842.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study complete!\n",
      "Total number of trials in study: 100\n",
      "\n",
      "Best trial:\n",
      "  Value (Mean F1): 0.7931\n",
      "  Best Params: \n",
      "{'learning_rate': 0.06883376056802966, 'num_leaves': 46, 'max_depth': 6, 'min_child_samples': 14, 'subsample': 0.9091917489759059, 'colsample_bytree': 0.6281804943827396}\n",
      "\n",
      "Successfully stored parameters in 'best_params' variable.\n",
      "\n",
      "Optuna-Tuned LightGBM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0   0.858294  0.837334  0.847685      2416\n",
      "         1.0   0.784421  0.810658  0.797324      1764\n",
      "\n",
      "    accuracy                       0.826077      4180\n",
      "   macro avg   0.821358  0.823996  0.822504      4180\n",
      "weighted avg   0.827119  0.826077  0.826432      4180\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import optuna\n",
    "import numpy as np\n",
    "import lightgbm as lgbm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "import sklearn\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    train_df = pandas.read_csv('./data/salary.train.processed.csv', index_col='id')\n",
    "    test_df = pandas.read_csv('./data/salary.test.processed.csv', index_col='id')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Could not find the processed CSV files.\")\n",
    "    raise\n",
    "\n",
    "X_train = train_df.drop('label', axis=1)\n",
    "y_train = train_df['label']\n",
    "X_test = test_df.drop('label', axis=1)\n",
    "y_test = test_df['label']\n",
    "# --- End of Data Loading ---\n",
    "\n",
    "# --- Step 1: Define the Objective Function ---\n",
    "def objective_lgbm(trial):\n",
    "    \"\"\"\n",
    "    Objective function with manual Cross-Validation and Early Stopping.\n",
    "    \"\"\"\n",
    "    # 1. Define the search space for LightGBM\n",
    "    param = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'random_state': 42,\n",
    "        'n_estimators': 1000,\n",
    "        'verbose': -1,\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 10, 50),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 30),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0)\n",
    "    }\n",
    "    \n",
    "    # 2. Set up 3-Fold Cross-Validation\n",
    "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "\n",
    "    # 3. Manually run the CV loop\n",
    "    for train_idx, val_idx in skf.split(X_train, y_train):\n",
    "        X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        \n",
    "        model = lgbm.LGBMClassifier(**param)\n",
    "        \n",
    "        # Train with early stopping\n",
    "        model.fit(\n",
    "            X_train_fold, \n",
    "            y_train_fold,\n",
    "            eval_set=[(X_val_fold, y_val_fold)],\n",
    "            eval_metric='logloss',\n",
    "            callbacks=[lgbm.early_stopping(50, verbose=False)]\n",
    "        )\n",
    "        \n",
    "        preds = model.predict(X_val_fold)\n",
    "        f1 = sklearn.metrics.f1_score(y_val_fold, preds, average='binary')\n",
    "        scores.append(f1)\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "# --- Step 2: Create and Run the Study ---\n",
    "study_name = \"lgbm_salary_tuning\"  # ‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå (‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á storage)\n",
    "\n",
    "print(f\"Starting Optuna study: '{study_name}'\")\n",
    "print(f\"Running 200 new trials...\")\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á Study ‡πÉ‡∏´‡∏°‡πà‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ storage\n",
    "study_lgbm = optuna.create_study(\n",
    "    study_name=study_name,\n",
    "    direction='maximize'\n",
    ")\n",
    "\n",
    "# ‡∏£‡∏±‡∏ô optimization\n",
    "study_lgbm.optimize(objective_lgbm, n_trials=100)\n",
    "print(\"Study complete!\")\n",
    "print(f\"Total number of trials in study: {len(study_lgbm.trials)}\")\n",
    "\n",
    "# --- Step 3: Get Best Params ---\n",
    "print(\"\\nBest trial:\")\n",
    "print(f\"  Value (Mean F1): {study_lgbm.best_value:.4f}\")\n",
    "print(\"  Best Params: \")\n",
    "print(study_lgbm.best_params)\n",
    "\n",
    "# ‡πÄ‡∏Å‡πá‡∏ö‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\n",
    "best_params = study_lgbm.best_params\n",
    "print(f\"\\nSuccessfully stored parameters in 'best_params' variable.\")\n",
    "\n",
    "# --- Step 4: Train the FINAL Model ---\n",
    "X_train_final, X_val_final, y_train_final, y_val_final = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "final_lgbm = lgbm.LGBMClassifier(\n",
    "    random_state=42,\n",
    "    n_estimators=1000,\n",
    "    **best_params\n",
    ")\n",
    "\n",
    "final_lgbm.fit(\n",
    "    X_train_final, \n",
    "    y_train_final,\n",
    "    eval_set=[(X_val_final, y_val_final)],\n",
    "    eval_metric='logloss',\n",
    "    callbacks=[lgbm.early_stopping(50, verbose=False)]\n",
    ")\n",
    "\n",
    "# --- Step 5: Evaluate the Optuna-Tuned Model on the TEST set ---\n",
    "y_pred_optuna_lgbm = final_lgbm.predict(X_test)\n",
    "\n",
    "print(\"\\nOptuna-Tuned LightGBM Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_optuna_lgbm, digits=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb534a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.06883376056802966, 'num_leaves': 46, 'max_depth': 6, 'min_child_samples': 14, 'subsample': 0.9091917489759059, 'colsample_bytree': 0.6281804943827396}\n"
     ]
    }
   ],
   "source": [
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f9ee96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aadb4ae1",
   "metadata": {},
   "source": [
    "### ClassWeight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24b9b648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated scale_pos_weight: 1.3882\n",
      "\n",
      "Training LightGBM model with class weighting...\n",
      "Model training complete.\n",
      "\n",
      "Evaluating model on test data...\n",
      "\n",
      "LightGBM (Tuned + Weighted) Report:\n",
      "              precision    recall  f1-score      support\n",
      "0.0            0.887134  0.790563  0.836069  2416.000000\n",
      "1.0            0.750370  0.862245  0.802427  1764.000000\n",
      "accuracy       0.820813  0.820813  0.820813     0.820813\n",
      "macro avg      0.818752  0.826404  0.819248  4180.000000\n",
      "weighted avg   0.829418  0.820813  0.821872  4180.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import lightgbm as lgb\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# --- 1. Load Data (to calculate the weight) ---\n",
    "try:\n",
    "    data_train_full = pandas.read_csv('./data/salary.train.processed.csv').set_index('id')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå salary.train.processed.csv ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö path\")\n",
    "    # exit() \n",
    "\n",
    "X_full = data_train_full.drop(['label'], axis='columns')\n",
    "y_full = data_train_full['label']\n",
    "\n",
    "# --- 2. Calculate scale_pos_weight ---\n",
    "# This is the same logic as your XGBoost code\n",
    "scale_pos_weight = len(y_full[y_full == 0]) / len(y_full[y_full == 1])\n",
    "print(f\"Calculated scale_pos_weight: {scale_pos_weight:.4f}\")\n",
    "\n",
    "# --- 3. Define Your Best Parameters ---\n",
    "# These are the params you provided in your prompt\n",
    "best_lgbm_params = best_params\n",
    "\n",
    "# --- 4. Create and Train the Model ---\n",
    "print(\"\\nTraining LightGBM model with class weighting...\")\n",
    "\n",
    "lgbm_model_final = lgb.LGBMClassifier(\n",
    "    **best_lgbm_params,       # Apply all your tuned parameters\n",
    "    scale_pos_weight=scale_pos_weight, # üëà Here is the class weight\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train the model on the full training dataset\n",
    "lgbm_model_final.fit(X_full, y_full)\n",
    "\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# --- 5. Evaluate on Test Data (Recommended) ---\n",
    "print(\"\\nEvaluating model on test data...\")\n",
    "\n",
    "data_test_lgbm = pandas.read_csv('./data/salary.test.processed.csv').set_index('id')\n",
    "data_test_lgbm['prediction'] = lgbm_model_final.predict(data_test_lgbm.drop(['label'], axis='columns'))\n",
    "\n",
    "# Print the report\n",
    "report_scores_lgbm = sklearn.metrics.classification_report(\n",
    "    y_true=data_test_lgbm['label'],\n",
    "    y_pred=data_test_lgbm['prediction'],\n",
    "    digits=6,\n",
    "    output_dict=True\n",
    ")\n",
    "df_score_lgbm = pandas.DataFrame(report_scores_lgbm).transpose()\n",
    "\n",
    "print(\"\\nLightGBM (Tuned + Weighted) Report:\")\n",
    "print(df_score_lgbm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5be28d7",
   "metadata": {},
   "source": [
    "### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9335de8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training data shape: (16720, 56)\n",
      "Original label distribution:\n",
      "label\n",
      "0.0    9719\n",
      "1.0    7001\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Applying SMOTE to the training data...\n",
      "New resampled training data shape: (19438, 56)\n",
      "New resampled label distribution:\n",
      "label\n",
      "1.0    9719\n",
      "0.0    9719\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Training final LGBM model on SMOTEd data...\n",
      "Model training complete.\n",
      "\n",
      "Evaluating model on *original* test data...\n",
      "\n",
      "LightGBM (Tuned + SMOTE) Report:\n",
      "              precision    recall  f1-score      support\n",
      "0.0            0.876792  0.810017  0.842083  2416.000000\n",
      "1.0            0.764374  0.844104  0.802263  1764.000000\n",
      "accuracy       0.824402  0.824402  0.824402     0.824402\n",
      "macro avg      0.820583  0.827060  0.822173  4180.000000\n",
      "weighted avg   0.829350  0.824402  0.825278  4180.000000\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import joblib\n",
    "import pandas\n",
    "import lightgbm as lgb\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE  # üëà 1. Import SMOTE\n",
    "\n",
    "# --- 1. Load Data (Needed for training) ---\n",
    "try:\n",
    "    data_train_full = pandas.read_csv('./data/salary.train.processed.csv').set_index('id')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå salary.train.processed.csv ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö path\")\n",
    "    # exit() \n",
    "\n",
    "X_full = data_train_full.drop(['label'], axis='columns')\n",
    "y_full = data_train_full['label']\n",
    "\n",
    "print(f\"Original training data shape: {X_full.shape}\")\n",
    "print(f\"Original label distribution:\\n{y_full.value_counts()}\")\n",
    "\n",
    "# --- 2. Define Your Best Parameters ---\n",
    "# These are the params you provided\n",
    "best_lgbm_params = best_params\n",
    "\n",
    "# --- 3. Apply SMOTE to the Training Data ---\n",
    "print(\"\\nApplying SMOTE to the training data...\")\n",
    "smote = SMOTE(random_state=42, n_jobs=-1)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_full, y_full)\n",
    "\n",
    "print(f\"New resampled training data shape: {X_resampled.shape}\")\n",
    "print(f\"New resampled label distribution:\\n{y_resampled.value_counts()}\")\n",
    "\n",
    "# --- 4. Create and Train the Final Model (with NO class_weight) ---\n",
    "print(\"\\nTraining final LGBM model on SMOTEd data...\")\n",
    "\n",
    "lgbm_model_final = lgb.LGBMClassifier(\n",
    "    **best_lgbm_params,\n",
    "    # ‚ö†Ô∏è NO 'scale_pos_weight' or 'class_weight' here\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 5. Train the model on the NEW resampled data\n",
    "lgbm_model_final.fit(X_resampled, y_resampled)\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# --- 6. Evaluate on ORIGINAL Test Data ---\n",
    "print(\"\\nEvaluating model on *original* test data...\")\n",
    "\n",
    "data_test_lgbm = pandas.read_csv('./data/salary.test.processed.csv').set_index('id')\n",
    "\n",
    "# IMPORTANT: Do NOT apply SMOTE to the test data.\n",
    "X_test = data_test_lgbm.drop(['label'], axis='columns')\n",
    "y_test = data_test_lgbm['label']\n",
    "\n",
    "data_test_lgbm['prediction'] = lgbm_model_final.predict(X_test)\n",
    "\n",
    "# Print the report\n",
    "report_scores_lgbm = sklearn.metrics.classification_report(\n",
    "    y_true=y_test,\n",
    "    y_pred=data_test_lgbm['prediction'],\n",
    "    digits=6,\n",
    "    output_dict=True\n",
    ")\n",
    "df_score_lgbm = pandas.DataFrame(report_scores_lgbm).transpose()\n",
    "\n",
    "print(\"\\nLightGBM (Tuned + SMOTE) Report:\")\n",
    "print(df_score_lgbm)\n",
    "joblib.dump(lgbm_model_final, './model/lgbm/lgbm_model_final_smote.pkl')\n",
    "with open('./model/lgbm/lgbm_config.json','w')as f:\n",
    "    json.dump(\n",
    "        obj=lgbm_model_final.get_params(),\n",
    "        fp=f,\n",
    "        indent = 4\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4323c0cb",
   "metadata": {},
   "source": [
    "### SMOTETomek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3fe56d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 3. Testing LightGBM with SMOTETomek ---\n",
      "Applying SMOTETomek...\n",
      "New resampled label distribution:\n",
      "label\n",
      "1.0    9220\n",
      "0.0    9220\n",
      "Name: count, dtype: int64\n",
      "Model training complete.\n",
      "\n",
      "LightGBM (Tuned + SMOTETomek) Report:\n",
      "              precision    recall  f1-score      support\n",
      "0.0            0.876118  0.810844  0.842218  2416.000000\n",
      "1.0            0.764918  0.842971  0.802050  1764.000000\n",
      "accuracy       0.824402  0.824402  0.824402     0.824402\n",
      "macro avg      0.820518  0.826907  0.822134  4180.000000\n",
      "weighted avg   0.829190  0.824402  0.825267  4180.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import lightgbm as lgb\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "print(\"\\n--- 3. Testing LightGBM with SMOTETomek ---\")\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    data_train_full = pandas.read_csv('./data/salary.train.processed.csv').set_index('id')\n",
    "    data_test_lgbm = pandas.read_csv('./data/salary.test.processed.csv').set_index('id')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå salary.train.processed.csv ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö path\")\n",
    "    # exit() \n",
    "\n",
    "X_full = data_train_full.drop(['label'], axis='columns')\n",
    "y_full = data_train_full['label']\n",
    "X_test = data_test_lgbm.drop(['label'], axis='columns')\n",
    "y_test = data_test_lgbm['label']\n",
    "\n",
    "# --- Apply SMOTETomek ---\n",
    "print(\"Applying SMOTETomek...\")\n",
    "smt = SMOTETomek(random_state=42, n_jobs=-1)\n",
    "X_resampled, y_resampled = smt.fit_resample(X_full, y_full)\n",
    "print(f\"New resampled label distribution:\\n{y_resampled.value_counts()}\")\n",
    "\n",
    "# --- Define Parameters ---\n",
    "best_lgbm_params = best_params\n",
    "\n",
    "# --- Create and Train Model ---\n",
    "lgbm_model = lgb.LGBMClassifier(\n",
    "    **best_lgbm_params,\n",
    "    # ‚ö†Ô∏è NO 'scale_pos_weight' or 'class_weight'\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "lgbm_model.fit(X_resampled, y_resampled) # Train on SMOTETomek data\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# --- Evaluate ---\n",
    "y_pred = lgbm_model.predict(X_test)\n",
    "report = classification_report(y_test, y_pred, digits=6, output_dict=True)\n",
    "df_report = pandas.DataFrame(report).transpose()\n",
    "\n",
    "print(\"\\nLightGBM (Tuned + SMOTETomek) Report:\")\n",
    "print(df_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddda6884",
   "metadata": {},
   "source": [
    "### ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "03306bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 4. Testing LightGBM with ADASYN ---\n",
      "Applying ADASYN...\n",
      "New resampled label distribution:\n",
      "label\n",
      "0.0    9719\n",
      "1.0    9698\n",
      "Name: count, dtype: int64\n",
      "Model training complete.\n",
      "\n",
      "LightGBM (Tuned + ADASYN) Report:\n",
      "              precision    recall  f1-score      support\n",
      "0.0            0.886070  0.795116  0.838133  2416.000000\n",
      "1.0            0.753976  0.859977  0.803496  1764.000000\n",
      "accuracy       0.822488  0.822488  0.822488     0.822488\n",
      "macro avg      0.820023  0.827547  0.820814  4180.000000\n",
      "weighted avg   0.830325  0.822488  0.823516  4180.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import lightgbm as lgb\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "print(\"\\n--- 4. Testing LightGBM with ADASYN ---\")\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    data_train_full = pandas.read_csv('./data/salary.train.processed.csv').set_index('id')\n",
    "    data_test_lgbm = pandas.read_csv('./data/salary.test.processed.csv').set_index('id')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå salary.train.processed.csv ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö path\")\n",
    "    # exit() \n",
    "\n",
    "X_full = data_train_full.drop(['label'], axis='columns')\n",
    "y_full = data_train_full['label']\n",
    "X_test = data_test_lgbm.drop(['label'], axis='columns')\n",
    "y_test = data_test_lgbm['label']\n",
    "\n",
    "# --- Apply ADASYN ---\n",
    "print(\"Applying ADASYN...\")\n",
    "ada = ADASYN(random_state=42, n_jobs=-1)\n",
    "X_resampled, y_resampled = ada.fit_resample(X_full, y_full)\n",
    "print(f\"New resampled label distribution:\\n{y_resampled.value_counts()}\")\n",
    "\n",
    "# --- Define Parameters ---\n",
    "best_lgbm_params = best_params\n",
    "\n",
    "# --- Create and Train Model ---\n",
    "lgbm_model = lgb.LGBMClassifier(\n",
    "    **best_lgbm_params,\n",
    "    # ‚ö†Ô∏è NO 'scale_pos_weight' or 'class_weight'\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "lgbm_model.fit(X_resampled, y_resampled) # Train on ADASYN data\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# --- Evaluate ---\n",
    "y_pred = lgbm_model.predict(X_test)\n",
    "report = classification_report(y_test, y_pred, digits=6, output_dict=True)\n",
    "df_report = pandas.DataFrame(report).transpose()\n",
    "\n",
    "print(\"\\nLightGBM (Tuned + ADASYN) Report:\")\n",
    "print(df_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
