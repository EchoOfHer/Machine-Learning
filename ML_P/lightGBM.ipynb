{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4816981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "data_train = pd.read_csv('./data/salary.train.processed.csv').set_index('id')\n",
    "data_test = pd.read_csv('./data/salary.test.processed.csv').set_index('id')\n",
    "# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î target\n",
    "target = 'label'\n",
    "\n",
    "# ‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ù‡∏∂‡∏Å‡πÅ‡∏•‡∏∞‡∏ó‡∏î‡∏™‡∏≠‡∏ö\n",
    "X_train = data_train.drop(columns=[target])\n",
    "y_train = data_train[target]\n",
    "X_test = data_test.drop(columns=[target])\n",
    "y_test = data_test[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "492eb3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in c:\\users\\natth\\anaconda3\\lib\\site-packages (4.5.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in c:\\users\\natth\\anaconda3\\lib\\site-packages (from optuna) (1.13.3)\n",
      "Requirement already satisfied: colorlog in c:\\users\\natth\\anaconda3\\lib\\site-packages (from optuna) (6.10.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\natth\\anaconda3\\lib\\site-packages (from optuna) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\natth\\anaconda3\\lib\\site-packages (from optuna) (24.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in c:\\users\\natth\\anaconda3\\lib\\site-packages (from optuna) (2.0.34)\n",
      "Requirement already satisfied: tqdm in c:\\users\\natth\\anaconda3\\lib\\site-packages (from optuna) (4.66.5)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\natth\\anaconda3\\lib\\site-packages (from optuna) (6.0.1)\n",
      "Requirement already satisfied: Mako in c:\\users\\natth\\anaconda3\\lib\\site-packages (from alembic>=1.5.0->optuna) (1.2.3)\n",
      "Requirement already satisfied: typing-extensions>=4 in c:\\users\\natth\\anaconda3\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.11.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\natth\\anaconda3\\lib\\site-packages (from sqlalchemy>=1.4.2->optuna) (3.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\natth\\anaconda3\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\natth\\anaconda3\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a260cad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in c:\\users\\natth\\anaconda3\\lib\\site-packages (4.6.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\natth\\anaconda3\\lib\\site-packages (from lightgbm) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\natth\\anaconda3\\lib\\site-packages (from lightgbm) (1.13.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lightgbm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24ac8e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-21 19:23:41,377] A new study created in memory with name: no-name-39577086-16a2-4ab3-88e1-5ba6949a13dd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Optuna study for LightGBM... (This should be fast!)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"c:\\Users\\natth\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\natth\\anaconda3\\Lib\\subprocess.py\", line 548, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\natth\\anaconda3\\Lib\\subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"c:\\Users\\natth\\anaconda3\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[I 2025-10-21 19:23:44,555] Trial 0 finished with value: 0.7911721121847751 and parameters: {'learning_rate': 0.011720722404824632, 'num_leaves': 17, 'max_depth': 5, 'min_child_samples': 20, 'subsample': 0.6562270463965464, 'colsample_bytree': 0.8444479593206774}. Best is trial 0 with value: 0.7911721121847751.\n",
      "[I 2025-10-21 19:23:47,160] Trial 1 finished with value: 0.7908259272644496 and parameters: {'learning_rate': 0.02469544449349408, 'num_leaves': 14, 'max_depth': 3, 'min_child_samples': 27, 'subsample': 0.668776900596309, 'colsample_bytree': 0.6114038683019589}. Best is trial 0 with value: 0.7911721121847751.\n",
      "[I 2025-10-21 19:23:48,054] Trial 2 finished with value: 0.7923969227043717 and parameters: {'learning_rate': 0.08271802408573575, 'num_leaves': 29, 'max_depth': 6, 'min_child_samples': 21, 'subsample': 0.6263119856045365, 'colsample_bytree': 0.9476508429823108}. Best is trial 2 with value: 0.7923969227043717.\n",
      "[I 2025-10-21 19:23:49,577] Trial 3 finished with value: 0.7920776893489769 and parameters: {'learning_rate': 0.026605572816526934, 'num_leaves': 17, 'max_depth': 4, 'min_child_samples': 21, 'subsample': 0.7028029501553634, 'colsample_bytree': 0.8258255295366062}. Best is trial 2 with value: 0.7923969227043717.\n",
      "[I 2025-10-21 19:23:51,093] Trial 4 finished with value: 0.7909066665168792 and parameters: {'learning_rate': 0.06873178469998123, 'num_leaves': 27, 'max_depth': 3, 'min_child_samples': 25, 'subsample': 0.7518968340643527, 'colsample_bytree': 0.8931625165906402}. Best is trial 2 with value: 0.7923969227043717.\n",
      "[I 2025-10-21 19:23:52,215] Trial 5 finished with value: 0.7910855241878029 and parameters: {'learning_rate': 0.0474221368248353, 'num_leaves': 18, 'max_depth': 6, 'min_child_samples': 21, 'subsample': 0.8803962893404553, 'colsample_bytree': 0.8031372327274339}. Best is trial 2 with value: 0.7923969227043717.\n",
      "[I 2025-10-21 19:23:52,785] Trial 6 finished with value: 0.7896826955407924 and parameters: {'learning_rate': 0.1960830806370154, 'num_leaves': 39, 'max_depth': 3, 'min_child_samples': 6, 'subsample': 0.9039543001031771, 'colsample_bytree': 0.6418873344898731}. Best is trial 2 with value: 0.7923969227043717.\n",
      "[I 2025-10-21 19:23:53,355] Trial 7 finished with value: 0.791044466141218 and parameters: {'learning_rate': 0.15915992803060844, 'num_leaves': 40, 'max_depth': 6, 'min_child_samples': 11, 'subsample': 0.6497764528365909, 'colsample_bytree': 0.9320692705770649}. Best is trial 2 with value: 0.7923969227043717.\n",
      "[I 2025-10-21 19:23:56,415] Trial 8 finished with value: 0.7925661589919363 and parameters: {'learning_rate': 0.018524334436729354, 'num_leaves': 39, 'max_depth': 5, 'min_child_samples': 17, 'subsample': 0.9458561593976703, 'colsample_bytree': 0.6151347394613982}. Best is trial 8 with value: 0.7925661589919363.\n",
      "[I 2025-10-21 19:23:58,831] Trial 9 finished with value: 0.7918890366646062 and parameters: {'learning_rate': 0.018702647660630345, 'num_leaves': 47, 'max_depth': 6, 'min_child_samples': 27, 'subsample': 0.8965055541483635, 'colsample_bytree': 0.8914726267823276}. Best is trial 8 with value: 0.7925661589919363.\n",
      "[I 2025-10-21 19:24:02,813] Trial 10 finished with value: 0.790323871468717 and parameters: {'learning_rate': 0.014274926231249927, 'num_leaves': 50, 'max_depth': 9, 'min_child_samples': 14, 'subsample': 0.9962561990830767, 'colsample_bytree': 0.7039688620223792}. Best is trial 8 with value: 0.7925661589919363.\n",
      "[I 2025-10-21 19:24:03,638] Trial 11 finished with value: 0.7918361496594429 and parameters: {'learning_rate': 0.08346930055973821, 'num_leaves': 29, 'max_depth': 8, 'min_child_samples': 16, 'subsample': 0.8241553766937864, 'colsample_bytree': 0.9967575963817141}. Best is trial 8 with value: 0.7925661589919363.\n",
      "[I 2025-10-21 19:24:04,280] Trial 12 finished with value: 0.7924419824518555 and parameters: {'learning_rate': 0.10780138912053223, 'num_leaves': 37, 'max_depth': 8, 'min_child_samples': 12, 'subsample': 0.9568642291884656, 'colsample_bytree': 0.7323258096646266}. Best is trial 8 with value: 0.7925661589919363.\n",
      "[I 2025-10-21 19:24:06,066] Trial 13 finished with value: 0.7917613217801175 and parameters: {'learning_rate': 0.04194720749359943, 'num_leaves': 38, 'max_depth': 8, 'min_child_samples': 10, 'subsample': 0.9974087212117547, 'colsample_bytree': 0.7263581843111363}. Best is trial 8 with value: 0.7925661589919363.\n",
      "[I 2025-10-21 19:24:06,843] Trial 14 finished with value: 0.7914124291209194 and parameters: {'learning_rate': 0.11633710697678329, 'num_leaves': 35, 'max_depth': 10, 'min_child_samples': 12, 'subsample': 0.9551823366466744, 'colsample_bytree': 0.7316533488147572}. Best is trial 8 with value: 0.7925661589919363.\n",
      "[I 2025-10-21 19:24:07,741] Trial 15 finished with value: 0.7905336529446169 and parameters: {'learning_rate': 0.2349067824283496, 'num_leaves': 45, 'max_depth': 8, 'min_child_samples': 6, 'subsample': 0.8270898157548774, 'colsample_bytree': 0.669203664666245}. Best is trial 8 with value: 0.7925661589919363.\n",
      "[I 2025-10-21 19:24:08,928] Trial 16 finished with value: 0.7912394670306732 and parameters: {'learning_rate': 0.03728378073502418, 'num_leaves': 34, 'max_depth': 7, 'min_child_samples': 16, 'subsample': 0.9410206339961981, 'colsample_bytree': 0.7698337391104911}. Best is trial 8 with value: 0.7925661589919363.\n",
      "[I 2025-10-21 19:24:09,440] Trial 17 finished with value: 0.7918692810000344 and parameters: {'learning_rate': 0.13513541820881927, 'num_leaves': 22, 'max_depth': 5, 'min_child_samples': 9, 'subsample': 0.7804430401470944, 'colsample_bytree': 0.6764681964093772}. Best is trial 8 with value: 0.7925661589919363.\n",
      "[I 2025-10-21 19:24:10,024] Trial 18 finished with value: 0.7910546273205193 and parameters: {'learning_rate': 0.29201920981836016, 'num_leaves': 43, 'max_depth': 10, 'min_child_samples': 18, 'subsample': 0.8627402917018696, 'colsample_bytree': 0.6065215396303805}. Best is trial 8 with value: 0.7925661589919363.\n",
      "[I 2025-10-21 19:24:13,889] Trial 19 finished with value: 0.7924321329223044 and parameters: {'learning_rate': 0.010234001622324579, 'num_leaves': 34, 'max_depth': 7, 'min_child_samples': 30, 'subsample': 0.9393985637840327, 'colsample_bytree': 0.7655024628140512}. Best is trial 8 with value: 0.7925661589919363.\n",
      "[I 2025-10-21 19:24:15,235] Trial 20 finished with value: 0.7916297280764867 and parameters: {'learning_rate': 0.03177754891634066, 'num_leaves': 24, 'max_depth': 5, 'min_child_samples': 14, 'subsample': 0.9622639803336615, 'colsample_bytree': 0.648405790036228}. Best is trial 8 with value: 0.7925661589919363.\n",
      "[I 2025-10-21 19:24:18,351] Trial 21 finished with value: 0.7931476012341306 and parameters: {'learning_rate': 0.010218293649179498, 'num_leaves': 34, 'max_depth': 7, 'min_child_samples': 30, 'subsample': 0.9263071607029089, 'colsample_bytree': 0.7590492971341184}. Best is trial 21 with value: 0.7931476012341306.\n",
      "[I 2025-10-21 19:24:20,089] Trial 22 finished with value: 0.7919881715268481 and parameters: {'learning_rate': 0.01819866662176293, 'num_leaves': 33, 'max_depth': 7, 'min_child_samples': 18, 'subsample': 0.9155816939246495, 'colsample_bytree': 0.7677240677967152}. Best is trial 21 with value: 0.7931476012341306.\n",
      "[I 2025-10-21 19:24:22,668] Trial 23 finished with value: 0.7920526360014363 and parameters: {'learning_rate': 0.0162306985659902, 'num_leaves': 42, 'max_depth': 9, 'min_child_samples': 24, 'subsample': 0.8544402213894596, 'colsample_bytree': 0.7054026215710394}. Best is trial 21 with value: 0.7931476012341306.\n",
      "[I 2025-10-21 19:24:25,058] Trial 24 finished with value: 0.7906824588237553 and parameters: {'learning_rate': 0.013220546394665076, 'num_leaves': 37, 'max_depth': 4, 'min_child_samples': 8, 'subsample': 0.972873277458405, 'colsample_bytree': 0.743225235596354}. Best is trial 21 with value: 0.7931476012341306.\n",
      "[I 2025-10-21 19:24:25,890] Trial 25 finished with value: 0.7898428506202345 and parameters: {'learning_rate': 0.05914497108457088, 'num_leaves': 31, 'max_depth': 9, 'min_child_samples': 13, 'subsample': 0.9238998545935588, 'colsample_bytree': 0.8491196024544588}. Best is trial 21 with value: 0.7931476012341306.\n",
      "[I 2025-10-21 19:24:27,391] Trial 26 finished with value: 0.7900836551391034 and parameters: {'learning_rate': 0.02381716543805778, 'num_leaves': 42, 'max_depth': 8, 'min_child_samples': 30, 'subsample': 0.9760729996222539, 'colsample_bytree': 0.8010825737357637}. Best is trial 21 with value: 0.7931476012341306.\n",
      "[I 2025-10-21 19:24:28,219] Trial 27 finished with value: 0.789161372602428 and parameters: {'learning_rate': 0.09821069870197109, 'num_leaves': 48, 'max_depth': 7, 'min_child_samples': 16, 'subsample': 0.8715510059881048, 'colsample_bytree': 0.6955141047492751}. Best is trial 21 with value: 0.7931476012341306.\n",
      "[I 2025-10-21 19:24:30,887] Trial 28 finished with value: 0.7900663902915377 and parameters: {'learning_rate': 0.010385250920618126, 'num_leaves': 26, 'max_depth': 4, 'min_child_samples': 23, 'subsample': 0.8380658929771811, 'colsample_bytree': 0.6487242934062888}. Best is trial 21 with value: 0.7931476012341306.\n",
      "[I 2025-10-21 19:24:33,233] Trial 29 finished with value: 0.792108440692559 and parameters: {'learning_rate': 0.01292960365564472, 'num_leaves': 37, 'max_depth': 5, 'min_child_samples': 19, 'subsample': 0.7744423186824638, 'colsample_bytree': 0.8375735838121566}. Best is trial 21 with value: 0.7931476012341306.\n",
      "[I 2025-10-21 19:24:35,067] Trial 30 finished with value: 0.7919857539969302 and parameters: {'learning_rate': 0.019124943872018456, 'num_leaves': 31, 'max_depth': 7, 'min_child_samples': 14, 'subsample': 0.9290413705061219, 'colsample_bytree': 0.6268506701993082}. Best is trial 21 with value: 0.7931476012341306.\n",
      "[I 2025-10-21 19:24:38,300] Trial 31 finished with value: 0.7929491438535171 and parameters: {'learning_rate': 0.010233933624441737, 'num_leaves': 35, 'max_depth': 7, 'min_child_samples': 30, 'subsample': 0.9434253619329627, 'colsample_bytree': 0.7725797403771691}. Best is trial 21 with value: 0.7931476012341306.\n",
      "[I 2025-10-21 19:24:41,627] Trial 32 finished with value: 0.7914759027921416 and parameters: {'learning_rate': 0.011189410122097997, 'num_leaves': 36, 'max_depth': 8, 'min_child_samples': 27, 'subsample': 0.8937130181121562, 'colsample_bytree': 0.7485533932576603}. Best is trial 21 with value: 0.7931476012341306.\n",
      "[I 2025-10-21 19:24:44,506] Trial 33 finished with value: 0.7916968570300266 and parameters: {'learning_rate': 0.01516225294838601, 'num_leaves': 32, 'max_depth': 6, 'min_child_samples': 28, 'subsample': 0.9500818013460126, 'colsample_bytree': 0.7892586746254588}. Best is trial 21 with value: 0.7931476012341306.\n",
      "[I 2025-10-21 19:24:47,696] Trial 34 finished with value: 0.7920689525949459 and parameters: {'learning_rate': 0.012288252801485985, 'num_leaves': 41, 'max_depth': 7, 'min_child_samples': 29, 'subsample': 0.9894756222651215, 'colsample_bytree': 0.8609001448469488}. Best is trial 21 with value: 0.7931476012341306.\n",
      "[I 2025-10-21 19:24:50,354] Trial 35 finished with value: 0.7919073200912137 and parameters: {'learning_rate': 0.023048476302879, 'num_leaves': 44, 'max_depth': 6, 'min_child_samples': 25, 'subsample': 0.7103587536229405, 'colsample_bytree': 0.8107320785172556}. Best is trial 21 with value: 0.7931476012341306.\n",
      "[I 2025-10-21 19:24:51,794] Trial 36 finished with value: 0.7910186706004204 and parameters: {'learning_rate': 0.03289474541536504, 'num_leaves': 27, 'max_depth': 9, 'min_child_samples': 26, 'subsample': 0.9165358426895864, 'colsample_bytree': 0.778837862388281}. Best is trial 21 with value: 0.7931476012341306.\n",
      "[I 2025-10-21 19:24:53,531] Trial 37 finished with value: 0.7911204142615026 and parameters: {'learning_rate': 0.054614824236405166, 'num_leaves': 39, 'max_depth': 5, 'min_child_samples': 21, 'subsample': 0.8887608977397456, 'colsample_bytree': 0.6821793278759108}. Best is trial 21 with value: 0.7931476012341306.\n",
      "[I 2025-10-21 19:24:55,106] Trial 38 finished with value: 0.7927446371171447 and parameters: {'learning_rate': 0.02752688686422315, 'num_leaves': 36, 'max_depth': 6, 'min_child_samples': 23, 'subsample': 0.9691559238219934, 'colsample_bytree': 0.7206935213046151}. Best is trial 21 with value: 0.7931476012341306.\n",
      "[I 2025-10-21 19:24:56,683] Trial 39 finished with value: 0.7918341710675446 and parameters: {'learning_rate': 0.02148704889478228, 'num_leaves': 20, 'max_depth': 6, 'min_child_samples': 23, 'subsample': 0.9741325497956836, 'colsample_bytree': 0.8769084935116034}. Best is trial 21 with value: 0.7931476012341306.\n",
      "[I 2025-10-21 19:24:57,850] Trial 40 finished with value: 0.7910501006756044 and parameters: {'learning_rate': 0.027274868621485707, 'num_leaves': 10, 'max_depth': 4, 'min_child_samples': 28, 'subsample': 0.8025263312933092, 'colsample_bytree': 0.8206166582509041}. Best is trial 21 with value: 0.7931476012341306.\n",
      "[I 2025-10-21 19:25:00,992] Trial 41 finished with value: 0.7933812423565015 and parameters: {'learning_rate': 0.0161945307562969, 'num_leaves': 36, 'max_depth': 6, 'min_child_samples': 22, 'subsample': 0.9537604399255435, 'colsample_bytree': 0.7231530404355765}. Best is trial 41 with value: 0.7933812423565015.\n",
      "[I 2025-10-21 19:25:03,666] Trial 42 finished with value: 0.7924755526016313 and parameters: {'learning_rate': 0.016102995030697773, 'num_leaves': 40, 'max_depth': 6, 'min_child_samples': 20, 'subsample': 0.9351548021407846, 'colsample_bytree': 0.7163646831221229}. Best is trial 41 with value: 0.7933812423565015.\n",
      "[I 2025-10-21 19:25:06,794] Trial 43 finished with value: 0.7920799453616348 and parameters: {'learning_rate': 0.012083443059485737, 'num_leaves': 35, 'max_depth': 5, 'min_child_samples': 22, 'subsample': 0.9083313977684222, 'colsample_bytree': 0.757233591499117}. Best is trial 41 with value: 0.7933812423565015.\n",
      "[I 2025-10-21 19:25:08,001] Trial 44 finished with value: 0.7922168829714042 and parameters: {'learning_rate': 0.028318717266370797, 'num_leaves': 29, 'max_depth': 6, 'min_child_samples': 26, 'subsample': 0.9827210612364226, 'colsample_bytree': 0.7887178041595198}. Best is trial 41 with value: 0.7933812423565015.\n",
      "[I 2025-10-21 19:25:10,078] Trial 45 finished with value: 0.7924422321004694 and parameters: {'learning_rate': 0.02041716526603346, 'num_leaves': 33, 'max_depth': 5, 'min_child_samples': 29, 'subsample': 0.954821266509143, 'colsample_bytree': 0.6265560628969021}. Best is trial 41 with value: 0.7933812423565015.\n",
      "[I 2025-10-21 19:25:12,166] Trial 46 finished with value: 0.7920770968386167 and parameters: {'learning_rate': 0.016726419918034415, 'num_leaves': 38, 'max_depth': 6, 'min_child_samples': 25, 'subsample': 0.6225528327912138, 'colsample_bytree': 0.6656060285783064}. Best is trial 41 with value: 0.7933812423565015.\n",
      "[I 2025-10-21 19:25:14,613] Trial 47 finished with value: 0.7925571583965801 and parameters: {'learning_rate': 0.01364721399088706, 'num_leaves': 30, 'max_depth': 7, 'min_child_samples': 19, 'subsample': 0.9658535314242713, 'colsample_bytree': 0.6949692063719424}. Best is trial 41 with value: 0.7933812423565015.\n",
      "[I 2025-10-21 19:25:19,405] Trial 48 finished with value: 0.7925427093396983 and parameters: {'learning_rate': 0.010003150827411083, 'num_leaves': 46, 'max_depth': 6, 'min_child_samples': 23, 'subsample': 0.999943522962182, 'colsample_bytree': 0.7230076942640539}. Best is trial 41 with value: 0.7933812423565015.\n",
      "[I 2025-10-21 19:25:21,646] Trial 49 finished with value: 0.7914522270984686 and parameters: {'learning_rate': 0.015002368904169403, 'num_leaves': 35, 'max_depth': 4, 'min_child_samples': 17, 'subsample': 0.600594130309701, 'colsample_bytree': 0.7434758484058211}. Best is trial 41 with value: 0.7933812423565015.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study complete!\n",
      "\n",
      "Best trial:\n",
      "  Value (Mean F1): 0.7934\n",
      "  Best Params: \n",
      "{'learning_rate': 0.0161945307562969, 'num_leaves': 36, 'max_depth': 6, 'min_child_samples': 22, 'subsample': 0.9537604399255435, 'colsample_bytree': 0.7231530404355765}\n",
      "\n",
      "Optuna-Tuned LightGBM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.84      0.85      2416\n",
      "         1.0       0.79      0.81      0.80      1764\n",
      "\n",
      "    accuracy                           0.83      4180\n",
      "   macro avg       0.82      0.83      0.82      4180\n",
      "weighted avg       0.83      0.83      0.83      4180\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "import lightgbm as lgbm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Step 1: Define the Objective Function ---\n",
    "def objective_lgbm(trial):\n",
    "    \"\"\"\n",
    "    Objective function with manual Cross-Validation and Early Stopping.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Define the search space for LightGBM\n",
    "    param = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'random_state': 42,\n",
    "        'n_estimators': 1000,\n",
    "        'verbose': -1, # Suppress LightGBM's own logging\n",
    "        \n",
    "        # These are the key parameters we'll tune\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 10, 50), # Key LGBM param\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 30),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0)\n",
    "    }\n",
    "    \n",
    "    # 2. Set up 3-Fold Cross-Validation\n",
    "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "\n",
    "    # 3. Manually run the CV loop\n",
    "    for train_idx, val_idx in skf.split(X_train, y_train):\n",
    "        X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        \n",
    "        model = lgbm.LGBMClassifier(**param)\n",
    "        \n",
    "        # Train with early stopping\n",
    "        model.fit(\n",
    "            X_train_fold, \n",
    "            y_train_fold,\n",
    "            eval_set=[(X_val_fold, y_val_fold)],\n",
    "            eval_metric='logloss',\n",
    "            # Use the 'callbacks' argument for early stopping\n",
    "            callbacks=[lgbm.early_stopping(50, verbose=False)]\n",
    "        )\n",
    "        \n",
    "        preds = model.predict(X_val_fold)\n",
    "        f1 = f1_score(y_val_fold, preds, average='binary')\n",
    "        scores.append(f1)\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "# --- Step 2: Create and Run the Study ---\n",
    "print(\"Starting Optuna study for LightGBM... (This should be fast!)\")\n",
    "study_lgbm = optuna.create_study(direction='maximize')\n",
    "study_lgbm.optimize(objective_lgbm, n_trials=50) # 50 trials\n",
    "print(\"Study complete!\")\n",
    "\n",
    "# --- Step 3: Get Best Params ---\n",
    "print(\"\\nBest trial:\")\n",
    "print(f\"  Value (Mean F1): {study_lgbm.best_value:.4f}\")\n",
    "print(\"  Best Params: \")\n",
    "print(study_lgbm.best_params)\n",
    "\n",
    "\n",
    "# --- Step 4: Train the FINAL Model ---\n",
    "# 1. Create a new train/validation split\n",
    "X_train_final, X_val_final, y_train_final, y_val_final = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "# 2. Create the final model\n",
    "final_lgbm = lgbm.LGBMClassifier(\n",
    "    random_state=42,\n",
    "    n_estimators=1000,\n",
    "    **study_lgbm.best_params # Use best params from Optuna\n",
    ")\n",
    "\n",
    "# 3. Train it with early stopping\n",
    "final_lgbm.fit(\n",
    "    X_train_final, \n",
    "    y_train_final,\n",
    "    eval_set=[(X_val_final, y_val_final)],\n",
    "    eval_metric='logloss',\n",
    "    callbacks=[lgbm.early_stopping(50, verbose=False)]\n",
    ")\n",
    "\n",
    "# --- Step 5: Evaluate the Optuna-Tuned Model on the TEST set ---\n",
    "y_pred_optuna_lgbm = final_lgbm.predict(X_test)\n",
    "\n",
    "print(\"\\nOptuna-Tuned LightGBM Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_optuna_lgbm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75bd6cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {\n",
    "    'learning_rate':  0.0161945307562969, \n",
    "    'num_leaves': 36, \n",
    "    'max_depth': 6, \n",
    "    'min_child_samples': 22, \n",
    "    'subsample': 0.9537604399255435, \n",
    "    'colsample_bytree': 0.7231530404355765\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadb4ae1",
   "metadata": {},
   "source": [
    "### ClassWeight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24b9b648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated scale_pos_weight: 1.3882\n",
      "\n",
      "Training LightGBM model with class weighting...\n",
      "Model training complete.\n",
      "\n",
      "Evaluating model on test data...\n",
      "\n",
      "LightGBM (Tuned + Weighted) Report:\n",
      "              precision    recall  f1-score      support\n",
      "0.0            0.885727  0.798841  0.840044  2416.000000\n",
      "1.0            0.757121  0.858844  0.804781  1764.000000\n",
      "accuracy       0.824163  0.824163  0.824163     0.824163\n",
      "macro avg      0.821424  0.828842  0.822412  4180.000000\n",
      "weighted avg   0.831454  0.824163  0.825162  4180.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import lightgbm as lgb\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# --- 1. Load Data (to calculate the weight) ---\n",
    "try:\n",
    "    data_train_full = pandas.read_csv('./data/salary.train.processed.csv').set_index('id')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå salary.train.processed.csv ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö path\")\n",
    "    # exit() \n",
    "\n",
    "X_full = data_train_full.drop(['label'], axis='columns')\n",
    "y_full = data_train_full['label']\n",
    "\n",
    "# --- 2. Calculate scale_pos_weight ---\n",
    "# This is the same logic as your XGBoost code\n",
    "scale_pos_weight = len(y_full[y_full == 0]) / len(y_full[y_full == 1])\n",
    "print(f\"Calculated scale_pos_weight: {scale_pos_weight:.4f}\")\n",
    "\n",
    "# --- 3. Define Your Best Parameters ---\n",
    "# These are the params you provided in your prompt\n",
    "best_lgbm_params = {\n",
    "    'learning_rate': 0.11760370695477697, \n",
    "    'num_leaves': 28, \n",
    "    'max_depth': 7, \n",
    "    'min_child_samples': 19, \n",
    "    'subsample': 0.6811934251139399, \n",
    "    'colsample_bytree': 0.7733362388832487\n",
    "}\n",
    "\n",
    "# --- 4. Create and Train the Model ---\n",
    "print(\"\\nTraining LightGBM model with class weighting...\")\n",
    "\n",
    "lgbm_model_final = lgb.LGBMClassifier(\n",
    "    **best_lgbm_params,       # Apply all your tuned parameters\n",
    "    scale_pos_weight=scale_pos_weight, # üëà Here is the class weight\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train the model on the full training dataset\n",
    "lgbm_model_final.fit(X_full, y_full)\n",
    "\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# --- 5. Evaluate on Test Data (Recommended) ---\n",
    "print(\"\\nEvaluating model on test data...\")\n",
    "\n",
    "data_test_lgbm = pandas.read_csv('./data/salary.test.processed.csv').set_index('id')\n",
    "data_test_lgbm['prediction'] = lgbm_model_final.predict(data_test_lgbm.drop(['label'], axis='columns'))\n",
    "\n",
    "# Print the report\n",
    "report_scores_lgbm = sklearn.metrics.classification_report(\n",
    "    y_true=data_test_lgbm['label'],\n",
    "    y_pred=data_test_lgbm['prediction'],\n",
    "    digits=6,\n",
    "    output_dict=True\n",
    ")\n",
    "df_score_lgbm = pandas.DataFrame(report_scores_lgbm).transpose()\n",
    "\n",
    "print(\"\\nLightGBM (Tuned + Weighted) Report:\")\n",
    "print(df_score_lgbm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5be28d7",
   "metadata": {},
   "source": [
    "### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9335de8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training data shape: (16720, 89)\n",
      "Original label distribution:\n",
      "label\n",
      "0.0    9719\n",
      "1.0    7001\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Applying SMOTE to the training data...\n",
      "New resampled training data shape: (19438, 89)\n",
      "New resampled label distribution:\n",
      "label\n",
      "1.0    9719\n",
      "0.0    9719\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Training final LGBM model on SMOTEd data...\n",
      "Model training complete.\n",
      "\n",
      "Evaluating model on *original* test data...\n",
      "\n",
      "LightGBM (Tuned + SMOTE) Report:\n",
      "              precision    recall  f1-score      support\n",
      "0.0            0.877354  0.790563  0.831700  2416.000000\n",
      "1.0            0.747379  0.848639  0.794797  1764.000000\n",
      "accuracy       0.815072  0.815072  0.815072     0.815072\n",
      "macro avg      0.812367  0.819601  0.813249  4180.000000\n",
      "weighted avg   0.822503  0.815072  0.816127  4180.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import lightgbm as lgb\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE  # üëà 1. Import SMOTE\n",
    "\n",
    "# --- 1. Load Data (Needed for training) ---\n",
    "try:\n",
    "    data_train_full = pandas.read_csv('./data/salary.train.processed.csv').set_index('id')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå salary.train.processed.csv ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö path\")\n",
    "    # exit() \n",
    "\n",
    "X_full = data_train_full.drop(['label'], axis='columns')\n",
    "y_full = data_train_full['label']\n",
    "\n",
    "print(f\"Original training data shape: {X_full.shape}\")\n",
    "print(f\"Original label distribution:\\n{y_full.value_counts()}\")\n",
    "\n",
    "# --- 2. Define Your Best Parameters ---\n",
    "# These are the params you provided\n",
    "best_lgbm_params = best_params\n",
    "\n",
    "# --- 3. Apply SMOTE to the Training Data ---\n",
    "print(\"\\nApplying SMOTE to the training data...\")\n",
    "smote = SMOTE(random_state=42, n_jobs=-1)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_full, y_full)\n",
    "\n",
    "print(f\"New resampled training data shape: {X_resampled.shape}\")\n",
    "print(f\"New resampled label distribution:\\n{y_resampled.value_counts()}\")\n",
    "\n",
    "# --- 4. Create and Train the Final Model (with NO class_weight) ---\n",
    "print(\"\\nTraining final LGBM model on SMOTEd data...\")\n",
    "\n",
    "lgbm_model_final = lgb.LGBMClassifier(\n",
    "    **best_lgbm_params,\n",
    "    # ‚ö†Ô∏è NO 'scale_pos_weight' or 'class_weight' here\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 5. Train the model on the NEW resampled data\n",
    "lgbm_model_final.fit(X_resampled, y_resampled)\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# --- 6. Evaluate on ORIGINAL Test Data ---\n",
    "print(\"\\nEvaluating model on *original* test data...\")\n",
    "\n",
    "data_test_lgbm = pandas.read_csv('./data/salary.test.processed.csv').set_index('id')\n",
    "\n",
    "# IMPORTANT: Do NOT apply SMOTE to the test data.\n",
    "X_test = data_test_lgbm.drop(['label'], axis='columns')\n",
    "y_test = data_test_lgbm['label']\n",
    "\n",
    "data_test_lgbm['prediction'] = lgbm_model_final.predict(X_test)\n",
    "\n",
    "# Print the report\n",
    "report_scores_lgbm = sklearn.metrics.classification_report(\n",
    "    y_true=y_test,\n",
    "    y_pred=data_test_lgbm['prediction'],\n",
    "    digits=6,\n",
    "    output_dict=True\n",
    ")\n",
    "df_score_lgbm = pandas.DataFrame(report_scores_lgbm).transpose()\n",
    "\n",
    "print(\"\\nLightGBM (Tuned + SMOTE) Report:\")\n",
    "print(df_score_lgbm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4323c0cb",
   "metadata": {},
   "source": [
    "### SMOTETomek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fe56d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 3. Testing LightGBM with SMOTETomek ---\n",
      "Applying SMOTETomek...\n",
      "New resampled label distribution:\n",
      "label\n",
      "1.0    8914\n",
      "0.0    8914\n",
      "Name: count, dtype: int64\n",
      "Model training complete.\n",
      "\n",
      "LightGBM (Tuned + SMOTETomek) Report:\n",
      "              precision    recall  f1-score      support\n",
      "0.0            0.875572  0.792219  0.831812  2416.000000\n",
      "1.0            0.748245  0.845805  0.794039  1764.000000\n",
      "accuracy       0.814833  0.814833  0.814833     0.814833\n",
      "macro avg      0.811908  0.819012  0.812926  4180.000000\n",
      "weighted avg   0.821839  0.814833  0.815872  4180.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import lightgbm as lgb\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "print(\"\\n--- 3. Testing LightGBM with SMOTETomek ---\")\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    data_train_full = pandas.read_csv('./data/salary.train.processed.csv').set_index('id')\n",
    "    data_test_lgbm = pandas.read_csv('./data/salary.test.processed.csv').set_index('id')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå salary.train.processed.csv ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö path\")\n",
    "    # exit() \n",
    "\n",
    "X_full = data_train_full.drop(['label'], axis='columns')\n",
    "y_full = data_train_full['label']\n",
    "X_test = data_test_lgbm.drop(['label'], axis='columns')\n",
    "y_test = data_test_lgbm['label']\n",
    "\n",
    "# --- Apply SMOTETomek ---\n",
    "print(\"Applying SMOTETomek...\")\n",
    "smt = SMOTETomek(random_state=42, n_jobs=-1)\n",
    "X_resampled, y_resampled = smt.fit_resample(X_full, y_full)\n",
    "print(f\"New resampled label distribution:\\n{y_resampled.value_counts()}\")\n",
    "\n",
    "# --- Define Parameters ---\n",
    "best_lgbm_params = best_params\n",
    "\n",
    "# --- Create and Train Model ---\n",
    "lgbm_model = lgb.LGBMClassifier(\n",
    "    **best_lgbm_params,\n",
    "    # ‚ö†Ô∏è NO 'scale_pos_weight' or 'class_weight'\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "lgbm_model.fit(X_resampled, y_resampled) # Train on SMOTETomek data\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# --- Evaluate ---\n",
    "y_pred = lgbm_model.predict(X_test)\n",
    "report = classification_report(y_test, y_pred, digits=6, output_dict=True)\n",
    "df_report = pandas.DataFrame(report).transpose()\n",
    "\n",
    "print(\"\\nLightGBM (Tuned + SMOTETomek) Report:\")\n",
    "print(df_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddda6884",
   "metadata": {},
   "source": [
    "### ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03306bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 4. Testing LightGBM with ADASYN ---\n",
      "Applying ADASYN...\n",
      "New resampled label distribution:\n",
      "label\n",
      "1.0    9726\n",
      "0.0    9719\n",
      "Name: count, dtype: int64\n",
      "Model training complete.\n",
      "\n",
      "LightGBM (Tuned + ADASYN) Report:\n",
      "              precision    recall  f1-score      support\n",
      "0.0            0.893762  0.759106  0.820949  2416.000000\n",
      "1.0            0.726504  0.876417  0.794450  1764.000000\n",
      "accuracy       0.808612  0.808612  0.808612     0.808612\n",
      "macro avg      0.810133  0.817762  0.807700  4180.000000\n",
      "weighted avg   0.823178  0.808612  0.809766  4180.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import lightgbm as lgb\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "print(\"\\n--- 4. Testing LightGBM with ADASYN ---\")\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    data_train_full = pandas.read_csv('./data/salary.train.processed.csv').set_index('id')\n",
    "    data_test_lgbm = pandas.read_csv('./data/salary.test.processed.csv').set_index('id')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå salary.train.processed.csv ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö path\")\n",
    "    # exit() \n",
    "\n",
    "X_full = data_train_full.drop(['label'], axis='columns')\n",
    "y_full = data_train_full['label']\n",
    "X_test = data_test_lgbm.drop(['label'], axis='columns')\n",
    "y_test = data_test_lgbm['label']\n",
    "\n",
    "# --- Apply ADASYN ---\n",
    "print(\"Applying ADASYN...\")\n",
    "ada = ADASYN(random_state=42, n_jobs=-1)\n",
    "X_resampled, y_resampled = ada.fit_resample(X_full, y_full)\n",
    "print(f\"New resampled label distribution:\\n{y_resampled.value_counts()}\")\n",
    "\n",
    "# --- Define Parameters ---\n",
    "best_lgbm_params = best_params\n",
    "\n",
    "# --- Create and Train Model ---\n",
    "lgbm_model = lgb.LGBMClassifier(\n",
    "    **best_lgbm_params,\n",
    "    # ‚ö†Ô∏è NO 'scale_pos_weight' or 'class_weight'\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "lgbm_model.fit(X_resampled, y_resampled) # Train on ADASYN data\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# --- Evaluate ---\n",
    "y_pred = lgbm_model.predict(X_test)\n",
    "report = classification_report(y_test, y_pred, digits=6, output_dict=True)\n",
    "df_report = pandas.DataFrame(report).transpose()\n",
    "\n",
    "print(\"\\nLightGBM (Tuned + ADASYN) Report:\")\n",
    "print(df_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4e2d25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
